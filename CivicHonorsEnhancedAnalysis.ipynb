{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOg9ARYJdUVzU+02MyPfPFb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/KnowledgeReduce_CivicHonorsEnhancedAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Civic Honors webpage Step-by-Step enhanced anaysis"
      ],
      "metadata": {
        "id": "YgCU3mtoQd3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The updated Google Colab notebook now features a more sophisticated approach to analyzing webpage content, incorporating advanced natural language processing (NLP) and data analysis techniques. The goal is to derive deeper insights from the content of https://civichonors.com/. Here's a summary of the updated process:\n",
        "\n",
        "Setup: The initial step involves installing necessary Python packages like requests, spacy, beautifulsoup4, nltk, and scikit-learn. These libraries are essential for fetching and processing webpage content, and for performing advanced NLP tasks.\n",
        "\n",
        "Import Libraries: This step imports the installed libraries, which will be used in the subsequent steps for various tasks like web scraping, text processing, sentiment analysis, and topic modeling.\n",
        "\n",
        "Function Definitions:\n",
        "\n",
        "* Fetch Webpage Content: This function retrieves the HTML content from the specified URL, using BeautifulSoup to parse the HTML and extract the main text content for analysis.\n",
        "\n",
        "* Enhanced Map Phase: The webpage's text is processed to extract meaningful data including noun chunks, named entities, verbs, and sentences. Additionally, this phase conducts sentiment analysis on each sentence using NLTK's SentimentIntensityAnalyzer and prepares data for topic modeling.\n",
        "\n",
        "* Topic Modeling: Within the map phase, Latent Dirichlet Allocation (LDA) is employed to identify major topics within the text. This is achieved by transforming the sentences into a bag-of-words model and then applying LDA to discern key themes.\n",
        "\n",
        "* Shuffle and Sort: The extracted data (noun chunks, entities, and verbs) is organized by frequency using Counter from the collections module.\n",
        "\n",
        "* Advanced Reduce Phase: This phase aggregates the results, including the most common noun chunks, entities, verbs, the average sentiment score across all sentences, and the identified topics from the topic modeling.\n",
        "\n",
        "Execution and Output:\n",
        "\n",
        "* The process is executed in sequence: fetching content, mapping (detailed NLP analysis including sentiment analysis and topic modeling), shuffling and sorting (organizing the data), and reducing (summarizing and presenting key insights).\n",
        "* The results are displayed in a structured format, highlighting top noun chunks, named entities, verbs, the average sentiment of the text, and the main topics identified. This offers a comprehensive view of the content, from its basic structure to its underlying themes and sentiment.\n",
        "\n",
        "This enhanced notebook provides a multifaceted analysis of the webpage content, combining basic NLP tasks with more complex analyses like sentiment analysis and topic modeling. Such a comprehensive approach allows for a deeper understanding of the text, capturing not only the key elements but also the general tone and main topics, which are crucial for thorough content analysis."
      ],
      "metadata": {
        "id": "T1WuVSzlQjAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup"
      ],
      "metadata": {
        "id": "Co2uwmv7QlY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3BLNn_IO8LX",
        "outputId": "f2fff286-be61-45d4-c030-ecc06bf81b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-12-15 16:25:49.578902: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-15 16:25:49.578963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-15 16:25:49.581233: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-15 16:25:49.589474: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-15 16:25:50.862771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install requests spacy beautifulsoup4 nltk scikit-learn\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m nltk.downloader vader_lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import Libraries"
      ],
      "metadata": {
        "id": "x7SyjLvvQtLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_emu92MsQtfO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Define Functions"
      ],
      "metadata": {
        "id": "6OQUlhyYQ04b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch Webpage Content Function"
      ],
      "metadata": {
        "id": "tPBLp_-rQ7Ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_webpage_content():\n",
        "    url = 'https://civichonors.com/'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        main_content = soup.find('main')\n",
        "        return main_content.get_text() if main_content else ''\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "WyzNhOsZQ47I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhanced Map Phase Function"
      ],
      "metadata": {
        "id": "N6w7tkmOQ9xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def map_phase(content):\n",
        "    doc = nlp(content)\n",
        "    data = {\n",
        "        'noun_chunks': [chunk.text for chunk in doc.noun_chunks],\n",
        "        'named_entities': [(entity.text, entity.label_) for entity in doc.ents],\n",
        "        'verbs': [token.lemma_ for token in doc if token.pos_ == 'VERB'],\n",
        "        'sentences': [sent.text for sent in doc.sents]\n",
        "    }\n",
        "\n",
        "    sentiments = [sia.polarity_scores(sent) for sent in data['sentences']]\n",
        "    data['sentiments'] = sentiments\n",
        "\n",
        "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
        "    dtm = vectorizer.fit_transform(data['sentences'])\n",
        "    LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    LDA.fit(dtm)\n",
        "\n",
        "    topics = []\n",
        "    for index, topic in enumerate(LDA.components_):\n",
        "        top_words_indices = topic.argsort()[-10:]\n",
        "        top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_indices]\n",
        "        topics.append(f\"Topic {index + 1}: {' '.join(top_words)}\")\n",
        "    data['topics'] = topics\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "svAVTHpZQ_pk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhanced Shuffle and Sort Function"
      ],
      "metadata": {
        "id": "_JmPmt-FRBrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_and_sort(mapped_data):\n",
        "    organized_data = {\n",
        "        'noun_chunks': Counter(mapped_data['noun_chunks']),\n",
        "        'named_entities': Counter([item[0] for item in mapped_data['named_entities']]),\n",
        "        'verbs': Counter(mapped_data['verbs'])\n",
        "    }\n",
        "    return organized_data"
      ],
      "metadata": {
        "id": "1dD0We8iRDeq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Reduce Phase Function"
      ],
      "metadata": {
        "id": "65tPyqAPRG9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_phase(organized_data, mapped_data):\n",
        "    reduced_data = {\n",
        "        'top_noun_chunks': organized_data['noun_chunks'].most_common(10),\n",
        "        'top_entities': organized_data['named_entities'].most_common(10),\n",
        "        'top_verbs': organized_data['verbs'].most_common(10),\n",
        "        'average_sentiment': np.mean([s['compound'] for s in mapped_data['sentiments']]),\n",
        "        'topics': mapped_data['topics']\n",
        "    }\n",
        "    return reduced_data"
      ],
      "metadata": {
        "id": "f387o8bURFFl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Execute the Process"
      ],
      "metadata": {
        "id": "DgjykXQYRKu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content = fetch_webpage_content()\n",
        "if content:\n",
        "    mapped_data = map_phase(content)\n",
        "    organized_data = shuffle_and_sort(mapped_data)\n",
        "    reduced_data = reduce_phase(organized_data, mapped_data)\n",
        "\n",
        "    print(\"Top Noun Chunks:\", reduced_data['top_noun_chunks'])\n",
        "    print(\"\\nTop Named Entities:\", reduced_data['top_entities'])\n",
        "    print(\"\\nTop Verbs:\", reduced_data['top_verbs'])\n",
        "    print(\"\\nAverage Sentiment Score:\", reduced_data['average_sentiment'])\n",
        "    print(\"\\nIdentified Topics:\")\n",
        "    for topic in reduced_data['topics']:\n",
        "        print(topic)\n",
        "else:\n",
        "    print(\"Failed to fetch content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g8IAL7fRMC-",
        "outputId": "c8375e30-4d6d-4b37-a759-ac4442c46a2a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Noun Chunks: [('the community', 489), ('the civic honors program', 156), ('individuals', 152), ('that', 135), ('organizations', 102), ('it', 98), ('the program', 83), ('what', 64), ('the potential', 64), ('the university', 61)]\n",
            "\n",
            "Top Named Entities: [('first', 20), ('one', 15), ('One', 15), ('New York', 15), ('1999', 6), ('Johnson County Community College', 5), ('2000', 5), ('1989', 5), ('two', 5), ('1984', 4)]\n",
            "\n",
            "Top Verbs: [('have', 187), ('develop', 121), ('become', 74), ('benefit', 70), ('allow', 70), ('work', 65), ('participate', 60), ('think', 52), ('build', 49), ('take', 47)]\n",
            "\n",
            "Average Sentiment Score: 0.40203881278538817\n",
            "\n",
            "Identified Topics:\n",
            "Topic 1: model level individuals time society change university able participation community\n",
            "Topic 2: information implementation message individuals university organizations community honors civic program\n",
            "Topic 3: participation potential society graduation program engagement individuals honors community civic\n",
            "Topic 4: understanding society organizations university honors civic individuals program technology community\n",
            "Topic 5: vision positive benefit action possible potential individual individuals message community\n"
          ]
        }
      ]
    }
  ]
}
