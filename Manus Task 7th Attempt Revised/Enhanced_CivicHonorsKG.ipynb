{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e793ef",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/Enhanced_CivicHonorsKG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Code Summary\n",
    "\n",
    "This notebook encapsulates an end-to-end pipeline designed for the meticulous extraction, processing, and systematic organization of data from specified web sources into a well-structured knowledge graph, with significant enhancements to the web crawling and knowledge reduction capabilities. The workflow is outlined in detailed steps, each contributing to the creation of a refined and informative dataset:\n",
    "\n",
    "### Step 1: Install Necessary Libraries\n",
    "- Installation of pivotal Python libraries such as `requests` for web scraping, `beautifulsoup4` for HTML parsing, `networkx` for graph-based operations, `spacy` for advanced natural language processing tasks, `scikit-learn` for machine learning algorithms, and `sentence-transformers` for state-of-the-art text embeddings.\n",
    "- Code to restart the runtime\n",
    "\n",
    "### Step 2: Import Libraries and Define Classes\n",
    "- Incorporation of essential Python modules and the establishment of foundational classes:\n",
    "  - `ReliabilityRating`: An enumeration (Enum) for classifying the reliability of the extracted information.\n",
    "  - `KnowledgeGraph`: A versatile class for constructing and managing a knowledge graph, encompassing functionalities for adding facts, computing and updating quality scores, and fact retrieval.\n",
    "\n",
    "### Step 3: Define RecursiveCrawler Class\n",
    "- Implementation of a sophisticated `RecursiveCrawler` class that can follow links across multiple pages.\n",
    "- Respect for robots.txt rules and implementation of rate limiting to avoid overloading servers.\n",
    "- URL frontier management for efficient crawling across multiple domains.\n",
    "- Proper link extraction and text processing capabilities.\n",
    "\n",
    "### Step 4: Define EnhancedKnowledgeReduction Class\n",
    "- Implementation of multiple similarity calculation methods (spaCy, TF-IDF, transformers).\n",
    "- Hierarchical clustering for better fact grouping.\n",
    "- Entity-based knowledge reduction to eliminate redundant information.\n",
    "- Multi-stage reduction pipeline for more effective deduplication.\n",
    "\n",
    "### Step 5: Define Helper Functions\n",
    "- Creation of utility functions for processing text into facts.\n",
    "- Functions for populating the knowledge graph from crawler results.\n",
    "- Implementation of knowledge reduction application to the knowledge graph.\n",
    "\n",
    "### Step 6: Advanced Web Crawling\n",
    "- Initialization of the crawler with multiple start URLs.\n",
    "- Configuration of crawling parameters such as depth, page limits, and rate limiting.\n",
    "- Execution of the crawling process with detailed statistics collection.\n",
    "\n",
    "### Step 7: Populate the KnowledgeGraph\n",
    "- Initialization and population of the `KnowledgeGraph` instance from crawled content.\n",
    "- Systematic incorporation of each fact into the graph, complete with comprehensive attributes.\n",
    "- Analysis of fact distribution across different crawl depths.\n",
    "\n",
    "### Step 8: Apply Advanced Knowledge Reduction\n",
    "- Application of the multi-stage knowledge reduction pipeline.\n",
    "- Comparison of original and reduced knowledge graph sizes.\n",
    "- Presentation of representative facts after reduction.\n",
    "\n",
    "### Step 9: Serialize the KnowledgeGraph for Portability\n",
    "- A crucial step focusing on rendering the Knowledge Graph portable via serialization into JSON format, facilitating easy storage, transfer, and reconstruction across different environments.\n",
    "- Implementation of sharding for efficient handling of large datasets.\n",
    "\n",
    "This enhanced pipeline significantly improves the ability to crawl and process multiple webpages while respecting web etiquette standards, and provides more sophisticated knowledge reduction capabilities for higher quality insights. It is particularly suited for scenarios demanding high data quality and structure, enriched with advanced NLP techniques to ensure a dataset devoid of redundancies and abundant in diverse, valuable insights.\n",
    "\n",
    "# Step 1: Install Necessary Libraries & restart the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 networkx spacy scikit-learn sentence-transformers\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf12c48",
   "metadata": {},
   "source": [
    "# Step 2: Import Libraries and Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764079f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import time\n",
    "import enum\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f611a0d",
   "metadata": {},
   "source": [
    "## Define ReliabilityRating Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReliabilityRating(enum.Enum):\n",
    "    LOW = 1\n",
    "    MEDIUM = 2\n",
    "    HIGH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c8ca23",
   "metadata": {},
   "source": [
    "## Define KnowledgeGraph Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d819ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.fact_count = 0\n",
    "    \n",
    "    def add_fact(self, fact_statement, source_id, source_name, reliability=ReliabilityRating.MEDIUM, \n",
    "                 category=\"General\", tags=None, quality_score=None):\n",
    "        \"\"\"Add a fact to the knowledge graph.\"\"\"\n",
    "        if tags is None:\n",
    "            tags = []\n",
    "        \n",
    "        # Calculate quality score if not provided\n",
    "        if quality_score is None:\n",
    "            quality_score = self._compute_quality_score(fact_statement, reliability)\n",
    "        \n",
    "        fact = {\n",
    "            'fact_id': self.fact_count,\n",
    "            'fact_statement': fact_statement,\n",
    "            'source_id': source_id,\n",
    "            'source_name': source_name,\n",
    "            'reliability': reliability,\n",
    "            'category': category,\n",
    "            'tags': tags,\n",
    "            'quality_score': quality_score\n",
    "        }\n",
    "        \n",
    "        self.data.append(fact)\n",
    "        self.fact_count += 1\n",
    "        return fact['fact_id']\n",
    "    \n",
    "    def _compute_quality_score(self, fact_statement, reliability):\n",
    "        \"\"\"Compute quality score based on fact length and reliability.\"\"\"\n",
    "        # Length component: longer facts (up to a point) are considered higher quality\n",
    "        length = len(fact_statement)\n",
    "        length_score = min(length / 200, 1.0)  # Cap at 1.0 for facts longer than 200 chars\n",
    "        \n",
    "        # Reliability component: convert enum to numerical value\n",
    "        reliability_value = reliability.value / 3.0  # Normalize to [0.33, 1.0]\n",
    "        \n",
    "        # Combine scores (equal weight)\n",
    "        return (length_score + reliability_value) / 2.0\n",
    "    \n",
    "    def update_quality_scores(self):\n",
    "        \"\"\"Update quality scores for all facts.\"\"\"\n",
    "        for fact in self.data:\n",
    "            fact['quality_score'] = self._compute_quality_score(\n",
    "                fact['fact_statement'], fact['reliability'])\n",
    "    \n",
    "    def get_facts_by_quality(self, min_score=0.0, max_facts=None):\n",
    "        \"\"\"Get facts filtered by minimum quality score, sorted by quality.\"\"\"\n",
    "        filtered_facts = [f for f in self.data if f['quality_score'] >= min_score]\n",
    "        sorted_facts = sorted(filtered_facts, key=lambda x: x['quality_score'], reverse=True)\n",
    "        \n",
    "        if max_facts is not None:\n",
    "            return sorted_facts[:max_facts]\n",
    "        return sorted_facts\n",
    "    \n",
    "    def get_facts_by_source(self, source_id):\n",
    "        \"\"\"Get facts from a specific source.\"\"\"\n",
    "        return [f for f in self.data if f['source_id'] == source_id]\n",
    "    \n",
    "    def get_facts_by_category(self, category):\n",
    "        \"\"\"Get facts from a specific category.\"\"\"\n",
    "        return [f for f in self.data if f['category'] == category]\n",
    "    \n",
    "    def get_facts_by_tag(self, tag):\n",
    "        \"\"\"Get facts with a specific tag.\"\"\"\n",
    "        return [f for f in self.data if tag in f['tags']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be17e8c",
   "metadata": {},
   "source": [
    "# Step 3: Define RecursiveCrawler Class\n",
    "\n",
    "The `RecursiveCrawler` class is a significant enhancement over the original implementation, providing advanced web crawling capabilities:\n",
    "\n",
    "1. **Recursive Link Following**: Ability to follow links across multiple pages up to a specified depth\n",
    "2. **Robots.txt Compliance**: Respects website crawling policies defined in robots.txt\n",
    "3. **Rate Limiting**: Implements delays between requests to avoid overloading servers\n",
    "4. **URL Frontier Management**: Efficiently manages the list of URLs to visit\n",
    "5. **Domain-specific Limits**: Controls the number of pages crawled per domain\n",
    "\n",
    "This implementation follows web etiquette standards while providing comprehensive crawling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a73897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveCrawler:\n",
    "    def __init__(self, start_urls, max_depth=2, max_pages_per_domain=20, respect_robots=True, \n",
    "                 rate_limit=1.0, user_agent=\"KnowledgeReduceCrawler/1.0\"):\n",
    "        \"\"\"\n",
    "        Initialize the recursive crawler.\n",
    "        \n",
    "        Args:\n",
    "            start_urls (list): List of URLs to start crawling from\n",
    "            max_depth (int): Maximum depth to crawl\n",
    "            max_pages_per_domain (int): Maximum pages to crawl per domain\n",
    "            respect_robots (bool): Whether to respect robots.txt\n",
    "            rate_limit (float): Minimum time between requests to the same domain in seconds\n",
    "            user_agent (str): User agent string to use for requests\n",
    "        \"\"\"\n",
    "        self.start_urls = start_urls\n",
    "        self.max_depth = max_depth\n",
    "        self.max_pages_per_domain = max_pages_per_domain\n",
    "        self.respect_robots = respect_robots\n",
    "        self.rate_limit = rate_limit\n",
    "        self.user_agent = user_agent\n",
    "        \n",
    "        # Data structures for crawler management\n",
    "        self.visited_urls = set()\n",
    "        self.url_frontier = []  # URLs to visit\n",
    "        self.domain_last_access = {}  # Domain -> timestamp of last access\n",
    "        self.robots_parsers = {}  # Domain -> robots parser\n",
    "        self.results = []  # Crawled pages content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409c115",
   "metadata": {},
   "source": [
    "## Robots.txt Handling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22474469",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_robots_parser(self, url):\n",
    "        \"\"\"Get robots.txt parser for a domain.\"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        \n",
    "        if domain in self.robots_parsers:\n",
    "            return self.robots_parsers[domain]\n",
    "        \n",
    "        # Create new parser\n",
    "        robots_parser = RobotFileParser()\n",
    "        robots_parser.set_url(f\"{domain}/robots.txt\")\n",
    "        \n",
    "        try:\n",
    "            robots_parser.read()\n",
    "            self.robots_parsers[domain] = robots_parser\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading robots.txt for {domain}: {e}\")\n",
    "            # If we can't read robots.txt, create an empty parser that allows everything\n",
    "            robots_parser = RobotFileParser()\n",
    "            self.robots_parsers[domain] = robots_parser\n",
    "        \n",
    "        return robots_parser\n",
    "    \n",
    "    def _can_fetch(self, url):\n",
    "        \"\"\"Check if we can fetch a URL according to robots.txt.\"\"\"\n",
    "        if not self.respect_robots:\n",
    "            return True\n",
    "        \n",
    "        robots_parser = self._get_robots_parser(url)\n",
    "        return robots_parser.can_fetch(self.user_agent, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ab284",
   "metadata": {},
   "source": [
    "## Rate Limiting and URL Management Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3cf082",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _respect_rate_limits(self, domain):\n",
    "        \"\"\"Respect rate limits for a domain by sleeping if necessary.\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if domain in self.domain_last_access:\n",
    "            elapsed = current_time - self.domain_last_access[domain]\n",
    "            if elapsed < self.rate_limit:\n",
    "                time.sleep(self.rate_limit - elapsed)\n",
    "        \n",
    "        self.domain_last_access[domain] = time.time()\n",
    "    \n",
    "    def _add_to_frontier(self, url, depth):\n",
    "        \"\"\"Add URL to frontier if it meets criteria.\"\"\"\n",
    "        if url in self.visited_urls:\n",
    "            return\n",
    "        \n",
    "        parsed_url = urlparse(url)\n",
    "        domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        \n",
    "        # Check domain page count\n",
    "        domain_count = sum(1 for visited_url in self.visited_urls \n",
    "                          if urlparse(visited_url).netloc == parsed_url.netloc)\n",
    "        \n",
    "        if domain_count >= self.max_pages_per_domain:\n",
    "            return\n",
    "        \n",
    "        # Check robots.txt\n",
    "        if not self._can_fetch(url):\n",
    "            return\n",
    "        \n",
    "        # Add to frontier\n",
    "        self.url_frontier.append((url, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cecb82a",
   "metadata": {},
   "source": [
    "## Content Extraction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7146d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _extract_links(self, soup, base_url):\n",
    "        \"\"\"Extract links from a BeautifulSoup object.\"\"\"\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            # Convert relative URLs to absolute\n",
    "            absolute_url = urljoin(base_url, href)\n",
    "            # Filter out non-HTTP/HTTPS URLs and fragments\n",
    "            parsed = urlparse(absolute_url)\n",
    "            if parsed.scheme in ('http', 'https') and not parsed.fragment:\n",
    "                links.append(absolute_url)\n",
    "        return links\n",
    "    \n",
    "    def _extract_text(self, soup):\n",
    "        \"\"\"Extract text content from a BeautifulSoup object.\"\"\"\n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        # Get text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        \n",
    "        # Break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        \n",
    "        # Drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd5457",
   "metadata": {},
   "source": [
    "## Main Crawling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d85644",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def crawl(self):\n",
    "        \"\"\"Start the crawling process.\"\"\"\n",
    "        # Initialize frontier with start URLs\n",
    "        for url in self.start_urls:\n",
    "            self._add_to_frontier(url, 0)\n",
    "        \n",
    "        # Process frontier until empty or max pages reached\n",
    "        while self.url_frontier and len(self.visited_urls) < self.max_pages_per_domain * len(set(urlparse(u).netloc for u, _ in self.url_frontier if u not in self.visited_urls)):\n",
    "            # Get next URL from frontier\n",
    "            url, depth = self.url_frontier.pop(0)\n",
    "            \n",
    "            if url in self.visited_urls:\n",
    "                continue\n",
    "            \n",
    "            # Mark as visited\n",
    "            self.visited_urls.add(url)\n",
    "            \n",
    "            # Respect rate limits\n",
    "            domain = urlparse(url).netloc\n",
    "            self._respect_rate_limits(domain)\n",
    "            \n",
    "            # Fetch page\n",
    "            try:\n",
    "                headers = {'User-Agent': self.user_agent}\n",
    "                response = requests.get(url, headers=headers, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Parse content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Store result\n",
    "                self.results.append({\n",
    "                    'url': url,\n",
    "                    'depth': depth,\n",
    "                    'soup': soup,\n",
    "                    'text_content': self._extract_text(soup),\n",
    "                    'title': soup.title.text if soup.title else url\n",
    "                })\n",
    "                \n",
    "                # If not at max depth, extract and add links to frontier\n",
    "                if depth < self.max_depth:\n",
    "                    links = self._extract_links(soup, url)\n",
    "                    for link in links:\n",
    "                        self._add_to_frontier(link, depth + 1)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error crawling {url}: {e}\")\n",
    "        \n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece762e3",
   "metadata": {},
   "source": [
    "# Step 4: Define EnhancedKnowledgeReduction Class\n",
    "\n",
    "The `EnhancedKnowledgeReduction` class provides sophisticated knowledge reduction capabilities:\n",
    "\n",
    "1. **Multiple Similarity Methods**:\n",
    "   - spaCy-based semantic similarity\n",
    "   - TF-IDF vectorization with cosine similarity\n",
    "   - Transformer-based embeddings using sentence-transformers\n",
    "   - Hybrid approach combining all methods with configurable weights\n",
    "\n",
    "2. **Advanced Clustering**:\n",
    "   - Hierarchical clustering for better fact grouping\n",
    "   - Central fact selection from clusters\n",
    "\n",
    "3. **Entity-Based Reduction**:\n",
    "   - Reduction based on entity overlap\n",
    "   - Jaccard similarity for entity sets\n",
    "\n",
    "4. **Multi-Stage Pipeline**:\n",
    "   - Configurable reduction stages\n",
    "   - Progressive filtering for optimal results\n",
    "\n",
    "This implementation significantly improves the knowledge reduction capabilities compared to the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedKnowledgeReduction:\n",
    "    def __init__(self, use_transformer=True, transformer_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the knowledge reduction system.\n",
    "        \n",
    "        Args:\n",
    "            use_transformer (bool): Whether to use transformer models for embeddings\n",
    "            transformer_model (str): Transformer model to use for embeddings\n",
    "        \"\"\"\n",
    "        # Load spaCy model\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        \n",
    "        # Set up transformer model if requested\n",
    "        self.use_transformer = use_transformer\n",
    "        if use_transformer:\n",
    "            self.transformer = SentenceTransformer(transformer_model)\n",
    "        \n",
    "        # Similarity calculation methods\n",
    "        self.similarity_methods = {\n",
    "            \"spacy\": self._spacy_similarity,\n",
    "            \"tfidf\": self._tfidf_similarity,\n",
    "            \"transformer\": self._transformer_similarity,\n",
    "            \"hybrid\": self._hybrid_similarity\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ef13b",
   "metadata": {},
   "source": [
    "## Similarity Calculation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1132c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _spacy_similarity(self, text1, text2):\n",
    "        \"\"\"Calculate similarity using spaCy.\"\"\"\n",
    "        doc1 = self.nlp(text1)\n",
    "        doc2 = self.nlp(text2)\n",
    "        return doc1.similarity(doc2)\n",
    "    \n",
    "    def _tfidf_similarity(self, texts):\n",
    "        \"\"\"Calculate TF-IDF similarity matrix for a list of texts.\"\"\"\n",
    "        # Create TF-IDF vectorizer\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        return cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    def _transformer_similarity(self, texts):\n",
    "        \"\"\"Calculate similarity using transformer embeddings.\"\"\"\n",
    "        if not self.use_transformer:\n",
    "            raise ValueError(\"Transformer model not initialized\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.transformer.encode(texts)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        return cosine_similarity(embeddings)\n",
    "    \n",
    "    def _hybrid_similarity(self, texts, weights={\"spacy\": 0.3, \"tfidf\": 0.3, \"transformer\": 0.4}):\n",
    "        \"\"\"Calculate similarity using a weighted combination of methods.\"\"\"\n",
    "        similarity_matrices = {}\n",
    "        \n",
    "        # Calculate similarity using each method\n",
    "        if \"spacy\" in weights:\n",
    "            spacy_sim = np.zeros((len(texts), len(texts)))\n",
    "            for i in range(len(texts)):\n",
    "                for j in range(i, len(texts)):\n",
    "                    sim = self._spacy_similarity(texts[i], texts[j])\n",
    "                    spacy_sim[i, j] = sim\n",
    "                    spacy_sim[j, i] = sim\n",
    "            similarity_matrices[\"spacy\"] = spacy_sim\n",
    "        \n",
    "        if \"tfidf\" in weights:\n",
    "            similarity_matrices[\"tfidf\"] = self._tfidf_similarity(texts)\n",
    "        \n",
    "        if \"transformer\" in weights and self.use_transformer:\n",
    "            similarity_matrices[\"transformer\"] = self._transformer_similarity(texts)\n",
    "        \n",
    "        # Combine similarity matrices using weights\n",
    "        combined_sim = np.zeros((len(texts), len(texts)))\n",
    "        weight_sum = sum(weights.values())\n",
    "        \n",
    "        for method, weight in weights.items():\n",
    "            if method in similarity_matrices:\n",
    "                combined_sim += (weight / weight_sum) * similarity_matrices[method]\n",
    "        \n",
    "        return combined_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21739f3",
   "metadata": {},
   "source": [
    "## Clustering and Knowledge Reduction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _find_central_fact(self, indices, similarity_matrix):\n",
    "        \"\"\"Find the most central fact in a cluster.\"\"\"\n",
    "        # Calculate centrality as the sum of similarities to other facts in the cluster\n",
    "        centrality = {}\n",
    "        for i in indices:\n",
    "            centrality[i] = sum(similarity_matrix[i, j] for j in indices if i != j)\n",
    "        \n",
    "        # Return the fact with highest centrality\n",
    "        return max(centrality.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    def reduce_knowledge(self, facts, method=\"hybrid\", threshold=0.85, min_cluster_size=2):\n",
    "        \"\"\"\n",
    "        Reduce knowledge by clustering similar facts.\n",
    "        \n",
    "        Args:\n",
    "            facts (list): List of fact statements\n",
    "            method (str): Similarity method to use\n",
    "            threshold (float): Similarity threshold for clustering\n",
    "            min_cluster_size (int): Minimum cluster size to consider\n",
    "            \n",
    "        Returns:\n",
    "            list: List of representative facts after reduction\n",
    "        \"\"\"\n",
    "        if len(facts) <= 1:\n",
    "            return facts\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        if method == \"hybrid\":\n",
    "            similarity_matrix = self._hybrid_similarity(facts)\n",
    "        elif method in self.similarity_methods:\n",
    "            similarity_matrix = self.similarity_methods[method](facts)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown similarity method: {method}\")\n",
    "        \n",
    "        # Apply hierarchical clustering\n",
    "        # Convert similarity to distance\n",
    "        distance_matrix = 1 - similarity_matrix\n",
    "        \n",
    "        # Apply clustering\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=1 - threshold,\n",
    "            affinity='precomputed',\n",
    "            linkage='average'\n",
    "        ).fit(distance_matrix)\n",
    "        \n",
    "        # Get cluster labels\n",
    "        labels = clustering.labels_\n",
    "        \n",
    "        # Group facts by cluster\n",
    "        clusters = {}\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(i)\n",
    "        \n",
    "        # Select representative fact from each cluster\n",
    "        representative_facts = []\n",
    "        for label, indices in clusters.items():\n",
    "            if len(indices) < min_cluster_size:\n",
    "                # Keep all facts from small clusters\n",
    "                for idx in indices:\n",
    "                    representative_facts.append(facts[idx])\n",
    "            else:\n",
    "                # Select the most central fact as representative\n",
    "                central_idx = self._find_central_fact(indices, similarity_matrix)\n",
    "                representative_facts.append(facts[central_idx])\n",
    "        \n",
    "        return representative_facts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec862278",
   "metadata": {},
   "source": [
    "## Entity-Based Reduction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc7315",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def entity_based_reduction(self, facts, threshold=0.85):\n",
    "        \"\"\"\n",
    "        Reduce knowledge based on entity overlap.\n",
    "        \n",
    "        Args:\n",
    "            facts (list): List of fact statements\n",
    "            threshold (float): Similarity threshold for entity overlap\n",
    "            \n",
    "        Returns:\n",
    "            list: List of facts after entity-based reduction\n",
    "        \"\"\"\n",
    "        # Process facts with spaCy to extract entities\n",
    "        docs = list(self.nlp.pipe(facts))\n",
    "        \n",
    "        # Create entity sets for each fact\n",
    "        entity_sets = []\n",
    "        for doc in docs:\n",
    "            entities = set()\n",
    "            for ent in doc.ents:\n",
    "                entities.add((ent.text, ent.label_))\n",
    "            entity_sets.append(entities)\n",
    "        \n",
    "        # Calculate entity overlap\n",
    "        unique_facts = []\n",
    "        unique_indices = []\n",
    "        \n",
    "        for i, (fact, entities) in enumerate(zip(facts, entity_sets)):\n",
    "            # Skip if no entities\n",
    "            if not entities:\n",
    "                unique_facts.append(fact)\n",
    "                unique_indices.append(i)\n",
    "                continue\n",
    "            \n",
    "            # Check if similar to any existing unique fact\n",
    "            is_unique = True\n",
    "            for j in unique_indices:\n",
    "                # Calculate Jaccard similarity of entity sets\n",
    "                overlap = len(entities.intersection(entity_sets[j]))\n",
    "                union = len(entities.union(entity_sets[j]))\n",
    "                \n",
    "                if union > 0 and overlap / union > threshold:\n",
    "                    is_unique = False\n",
    "                    break\n",
    "            \n",
    "            if is_unique:\n",
    "                unique_facts.append(fact)\n",
    "                unique_indices.append(i)\n",
    "        \n",
    "        return unique_facts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6aa916",
   "metadata": {},
   "source": [
    "## Multi-Stage Reduction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def multi_stage_reduction(self, facts, stages=None):\n",
    "        \"\"\"\n",
    "        Apply multi-stage knowledge reduction.\n",
    "        \n",
    "        Args:\n",
    "            facts (list): List of fact statements\n",
    "            stages (list): List of reduction stages to apply\n",
    "            \n",
    "        Returns:\n",
    "            list: List of facts after multi-stage reduction\n",
    "        \"\"\"\n",
    "        if stages is None:\n",
    "            stages = [\n",
    "                {\"method\": \"length_filter\", \"min_length\": 50},\n",
    "                {\"method\": \"entity_based\", \"threshold\": 0.8},\n",
    "                {\"method\": \"hybrid\", \"threshold\": 0.85}\n",
    "            ]\n",
    "        \n",
    "        reduced_facts = facts\n",
    "        \n",
    "        for stage in stages:\n",
    "            method = stage[\"method\"]\n",
    "            \n",
    "            if method == \"length_filter\":\n",
    "                min_length = stage.get(\"min_length\", 50)\n",
    "                reduced_facts = [f for f in reduced_facts if len(f) >= min_length]\n",
    "            \n",
    "            elif method == \"entity_based\":\n",
    "                threshold = stage.get(\"threshold\", 0.8)\n",
    "                reduced_facts = self.entity_based_reduction(reduced_facts, threshold)\n",
    "            \n",
    "            elif method in self.similarity_methods or method == \"hybrid\":\n",
    "                threshold = stage.get(\"threshold\", 0.85)\n",
    "                min_cluster_size = stage.get(\"min_cluster_size\", 2)\n",
    "                reduced_facts = self.reduce_knowledge(\n",
    "                    reduced_facts, method=method, \n",
    "                    threshold=threshold, min_cluster_size=min_cluster_size\n",
    "                )\n",
    "            \n",
    "            print(f\"After {method} reduction: {len(reduced_facts)} facts remaining\")\n",
    "        \n",
    "        return reduced_facts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78dd58d",
   "metadata": {},
   "source": [
    "# Step 5: Helper Functions\n",
    "\n",
    "These helper functions provide essential utilities for:\n",
    "\n",
    "1. **Processing Text into Facts**: Converting raw text content into structured facts\n",
    "2. **Populating Knowledge Graph**: Adding facts from crawler results to the knowledge graph\n",
    "3. **Applying Knowledge Reduction**: Using the enhanced reduction algorithms on the knowledge graph\n",
    "\n",
    "These functions bridge the gap between the crawler and knowledge graph components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_into_facts(text, min_length=30):\n",
    "    \"\"\"\n",
    "    Process text content into facts.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text content to process\n",
    "        min_length (int): Minimum length for a fact\n",
    "        \n",
    "    Returns:\n",
    "        list: List of facts\n",
    "    \"\"\"\n",
    "    # Split text into sentences (simple approach)\n",
    "    sentences = [s.strip() for s in text.replace('\\n', ' ').split('.') if s.strip()]\n",
    "    \n",
    "    # Combine short sentences\n",
    "    facts = []\n",
    "    current_fact = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_fact) + len(sentence) <= min_length:\n",
    "            current_fact += \" \" + sentence if current_fact else sentence\n",
    "        else:\n",
    "            if current_fact:\n",
    "                facts.append(current_fact + \".\")\n",
    "            current_fact = sentence\n",
    "    \n",
    "    # Add the last fact if not empty\n",
    "    if current_fact:\n",
    "        facts.append(current_fact + \".\")\n",
    "    \n",
    "    return facts\n",
    "\n",
    "def populate_knowledge_graph_from_crawler(kg, crawler_results):\n",
    "    \"\"\"\n",
    "    Populate knowledge graph from crawler results.\n",
    "    \n",
    "    Args:\n",
    "        kg: KnowledgeGraph instance\n",
    "        crawler_results: Results from the RecursiveCrawler\n",
    "    \"\"\"\n",
    "    for result in crawler_results:\n",
    "        url = result['url']\n",
    "        title = result['title']\n",
    "        text_content = result['text_content']\n",
    "        \n",
    "        # Process text content into facts\n",
    "        facts = process_text_into_facts(text_content)\n",
    "        \n",
    "        # Add facts to knowledge graph\n",
    "        for fact in facts:\n",
    "            kg.add_fact(\n",
    "                fact_statement=fact,\n",
    "                source_id=url,\n",
    "                source_name=title,\n",
    "                reliability=ReliabilityRating.MEDIUM,  # Default rating\n",
    "                category=\"Web Content\",\n",
    "                tags=[\"crawled\", f\"depth_{result['depth']}\"]\n",
    "            )\n",
    "\n",
    "def apply_knowledge_reduction(kg, reducer=None):\n",
    "    \"\"\"\n",
    "    Apply knowledge reduction to a knowledge graph.\n",
    "    \n",
    "    Args:\n",
    "        kg: KnowledgeGraph instance\n",
    "        reducer: EnhancedKnowledgeReduction instance (created if None)\n",
    "    \n",
    "    Returns:\n",
    "        KnowledgeGraph: New knowledge graph with reduced facts\n",
    "    \"\"\"\n",
    "    if reducer is None:\n",
    "        reducer = EnhancedKnowledgeReduction()\n",
    "    \n",
    "    # Extract facts from knowledge graph\n",
    "    facts = [fact['fact_statement'] for fact in kg.data]\n",
    "    \n",
    "    # Apply multi-stage reduction\n",
    "    reduced_facts = reducer.multi_stage_reduction(facts)\n",
    "    \n",
    "    # Create new knowledge graph with reduced facts\n",
    "    reduced_kg = KnowledgeGraph()\n",
    "    \n",
    "    # Add reduced facts to new knowledge graph\n",
    "    for fact_statement in reduced_facts:\n",
    "        # Find original fact in knowledge graph\n",
    "        for fact in kg.data:\n",
    "            if fact['fact_statement'] == fact_statement:\n",
    "                # Copy fact to new knowledge graph\n",
    "                reduced_kg.add_fact(\n",
    "                    fact_statement=fact['fact_statement'],\n",
    "                    source_id=fact['source_id'],\n",
    "                    source_name=fact['source_name'],\n",
    "                    reliability=fact['reliability'],\n",
    "                    category=fact['category'],\n",
    "                    tags=fact['tags']\n",
    "                )\n",
    "                break\n",
    "    \n",
    "    return reduced_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbda40",
   "metadata": {},
   "source": [
    "# Step 6: Advanced Web Crawling\n",
    "\n",
    "This section demonstrates the enhanced web crawling capabilities:\n",
    "\n",
    "1. **Multiple Start URLs**: Crawling begins from multiple seed URLs\n",
    "2. **Configurable Parameters**: Depth, page limits, and rate limiting are all configurable\n",
    "3. **Statistics Collection**: Detailed information about the crawling process is collected\n",
    "\n",
    "The crawler respects web etiquette standards while efficiently gathering content from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc97d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the crawler with multiple start URLs\n",
    "start_urls = [\n",
    "    \"https://civichonors.com/\",\n",
    "    \"https://www.nelslindahl.com/\",\n",
    "    \"https://en.wikipedia.org/wiki/Knowledge_graph\",\n",
    "    \"https://en.wikipedia.org/wiki/Web_crawler\"\n",
    "]\n",
    "\n",
    "# Create the crawler with custom settings\n",
    "crawler = RecursiveCrawler(\n",
    "    start_urls=start_urls,\n",
    "    max_depth=2,  # Follow links up to 2 levels deep\n",
    "    max_pages_per_domain=5,  # Limit to 5 pages per domain\n",
    "    respect_robots=True,  # Respect robots.txt\n",
    "    rate_limit=1.0,  # Wait at least 1 second between requests to the same domain\n",
    "    user_agent=\"KnowledgeReduceCrawler/1.0\"\n",
    ")\n",
    "\n",
    "# Start crawling\n",
    "print(\"Starting crawling process...\")\n",
    "results = crawler.crawl()\n",
    "print(f\"Crawling complete. Visited {len(crawler.visited_urls)} URLs, collected {len(results)} pages.\")\n",
    "\n",
    "# Print statistics about the crawled pages\n",
    "domains = {}\n",
    "depths = {}\n",
    "\n",
    "for result in results:\n",
    "    domain = urlparse(result['url']).netloc\n",
    "    depth = result['depth']\n",
    "    \n",
    "    if domain not in domains:\n",
    "        domains[domain] = 0\n",
    "    domains[domain] += 1\n",
    "    \n",
    "    if depth not in depths:\n",
    "        depths[depth] = 0\n",
    "    depths[depth] += 1\n",
    "\n",
    "print(\"\\nPages per domain:\")\n",
    "for domain, count in domains.items():\n",
    "    print(f\"  {domain}: {count} pages\")\n",
    "\n",
    "print(\"\\nPages per depth level:\")\n",
    "for depth, count in sorted(depths.items()):\n",
    "    print(f\"  Depth {depth}: {count} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0dadc",
   "metadata": {},
   "source": [
    "# Step 7: Populate the KnowledgeGraph\n",
    "\n",
    "This section demonstrates how the knowledge graph is populated from the crawler results:\n",
    "\n",
    "1. **Fact Extraction**: Text content is processed into structured facts\n",
    "2. **Metadata Enrichment**: Facts are enriched with source information and crawl depth\n",
    "3. **Statistics Analysis**: Detailed information about the facts is collected and analyzed\n",
    "\n",
    "The knowledge graph provides a structured representation of the information gathered during crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc36c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KnowledgeGraph\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "# Populate the knowledge graph from crawler results\n",
    "populate_knowledge_graph_from_crawler(kg, results)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Knowledge graph populated with {len(kg.data)} facts from {len(results)} pages.\")\n",
    "\n",
    "# Display a sample of facts\n",
    "print(\"\\nSample facts:\")\n",
    "for fact in kg.data[:5]:\n",
    "    print(f\"Fact {fact['fact_id']}: {fact['fact_statement'][:100]}... (Source: {fact['source_name']})\")\n",
    "\n",
    "# Count facts by depth\n",
    "facts_by_depth = {}\n",
    "for fact in kg.data:\n",
    "    depth_tags = [tag for tag in fact['tags'] if tag.startswith('depth_')]\n",
    "    if depth_tags:\n",
    "        depth = int(depth_tags[0].split('_')[1])\n",
    "        if depth not in facts_by_depth:\n",
    "            facts_by_depth[depth] = 0\n",
    "        facts_by_depth[depth] += 1\n",
    "\n",
    "print(\"\\nFacts by crawl depth:\")\n",
    "for depth, count in sorted(facts_by_depth.items()):\n",
    "    print(f\"  Depth {depth}: {count} facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5371a",
   "metadata": {},
   "source": [
    "# Step 8: Apply Advanced Knowledge Reduction\n",
    "\n",
    "This section demonstrates the enhanced knowledge reduction capabilities:\n",
    "\n",
    "1. **Multi-Stage Reduction**: Facts are processed through multiple reduction stages\n",
    "2. **Comparison Analysis**: Original and reduced knowledge graph sizes are compared\n",
    "3. **Representative Facts**: The most central and informative facts are retained\n",
    "\n",
    "The knowledge reduction process significantly improves the quality and conciseness of the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ee22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the knowledge reducer\n",
    "reducer = EnhancedKnowledgeReduction(use_transformer=True)\n",
    "\n",
    "# Apply multi-stage knowledge reduction\n",
    "print(\"Applying multi-stage knowledge reduction...\")\n",
    "reduced_kg = apply_knowledge_reduction(kg, reducer)\n",
    "\n",
    "print(f\"\\nOriginal knowledge graph: {len(kg.data)} facts\")\n",
    "print(f\"Reduced knowledge graph: {len(reduced_kg.data)} facts\")\n",
    "print(f\"Reduction ratio: {len(reduced_kg.data) / len(kg.data):.2f}\")\n",
    "\n",
    "# Display sample of reduced facts\n",
    "print(\"\\nSample reduced facts:\")\n",
    "for fact in reduced_kg.data[:10]:\n",
    "    print(f\"Fact {fact['fact_id']}: {fact['fact_statement'][:100]}... (Source: {fact['source_name']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1df04",
   "metadata": {},
   "source": [
    "# Step 9: Serialize the KnowledgeGraph for Portability\n",
    "\n",
    "This section demonstrates how the knowledge graph is serialized for portability:\n",
    "\n",
    "1. **JSON Serialization**: The knowledge graph is converted to JSON format\n",
    "2. **Sharding Implementation**: Large datasets are split into manageable shards\n",
    "3. **Deserialization Support**: The serialized data can be reconstructed into a knowledge graph\n",
    "\n",
    "The serialization process facilitates easy storage, transfer, and reconstruction of the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8cdc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraphPortable:\n",
    "    def __init__(self, knowledge_graph=None):\n",
    "        \"\"\"\n",
    "        Initialize the portable knowledge graph.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_graph: KnowledgeGraph instance to serialize\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        if knowledge_graph:\n",
    "            self.serialize(knowledge_graph)\n",
    "    \n",
    "    def serialize(self, knowledge_graph):\n",
    "        \"\"\"\n",
    "        Serialize a knowledge graph to JSON.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_graph: KnowledgeGraph instance to serialize\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        for fact in knowledge_graph.data:\n",
    "            # Convert enum to string for JSON serialization\n",
    "            fact_copy = fact.copy()\n",
    "            if isinstance(fact_copy['reliability'], ReliabilityRating):\n",
    "                fact_copy['reliability'] = fact_copy['reliability'].name\n",
    "            self.data.append(fact_copy)\n",
    "    \n",
    "    def save_to_json(self, filename, shard_size=100):\n",
    "        \"\"\"\n",
    "        Save the serialized knowledge graph to JSON files with sharding.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): Base filename to save to\n",
    "            shard_size (int): Number of facts per shard\n",
    "        \"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)\n",
    "        \n",
    "        # Shard the data\n",
    "        for i in range(0, len(self.data), shard_size):\n",
    "            shard = self.data[i:i+shard_size]\n",
    "            shard_filename = f\"{os.path.splitext(filename)[0]}_shard_{i//shard_size}{os.path.splitext(filename)[1]}\"\n",
    "            with open(shard_filename, 'w') as f:\n",
    "                json.dump(shard, f, indent=2)\n",
    "    \n",
    "    def load_from_json(self, filename_pattern):\n",
    "        \"\"\"\n",
    "        Load the serialized knowledge graph from JSON files.\n",
    "        \n",
    "        Args:\n",
    "            filename_pattern (str): Pattern to match JSON files\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        \n",
    "        # Find all matching files\n",
    "        import glob\n",
    "        files = glob.glob(filename_pattern)\n",
    "        \n",
    "        for file in sorted(files):\n",
    "            with open(file, 'r') as f:\n",
    "                shard_data = json.load(f)\n",
    "                self.data.extend(shard_data)\n",
    "    \n",
    "    def deserialize(self):\n",
    "        \"\"\"\n",
    "        Deserialize the JSON data to a KnowledgeGraph instance.\n",
    "        \n",
    "        Returns:\n",
    "            KnowledgeGraph: Deserialized knowledge graph\n",
    "        \"\"\"\n",
    "        kg = KnowledgeGraph()\n",
    "        \n",
    "        for fact in self.data:\n",
    "            # Convert string back to enum\n",
    "            if isinstance(fact['reliability'], str):\n",
    "                fact['reliability'] = ReliabilityRating[fact['reliability']]\n",
    "            \n",
    "            # Add fact to knowledge graph\n",
    "            kg.add_fact(\n",
    "                fact_statement=fact['fact_statement'],\n",
    "                source_id=fact['source_id'],\n",
    "                source_name=fact['source_name'],\n",
    "                reliability=fact['reliability'],\n",
    "                category=fact['category'],\n",
    "                tags=fact['tags'],\n",
    "                quality_score=fact['quality_score']\n",
    "            )\n",
    "        \n",
    "        return kg\n",
    "\n",
    "# Create a portable version of the reduced knowledge graph\n",
    "kg_portable = KnowledgeGraphPortable(reduced_kg)\n",
    "\n",
    "# Save to JSON files with sharding\n",
    "kg_portable.save_to_json(\"reduced_knowledge_graph.json\", shard_size=100)\n",
    "\n",
    "# List the created JSON files\n",
    "import glob\n",
    "json_files = glob.glob(\"reduced_knowledge_graph*.json\")\n",
    "print(f\"Created {len(json_files)} JSON files:\")\n",
    "for file in sorted(json_files):\n",
    "    print(f\"  {file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
