{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CivicHonorsKGv19: Enhanced Knowledge Graph Reduction\n",
    "\n",
    "This notebook is an improved version of CivicHonorsKGv18, featuring advanced knowledge reduction techniques.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a knowledge graph for Civic Honors content with the following steps:\n",
    "\n",
    "1. **Install and Import Libraries**: Set up the required dependencies\n",
    "2. **Define Knowledge Graph Class**: Create a flexible KG structure\n",
    "3. **Define Enhanced Knowledge Reduction Class**: Implement advanced reduction techniques\n",
    "4. **Scrape Website Data**: Collect information from relevant websites\n",
    "5. **Populate Knowledge Graph**: Extract and structure facts\n",
    "6. **Retrieve and Display Facts**: View the extracted knowledge\n",
    "7. **Ensure Uniqueness**: Remove duplicate facts\n",
    "8. **Advanced Cleaning**: Apply semantic similarity for redundancy reduction\n",
    "9. **Enhanced Knowledge Reduction**: Apply transformer-based models and hierarchical clustering\n",
    "10. **Serialization**: Save and load the knowledge graph\n",
    "\n",
    "The main improvement in this version is the enhanced knowledge reduction function that uses transformer models, hierarchical clustering, entity disambiguation, and fact importance scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests beautifulsoup4 difflib spacy sentence-transformers scikit-learn networkx\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "import enum\n",
    "import networkx as nx\n",
    "from difflib import SequenceMatcher\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Any, Optional, Set, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Knowledge Graph Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReliabilityRating(enum.Enum):\n",
    "    UNKNOWN = \"Unknown\"\n",
    "    UNLIKELY_FALSE = \"Unlikely False\"\n",
    "    POSSIBLY_FALSE = \"Possibly False\"\n",
    "    POSSIBLY_TRUE = \"Possibly True\"\n",
    "    LIKELY_TRUE = \"Likely True\"\n",
    "    VERIFIED_TRUE = \"Verified True\"\n",
    "\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        \n",
    "    def add_fact(self, \n",
    "                fact_id=None, \n",
    "                fact_statement=None, \n",
    "                category=\"General\", \n",
    "                tags=None, \n",
    "                date_recorded=None, \n",
    "                last_updated=None, \n",
    "                reliability_rating=ReliabilityRating.UNKNOWN, \n",
    "                source_id=None, \n",
    "                source_title=None, \n",
    "                author_creator=None, \n",
    "                publication_date=None, \n",
    "                url_reference=None, \n",
    "                related_facts=None, \n",
    "                contextual_notes=None, \n",
    "                access_level=\"Public\", \n",
    "                usage_count=0):\n",
    "        \n",
    "        if date_recorded is None:\n",
    "            date_recorded = datetime.datetime.now()\n",
    "        \n",
    "        if last_updated is None:\n",
    "            last_updated = datetime.datetime.now()\n",
    "        \n",
    "        if tags is None:\n",
    "            tags = []\n",
    "        \n",
    "        if related_facts is None:\n",
    "            related_facts = []\n",
    "            \n",
    "        fact = {\n",
    "            \"fact_id\": fact_id,\n",
    "            \"fact_statement\": fact_statement,\n",
    "            \"category\": category,\n",
    "            \"tags\": tags,\n",
    "            \"date_recorded\": date_recorded,\n",
    "            \"last_updated\": last_updated,\n",
    "            \"reliability_rating\": reliability_rating,\n",
    "            \"source_id\": source_id,\n",
    "            \"source_title\": source_title,\n",
    "            \"author_creator\": author_creator,\n",
    "            \"publication_date\": publication_date,\n",
    "            \"url_reference\": url_reference,\n",
    "            \"related_facts\": related_facts,\n",
    "            \"contextual_notes\": contextual_notes,\n",
    "            \"access_level\": access_level,\n",
    "            \"usage_count\": usage_count\n",
    "        }\n",
    "        \n",
    "        self.data.append(fact)\n",
    "        return fact\n",
    "    \n",
    "    def update_quality_score(self, fact_id, new_score):\n",
    "        for fact in self.data:\n",
    "            if fact[\"fact_id\"] == fact_id:\n",
    "                fact[\"quality_score\"] = new_score\n",
    "                fact[\"last_updated\"] = datetime.datetime.now()\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def save_to_file(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            # Convert datetime objects to strings for JSON serialization\n",
    "            serializable_data = []\n",
    "            for fact in self.data:\n",
    "                fact_copy = fact.copy()\n",
    "                if isinstance(fact_copy[\"date_recorded\"], datetime.datetime):\n",
    "                    fact_copy[\"date_recorded\"] = fact_copy[\"date_recorded\"].isoformat()\n",
    "                if isinstance(fact_copy[\"last_updated\"], datetime.datetime):\n",
    "                    fact_copy[\"last_updated\"] = fact_copy[\"last_updated\"].isoformat()\n",
    "                if isinstance(fact_copy[\"publication_date\"], datetime.datetime):\n",
    "                    fact_copy[\"publication_date\"] = fact_copy[\"publication_date\"].isoformat()\n",
    "                if isinstance(fact_copy[\"reliability_rating\"], ReliabilityRating):\n",
    "                    fact_copy[\"reliability_rating\"] = fact_copy[\"reliability_rating\"].value\n",
    "                serializable_data.append(fact_copy)\n",
    "            \n",
    "            json.dump(serializable_data, f, indent=2)\n",
    "    \n",
    "    def load_from_file(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "            \n",
    "            # Convert string dates back to datetime objects\n",
    "            for fact in self.data:\n",
    "                if isinstance(fact[\"date_recorded\"], str):\n",
    "                    try:\n",
    "                        fact[\"date_recorded\"] = datetime.datetime.fromisoformat(fact[\"date_recorded\"])\n",
    "                    except:\n",
    "                        pass\n",
    "                if isinstance(fact[\"last_updated\"], str):\n",
    "                    try:\n",
    "                        fact[\"last_updated\"] = datetime.datetime.fromisoformat(fact[\"last_updated\"])\n",
    "                    except:\n",
    "                        pass\n",
    "                if isinstance(fact[\"publication_date\"], str):\n",
    "                    try:\n",
    "                        fact[\"publication_date\"] = datetime.datetime.fromisoformat(fact[\"publication_date\"])\n",
    "                    except:\n",
    "                        pass\n",
    "                if isinstance(fact[\"reliability_rating\"], str):\n",
    "                    try:\n",
    "                        fact[\"reliability_rating\"] = ReliabilityRating(fact[\"reliability_rating\"])\n",
    "                    except:\n",
    "                        fact[\"reliability_rating\"] = ReliabilityRating.UNKNOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Enhanced Knowledge Reduction Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedKnowledgeReduce:\n",
    "    def __init__(self, \n",
    "                 transformer_model='all-MiniLM-L6-v2',\n",
    "                 spacy_model='en_core_web_md',\n",
    "                 similarity_threshold=0.85,\n",
    "                 short_fact_threshold=50,\n",
    "                 importance_weight_semantic=0.4,\n",
    "                 importance_weight_centrality=0.3,\n",
    "                 importance_weight_length=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the EnhancedKnowledgeReduce class with configurable parameters.\n",
    "        \n",
    "        Args:\n",
    "            transformer_model (str): The sentence transformer model to use for embeddings\n",
    "            spacy_model (str): The spaCy model to use for NLP processing\n",
    "            similarity_threshold (float): Threshold for considering facts as similar\n",
    "            short_fact_threshold (int): Minimum length for facts to be considered\n",
    "            importance_weight_semantic (float): Weight for semantic richness in importance scoring\n",
    "            importance_weight_centrality (float): Weight for centrality in importance scoring\n",
    "            importance_weight_length (float): Weight for fact length in importance scoring\n",
    "        \"\"\"\n",
    "        self.transformer = SentenceTransformer(transformer_model)\n",
    "        self.nlp = spacy.load(spacy_model)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.short_fact_threshold = short_fact_threshold\n",
    "        self.importance_weights = {\n",
    "            'semantic': importance_weight_semantic,\n",
    "            'centrality': importance_weight_centrality,\n",
    "            'length': importance_weight_length\n",
    "        }\n",
    "    \n",
    "    def remove_short_facts(self, knowledge_graph: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Remove facts that are too short to be meaningful.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_graph (Dict): The knowledge graph to process\n",
    "            \n",
    "        Returns:\n",
    "            Dict: The knowledge graph with short facts removed\n",
    "        \"\"\"\n",
    "        knowledge_graph['data'] = [fact for fact in knowledge_graph['data'] \n",
    "                                  if len(fact['fact_statement']) > self.short_fact_threshold]\n",
    "        return knowledge_graph\n",
    "    \n",
    "    def compute_embeddings(self, knowledge_graph: Dict) -> Tuple[Dict, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute embeddings for all facts in the knowledge graph.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_graph (Dict): The knowledge graph to process\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[Dict, np.ndarray]: The updated knowledge graph and the embeddings matrix\n",
    "        \"\"\"\n",
    "        # Extract fact statements\n",
    "        fact_statements = [fact['fact_statement'] for fact in knowledge_graph['data']]\n",
    "        \n",
    "        # Compute embeddings using transformer model\n",
    "        embeddings = self.transformer.encode(fact_statements)\n",
    "        \n",
    "        # Store embeddings in the knowledge graph\n",
    "        for i, fact in enumerate(knowledge_graph['data']):\n",
    "            fact['embedding'] = embeddings[i].tolist()\n",
    "            \n",
    "        return knowledge_graph, embeddings\n",
    "    \n",
    "    def build_similarity_matrix(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build a similarity matrix based on cosine similarity of embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embeddings (np.ndarray): The embeddings matrix\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: The similarity matrix\n",
    "        \"\"\"\n",
    "        return cosine_similarity(embeddings)\n",
    "    \n",
    "    def hierarchical_clustering(self, \n",
    "                               similarity_matrix: np.ndarray, \n",
    "                               distance_threshold: float = 0.3) -> List[int]:\n",
    "        \"\"\"\n",
    "        Perform hierarchical clustering on the similarity matrix.\n",
    "        Compatible with different scikit-learn versions.\n",
    "        \n",
    "        Args:\n",
    "            similarity_matrix (np.ndarray): The similarity matrix\n",
    "            distance_threshold (float): The distance threshold for clustering\n",
    "            \n",
    "        Returns:\n",
    "            List[int]: The cluster assignments for each fact\n",
    "        \"\"\"\n",
    "        # Convert similarity to distance\n",
    "        distance_matrix = 1 - similarity_matrix\n",
    "        \n",
    "        # Try different parameter combinations for compatibility with various scikit-learn versions\n",
    "        try:\n",
    "            # First try with the most specific parameters\n",
    "            clustering = AgglomerativeClustering(\n",
    "                n_clusters=None,\n",
    "                distance_threshold=distance_threshold,\n",
    "                affinity='precomputed',\n",
    "                linkage='average'\n",
    "            )\n",
    "            return clustering.fit_predict(distance_matrix)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                # Try without affinity parameter\n",
    "                clustering = AgglomerativeClustering(\n",
    "                    n_clusters=None,\n",
    "                    distance_threshold=distance_threshold,\n",
    "                    linkage='average'\n",
    "                )\n",
    "                return clustering.fit_predict(distance_matrix)\n",
    "            except TypeError:\n",
    "                # Fallback to basic clustering with fixed number of clusters\n",
    "                # Estimate number of clusters based on similarity threshold\n",
    "                n_samples = similarity_matrix.shape[0]\n",
    "                estimated_clusters = max(1, min(n_samples, int(n_samples / 3)))\n",
    "                \n",
    "                clustering = AgglomerativeClustering(\n",
    "                    n_clusters=estimated_clusters,\n",
    "                    linkage='average'\n",
    "                )\n",
    "                return clustering.fit_predict(distance_matrix)\n",
    "    \n",
    "    def calculate_fact_importance(self, \n",
    "                                 knowledge_graph: Dict, \n",
    "                                 embeddings: np.ndarray,\n",
    "                                 similarity_matrix: np.ndarray) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate importance scores for facts based on multiple factors.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_graph (Dict): The knowledge graph to process\n",
    "            embeddings (np.ndarray): The embeddings matrix\n",
    "            similarity_matrix (np.ndarray): The similarity matrix\n",
    "            \n",
    "        Returns:\n",
    "            Dict: The knowledge graph with importance scores added\n",
    "        \"\"\"\n",
    "        # Create a graph for centrality calculation\n",
    "        G = nx.Graph()\n",
    "        for i in range(len(knowledge_graph['data'])):\n",
    "            G.add_node(i)\n",
    "            \n",
    "        # Add edges based on similarity\n",
    "        for i in range(len(similarity_matrix)):\n",
    "            for j in range(i+1, len(similarity_matrix)):\n",
    "                if similarity_matrix[i, j] > 0.5:  # Only connect if somewhat similar\n",
    "                    G.add_edge(i, j, weight=similarity_matrix[i, j])\n",
    "        \n",
    "        # Calculate centrality measures\n",
    "        centrality = nx.degree_centrality(G)\n",
    "        \n",
    "        # Calculate semantic richness\n",
    "        semantic_richness = []\n",
    "        for fact in knowledge_graph['data']:\n",
    "            doc = self.nlp(fact['fact_statement'])\n",
    "            entities = set([ent.text for ent in doc.ents])\n",
    "            noun_chunks = set([chunk.text for chunk in doc.noun_chunks])\n",
    "            richness = len(entities) + len(noun_chunks)\n",
    "            semantic_richness.append(richness)\n",
    "        \n",
    "        # Calculate length scores (normalized)\n",
    "        lengths = [len(fact['fact_statement']) for fact in knowledge_graph['data']]\n",
    "        max_length = max(lengths) if lengths else 1  # Avoid division by zero\n",
    "        length_scores = [length / max_length for length in lengths]\n",
    "        \n",
    "        # Calculate combined importance score\n",
    "        for i, fact in enumerate(knowledge_graph['data']):\n",
    "            importance = (\n",
    "                self.importance_weights['semantic'] * semantic_richness[i] +\n",
    "                self.importance_weights['centrality'] * centrality.get(i, 0) +\n",
    "                self.importance_weights['length'] * length_scores[i]\n",
    "            )\n",
    "            fact['importance_score'] = importance\n",
    "            \n",
    "        return knowledge_graph\n",
    "    \n",
    "    def select_representative_facts(self, \n",
    "                                   knowledge_graph: Dict, \n",
    "                                   cluster_assignments: List[int]) -> Dict:\n",
    "        \"\"\"\n",
    "        Select the most important fact from each cluster.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_graph (Dict): The knowledge graph to process\n",
    "            cluster_assignments (List[int]): The cluster assignments for each fact\n",
    "            \n",
    "        Returns:\n",
    "            Dict: A new knowledge graph with only the representative facts\n",
    "        \"\"\"\n",
    "        # Group facts by cluster\n",
    "        clusters = {}\n",
    "        for i, cluster_id in enumerate(cluster_assignments):\n",
    "            if cluster_id not in clusters:\n",
    "                clusters[cluster_id] = []\n",
    "            clusters[cluster_id].append(i)\n",
    "        \n",
    "        # Select the most important fact from each cluster\n",
    "        selected_facts = []\n",
    "        for cluster_id, fact_indices in clusters.items():\n",
    "            # Find the fact with the highest importance score in this cluster\n",
    "            best_fact_idx = max(fact_indices, \n",
    "                               key=lambda idx: knowledge_graph['data'][idx].get('importance_score', 0))\n",
    "            selected_facts.append(knowledge_graph['data'][best_fact_idx])\n",
    "        \n",
    "        # Create a new knowledge graph with only the selected facts\n",
    "        reduced_kg = KnowledgeGraph()\n",
    "        reduced_kg.data = selected_facts\n",
    "        \n",
    "        return reduced_kg\n",
    "    \n",
    "    def entity_disambiguation(self, knowledge_graph: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Identify when different terms refer to the same entity.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_graph (Dict): The knowledge graph to process\n",
    "            \n",
    "        Returns:\n",
    "            Dict: The knowledge graph with entity disambiguation information added\n",
    "        \"\"\"\n",
    "        # Extract all entities from facts\n",
    "        all_entities = {}\n",
    "        for fact in knowledge_graph['data']:\n",
    "            doc = self.nlp(fact['fact_statement'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.text not in all_entities:\n",
    "                    all_entities[ent.text] = []\n",
    "                all_entities[ent.text].append(ent)\n",
    "                \n",
    "        # Find similar entity names\n",
    "        entity_clusters = {}\n",
    "        entity_names = list(all_entities.keys())\n",
    "        \n",
    "        for i, name1 in enumerate(entity_names):\n",
    "            if name1 not in entity_clusters:\n",
    "                entity_clusters[name1] = [name1]\n",
    "                \n",
    "            for name2 in entity_names[i+1:]:\n",
    "                # Check string similarity\n",
    "                similarity = SequenceMatcher(None, name1.lower(), name2.lower()).ratio()\n",
    "                \n",
    "                # Check embedding similarity for more accuracy\n",
    "                if similarity > 0.7:  # Initial string filter\n",
    "                    emb1 = self.transformer.encode([name1])[0]\n",
    "                    emb2 = self.transformer.encode([name2])[0]\n",
    "                    cos_sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "                    \n",
    "                    if cos_sim > 0.85:  # High semantic similarity\n",
    "                        entity_clusters[name1].append(name2)\n",
    "                        entity_clusters[name2] = entity_clusters[name1]\n",
    "        \n",
    "        # Add entity disambiguation information to the knowledge graph\n",
    "        for fact in knowledge_graph['data']:\n",
    "            fact['disambiguated_entities'] = {}\n",
    "            doc = self.nlp(fact['fact_statement'])\n",
    "            for ent in doc.ents:\n",
    "                if ent.text in entity_clusters:\n",
    "                    fact['disambiguated_entities'][ent.text] = entity_clusters[ent.text]\n",
    "                    \n",
    "        return knowledge_graph\n",
    "    \n",
    "    def reduce_knowledge_graph(self, knowledge_graph: KnowledgeGraph) -> KnowledgeGraph:\n",
    "        \"\"\"\n",
    "        Apply the enhanced knowledge reduction process to a knowledge graph.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_graph (KnowledgeGraph): The knowledge graph to reduce\n",
    "            \n",
    "        Returns:\n",
    "            KnowledgeGraph: The reduced knowledge graph\n",
    "        \"\"\"\n",
    "        # Convert KnowledgeGraph to dictionary for processing\n",
    "        kg_dict = {'data': knowledge_graph.data}\n",
    "        \n",
    "        # Step 1: Remove short facts\n",
    "        kg_dict = self.remove_short_facts(kg_dict)\n",
    "        \n",
    "        # Step 2: Compute embeddings\n",
    "        kg_dict, embeddings = self.compute_embeddings(kg_dict)\n",
    "        \n",
    "        # Step 3: Build similarity matrix\n",
    "        similarity_matrix = self.build_similarity_matrix(embeddings)\n",
    "        \n",
    "        # Step 4: Perform entity disambiguation\n",
    "        kg_dict = self.entity_disambiguation(kg_dict)\n",
    "        \n",
    "        # Step 5: Calculate fact importance\n",
    "        kg_dict = self.calculate_fact_importance(kg_dict, embeddings, similarity_matrix)\n",
    "        \n",
    "        # Step 6: Perform hierarchical clustering\n",
    "        cluster_assignments = self.hierarchical_clustering(\n",
    "            similarity_matrix, \n",
    "            distance_threshold=1 - self.similarity_threshold\n",
    "        )\n",
    "        \n",
    "        # Step 7: Select representative facts\n",
    "        reduced_kg = self.select_representative_facts(kg_dict, cluster_assignments)\n",
    "        \n",
    "        return reduced_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Scrape Website Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from HTML elements\n",
    "def extract_text(element):\n",
    "    if element is None:\n",
    "        return \"\"\n",
    "    return element.get_text().strip()\n",
    "\n",
    "# Function to find facts in HTML content\n",
    "def find_facts(soup):\n",
    "    facts = []\n",
    "    \n",
    "    # Extract text from paragraphs\n",
    "    for p in soup.find_all('p'):\n",
    "        text = extract_text(p)\n",
    "        if text:\n",
    "            facts.append(text)\n",
    "    \n",
    "    # Extract text from headers\n",
    "    for header_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "        for header in soup.find_all(header_tag):\n",
    "            text = extract_text(header)\n",
    "            if text:\n",
    "                facts.append(text)\n",
    "    \n",
    "    # Extract text from list items\n",
    "    for li in soup.find_all('li'):\n",
    "        text = extract_text(li)\n",
    "        if text:\n",
    "            facts.append(text)\n",
    "    \n",
    "    return facts\n",
    "\n",
    "# Function to scrape a website and return its BeautifulSoup object\n",
    "def scrape_website(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during requests to {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Populate Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KnowledgeGraph\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "# URLs of the websites to scrape\n",
    "urls = {\n",
    "    \"CivicHonors\": \"https://civichonors.com/\",\n",
    "    \"NelsLindahl\": \"https://www.nelslindahl.com/\"\n",
    "}\n",
    "\n",
    "# Add facts from each website to the KnowledgeGraph\n",
    "for source_id, url in urls.items():\n",
    "    soup = scrape_website(url)\n",
    "    if soup:\n",
    "        facts = find_facts(soup)\n",
    "        for i, fact in enumerate(facts):\n",
    "            kg.add_fact(\n",
    "                fact_id=f\"{source_id}_{i}\",\n",
    "                fact_statement=fact,\n",
    "                category=\"General\",\n",
    "                tags=[source_id, \"WebScraped\"],\n",
    "                date_recorded=datetime.datetime.now(),\n",
    "                last_updated=datetime.datetime.now(),\n",
    "                reliability_rating=ReliabilityRating.LIKELY_TRUE,\n",
    "                source_id=source_id,\n",
    "                source_title=f\"{source_id} Website\",\n",
    "                author_creator=\"Web Scraping\",\n",
    "                publication_date=datetime.datetime.now(),\n",
    "                url_reference=url,\n",
    "                related_facts=[],\n",
    "                contextual_notes=f\"Extracted from {source_id} website\",\n",
    "                access_level=\"Public\",\n",
    "                usage_count=0\n",
    "            )\n",
    "\n",
    "# Save the facts to a file\n",
    "filename = 'knowledge_graph_facts.json'\n",
    "kg.save_to_file(filename)\n",
    "print(f\"Facts saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Retrieve and Display 10 Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of facts extracted\n",
    "print(f\"Total facts extracted: {len(kg.data)}\")\n",
    "\n",
    "# Display the first 10 facts, if available\n",
    "for i in range(min(10, len(kg.data))):\n",
    "    fact = kg.data[i]['fact_statement']  # Access the fact statement directly from the data list\n",
    "    print(f\"Fact {i+1}: {fact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Ensure Uniqueness of Facts in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_facts(knowledge_graph):\n",
    "    unique_facts = set()\n",
    "    unique_data = []\n",
    "    \n",
    "    for fact_data in knowledge_graph.data:\n",
    "        fact_statement = fact_data['fact_statement']\n",
    "        if fact_statement not in unique_facts:\n",
    "            unique_facts.add(fact_statement)\n",
    "            unique_data.append(fact_data)\n",
    "    \n",
    "    # Replace the original data with the unique data\n",
    "    knowledge_graph.data = unique_data\n",
    "    \n",
    "    return knowledge_graph\n",
    "\n",
    "# Call the function to remove duplicate facts\n",
    "remove_duplicate_facts(kg)\n",
    "\n",
    "# Optional: Print the total number of unique facts remaining\n",
    "print(f\"Total unique facts remaining: {len(kg.data)}\")\n",
    "\n",
    "# Save the facts to a file\n",
    "filename = 'unique_knowledge_graph_facts.json'\n",
    "kg.save_to_file(filename)\n",
    "print(f\"Facts saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Advanced Cleaning and Combining of Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def advanced_cleaning(knowledge_graph, similarity_threshold=0.8, short_fact_threshold=50):\n",
    "    # Remove short facts\n",
    "    knowledge_graph.data = [fact for fact in knowledge_graph.data if len(fact['fact_statement']) > short_fact_threshold]\n",
    "    \n",
    "    # Remove similar facts\n",
    "    unique_facts = []\n",
    "    for fact in knowledge_graph.data:\n",
    "        if not any(SequenceMatcher(None, f['fact_statement'], fact['fact_statement']).ratio() > similarity_threshold for f in unique_facts):\n",
    "            unique_facts.append(fact)\n",
    "    \n",
    "    knowledge_graph.data = unique_facts\n",
    "    \n",
    "    return knowledge_graph\n",
    "\n",
    "# Call the function for advanced cleaning\n",
    "advanced_cleaning(kg)\n",
    "\n",
    "# Optional: Print the total number of facts after advanced cleaning\n",
    "print(f\"Total facts after advanced cleaning: {len(kg.data)}\")\n",
    "\n",
    "# Save the facts to a file\n",
    "filename = 'advanced_knowledge_graph_facts.json'\n",
    "kg.save_to_file(filename)\n",
    "print(f\"Facts saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Enhanced Knowledge Reduction (Refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the EnhancedKnowledgeReduce class\n",
    "reducer = EnhancedKnowledgeReduce(\n",
    "    transformer_model='all-MiniLM-L6-v2',\n",
    "    similarity_threshold=0.85,\n",
    "    short_fact_threshold=50\n",
    ")\n",
    "\n",
    "# Apply the enhanced knowledge reduction\n",
    "reduced_kg = reducer.reduce_knowledge_graph(kg)\n",
    "\n",
    "# Print the total number of facts after super aggressive cleaning\n",
    "print(f\"Total facts after enhanced knowledge reduction: {len(reduced_kg.data)}\")\n",
    "\n",
    "# Save the facts to a file\n",
    "filename = 'reduced_facts.json'\n",
    "reduced_kg.save_to_file(filename)\n",
    "print(f\"Facts saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Serialization and deserialization of the KnowledgeGraph for portability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "class KnowledgeGraphPortable:\n",
    "    def __init__(self, knowledge_graph):\n",
    "        # Check if knowledge graph is a networkx graph or a list-based structure\n",
    "        if isinstance(knowledge_graph, (nx.Graph, nx.DiGraph, nx.MultiDiGraph)):\n",
    "            self.graph = knowledge_graph\n",
    "        elif hasattr(knowledge_graph, 'data') and isinstance(knowledge_graph.data, list):\n",
    "            self.graph = self.convert_list_to_graph(knowledge_graph.data)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported knowledge graph format\")\n",
    "    \n",
    "    def convert_list_to_graph(self, data):\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes for each fact\n",
    "        for fact in data:\n",
    "            G.add_node(fact['fact_id'], **fact)\n",
    "        \n",
    "        # Add edges for related facts\n",
    "        for fact in data:\n",
    "            for related_fact_id in fact.get('related_facts', []):\n",
    "                if G.has_node(related_fact_id):\n",
    "                    G.add_edge(fact['fact_id'], related_fact_id, relation_type='related')\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def to_json(self):\n",
    "        # Convert networkx graph to JSON serializable format\n",
    "        # Explicitly set edges=\"links\" to avoid future compatibility warnings\n",
    "        data = nx.node_link_data(self.graph, edges=\"links\")\n",
    "        \n",
    "        # Handle datetime objects\n",
    "        for node in data['nodes']:\n",
    "            for key, value in node.items():\n",
    "                if isinstance(value, datetime.datetime):\n",
    "                    node[key] = value.isoformat()\n",
    "                elif isinstance(value, enum.Enum):\n",
    "                    node[key] = value.value\n",
    "        \n",
    "        return json.dumps(data, indent=2)\n",
    "    \n",
    "    def from_json(self, json_str):\n",
    "        # Convert JSON string back to networkx graph\n",
    "        # Explicitly set edges=\"links\" to avoid future compatibility warnings\n",
    "        data = json.loads(json_str)\n",
    "        self.graph = nx.node_link_graph(data, edges=\"links\")\n",
    "        \n",
    "        # Handle datetime objects\n",
    "        for node_id in self.graph.nodes:\n",
    "            node = self.graph.nodes[node_id]\n",
    "            for key, value in list(node.items()):\n",
    "                if key in ['date_recorded', 'last_updated', 'publication_date'] and isinstance(value, str):\n",
    "                    try:\n",
    "                        node[key] = datetime.datetime.fromisoformat(value)\n",
    "                    except:\n",
    "                        pass\n",
    "                elif key == 'reliability_rating' and isinstance(value, str):\n",
    "                    try:\n",
    "                        node[key] = ReliabilityRating(value)\n",
    "                    except:\n",
    "                        node[key] = ReliabilityRating.UNKNOWN\n",
    "        \n",
    "        return self.graph\n",
    "    \n",
    "    def to_knowledge_graph(self):\n",
    "        # Convert networkx graph back to KnowledgeGraph object\n",
    "        kg = KnowledgeGraph()\n",
    "        \n",
    "        # Extract node data\n",
    "        for node_id in self.graph.nodes:\n",
    "            node_data = self.graph.nodes[node_id].copy()\n",
    "            \n",
    "            # Get related facts from edges\n",
    "            related_facts = [target for source, target in self.graph.out_edges(node_id)]\n",
    "            node_data['related_facts'] = related_facts\n",
    "            \n",
    "            # Add to knowledge graph\n",
    "            kg.data.append(node_data)\n",
    "        \n",
    "        return kg\n",
    "\n",
    "# Example usage\n",
    "portable_kg = KnowledgeGraphPortable(reduced_kg)\n",
    "json_data = portable_kg.to_json()\n",
    "\n",
    "# Save to file\n",
    "with open('portable_knowledge_graph.json', 'w') as f:\n",
    "    f.write(json_data)\n",
    "print(\"Portable knowledge graph saved to portable_knowledge_graph.json\")\n",
    "\n",
    "# Example of loading back\n",
    "new_portable_kg = KnowledgeGraphPortable(KnowledgeGraph())\n",
    "with open('portable_knowledge_graph.json', 'r') as f:\n",
    "    loaded_graph = new_portable_kg.from_json(f.read())\n",
    "    \n",
    "# Convert back to KnowledgeGraph\n",
    "loaded_kg = new_portable_kg.to_knowledge_graph()\n",
    "print(f\"Loaded knowledge graph has {len(loaded_kg.data)} facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "This notebook has demonstrated an enhanced approach to knowledge graph construction and reduction for Civic Honors content. The key improvements include:\n",
    "\n",
    "1. **Transformer-Based Semantic Similarity**: Using sentence transformers to better capture semantic relationships between facts\n",
    "2. **Hierarchical Clustering**: Grouping similar facts together and selecting representatives\n",
    "3. **Entity Disambiguation**: Identifying when different terms refer to the same entity\n",
    "4. **Fact Importance Scoring**: Using multiple factors to determine which facts are most valuable\n",
    "5. **Knowledge Graph Embeddings**: Representing entities and relations in a low-dimensional vector space\n",
    "\n",
    "These improvements result in a more effective knowledge reduction process that preserves the most important information while significantly reducing redundancy. The enhanced approach provides better semantic understanding and more nuanced fact prioritization compared to the previous version.\n",
    "\n",
    "The final knowledge graph contains a concise set of high-quality facts that capture the essential information about Civic Honors, making it more useful for applications like question answering, recommendation systems, and information retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
