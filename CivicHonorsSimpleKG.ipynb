{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+060Ax9bHKBl9o3bVeITp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/CivicHonorsSimpleKG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Overview"
      ],
      "metadata": {
        "id": "BaS8_jVHBUiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code is designed to fetch content from a specified webpage, process this content to extract meaningful information using natural language processing (NLP) techniques, and then organize this information into a complex knowledge graph. Here's a summary of each part of the code and its functionality:\n",
        "\n",
        "1. **Installing and Importing Libraries**:\n",
        "    - The code begins with the installation of necessary Python libraries (`nltk`, `spacy`, `textblob`) in a Google Colab environment, followed by importing other essential libraries (`requests`, `bs4`, `networkx`, `json`, `string`, `Counter` from `collections`).\n",
        "    - `nltk` and `spacy` are used for NLP tasks like tokenization, stopwords removal, and entity recognition. `textblob` is used for sentiment analysis.\n",
        "    - NetworkX is used for creating and manipulating the knowledge graph structure.\n",
        "\n",
        "2. **Function Definitions**:\n",
        "    - `fetch_webpage_content(url)`: Fetches the content of a webpage given its URL. It uses the `requests` library and handles HTTP responses.\n",
        "    - `parse_webpage_content(content)`: Parses the fetched content using `BeautifulSoup` to extract text. It then uses `spacy` for entity recognition, `TextBlob` for sentiment analysis, and `nltk` for keyword extraction. The function returns entities, overall sentiment, and the most common keywords.\n",
        "    - `create_knowledge_graph(entities, sentiment, keyword_freq)`: Creates a knowledge graph using NetworkX. This graph includes nodes for identified entities and keywords, and stores the overall sentiment of the text. The logic for linking (creating edges between) these nodes can be customized based on specific criteria.\n",
        "    - `save_knowledge_graph(graph, file_name)`: Saves the created knowledge graph into a JSON file, making it portable and easy to access later.\n",
        "\n",
        "3. **Execution Flow**:\n",
        "    - The process begins by fetching the webpage content using the specified URL.\n",
        "    - The fetched content is then parsed to extract entities, sentiment, and keywords.\n",
        "    - A knowledge graph is created using this extracted information, where entities and keywords become nodes, and their potential relationships are represented as edges.\n",
        "    - Finally, the knowledge graph is saved as a JSON file, which can be used for further analysis or visualization.\n",
        "\n",
        "This code is structured to run in a Google Colab environment, allowing for step-by-step execution and observation. It demonstrates the integration of web scraping, NLP, and graph theory to create a structured and meaningful representation of webpage content."
      ],
      "metadata": {
        "id": "JKVHa6gdBLjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Additional Libraries"
      ],
      "metadata": {
        "id": "5dwNGNJYANUQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5TjsLVnALTU",
        "outputId": "22ce80f9-11aa-4c96-c568-4a7689b49c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk spacy textblob\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import networkx as nx\n",
        "import json\n",
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Define Enhanced Functions"
      ],
      "metadata": {
        "id": "v_AV-Kl-AQcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_webpage_content(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        raise Exception(f\"Failed to fetch webpage content. Status Code: {response.status_code}\")\n",
        "\n",
        "def parse_webpage_content(content):\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "    text_content = soup.get_text()\n",
        "\n",
        "    doc = nlp(text_content)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    blob = TextBlob(text_content)\n",
        "    sentiment = blob.sentiment\n",
        "\n",
        "    words = [word.lower() for word in nltk.word_tokenize(text_content)\n",
        "             if word.lower() not in stopwords.words('english')\n",
        "             and word not in string.punctuation]\n",
        "    keyword_freq = Counter(words).most_common(10)\n",
        "\n",
        "    return entities, sentiment, keyword_freq\n",
        "\n",
        "def create_knowledge_graph(entities, sentiment, keyword_freq):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for entity, type in entities:\n",
        "        G.add_node(entity, type=type)\n",
        "\n",
        "    for word, freq in keyword_freq:\n",
        "        G.add_node(word, type='keyword', frequency=freq)\n",
        "\n",
        "    G.graph['sentiment'] = sentiment\n",
        "\n",
        "    # Define logic for creating edges based on your criteria\n",
        "\n",
        "    return G\n",
        "\n",
        "def save_knowledge_graph(graph, file_name):\n",
        "    data = nx.readwrite.json_graph.node_link_data(graph)\n",
        "    with open(file_name, 'w') as file:\n",
        "        json.dump(data, file)"
      ],
      "metadata": {
        "id": "CNx5jLOiARDd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Execute the Process"
      ],
      "metadata": {
        "id": "ApBtE-4qAWpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch Webpage Content"
      ],
      "metadata": {
        "id": "IDC3kAHkAY7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://civichonors.com/\"\n",
        "try:\n",
        "    content = fetch_webpage_content(url)\n",
        "    print(\"Webpage content fetched successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching webpage: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FzVVt0DAaI1",
        "outputId": "12b4f4a1-9d5c-4226-d07b-131ed28e1a49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webpage content fetched successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse Webpage and Create Knowledge Graph"
      ],
      "metadata": {
        "id": "MSX-rA4oAbr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    entities, sentiment, keyword_freq = parse_webpage_content(content)\n",
        "    graph = create_knowledge_graph(entities, sentiment, keyword_freq)\n",
        "    print(\"Knowledge graph created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in parsing and graph creation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MscxTo2MAb_s",
        "outputId": "db59ce99-5298-4d16-f70d-9dddba1f917c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge graph created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the Knowledge Graph"
      ],
      "metadata": {
        "id": "aISBA7lqAfNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    save_knowledge_graph(graph, '/content/knowledge_graph.json')\n",
        "    print(\"Knowledge graph saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving knowledge graph: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ici63BQ9Agb3",
        "outputId": "f72a8d1e-1cc6-46f7-cfae-97c7aa091dff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge graph saved successfully.\n"
          ]
        }
      ]
    }
  ]
}