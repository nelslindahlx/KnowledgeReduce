{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/CivicHonorsKGv17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLVuQtowdT4j"
      },
      "source": [
        "# Code Summary\n",
        "\n",
        "This notebook encapsulates an end-to-end pipeline designed for the meticulous extraction, processing, and systematic organization of data from specified web sources into a well-structured knowledge graph. The workflow is outlined in detailed steps, each contributing to the creation of a refined and informative dataset:\n",
        "\n",
        "### Step 1: Install Necessary Libraries\n",
        "- Installation of pivotal Python libraries such as `requests` for web scraping, `beautifulsoup4` for HTML parsing, `networkx` for graph-based operations, and `spacy` for advanced natural language processing tasks.\n",
        "\n",
        "### Step 2: Import Libraries and Define Classes\n",
        "- Incorporation of essential Python modules and the establishment of two foundational classes:\n",
        "  - `ReliabilityRating`: An enumeration (Enum) for classifying the reliability of the extracted information.\n",
        "  - `KnowledgeGraph`: A versatile class for constructing and managing a knowledge graph, encompassing functionalities for adding facts, computing and updating quality scores, and fact retrieval.\n",
        "\n",
        "### Step 3: Scrape Data from Websites\n",
        "- Automated retrieval and parsing of HTML content from two distinct websites, methodically extracting textual data from common HTML elements like paragraphs, headings, and lists.\n",
        "\n",
        "### Step 4: Populate the KnowledgeGraph\n",
        "- Initialization and population of the `KnowledgeGraph` instance.\n",
        "- Systematic incorporation of each fact into the graph, complete with comprehensive attributes including statement, category, tags, and more.\n",
        "\n",
        "### Step 5: Display Extracted Facts\n",
        "- Presentation of an initial subset of facts from the knowledge graph, providing insights into the nature of the extracted data and a count of the total facts obtained.\n",
        "\n",
        "### Step 6: Ensure Data Uniqueness\n",
        "- A focused effort to eliminate duplicate facts from the knowledge graph based on their statements, ensuring the uniqueness and quality of the dataset.\n",
        "\n",
        "### Step 7: Implement Advanced Data Cleaning\n",
        "- Advanced data cleaning procedures, including the removal of overly brief facts and deduplication of semantically similar facts through basic string comparison techniques.\n",
        "\n",
        "### Step 8: Execute Super Aggressive Advanced Cleaning\n",
        "- Application of the `en_core_web_md` spaCy model for sophisticated NLP operations.\n",
        "- Removal of facts based on complex criteria like semantic similarity, leveraging NLP techniques for enhanced dataset refinement.\n",
        "\n",
        "### Step 9: Serialize the KnowledgeGraph for Portability\n",
        "- A crucial step focusing on rendering the Knowledge Graph portable via serialization into JSON format, facilitating easy storage, transfer, and reconstruction across different environments. The process encompasses both serialization and deserialization functions, updated to include sharding for every 100 records.\n",
        "\n",
        "### Step 10: List JSON Files\n",
        "- A utility step for listing available JSON files, ensuring accessibility and management of serialized data.\n",
        "\n",
        "This comprehensive pipeline effectively transforms web-sourced information into a structured, clean, and semantically rich knowledge graph dataset. It is particularly suited for scenarios demanding high data quality and structure, enriched with advanced NLP techniques to ensure a dataset devoid of redundancies and abundant in diverse, valuable insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61hozs7HZsLB"
      },
      "source": [
        "# Step 1: Install Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "clRygd5kZpFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad73a443-76d4-438e-d324-6520f8e593f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: community 1.0.0b1\n",
            "Uninstalling community-1.0.0b1:\n",
            "  Successfully uninstalled community-1.0.0b1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.10/dist-packages (0.16)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2024-01-23 03:51:25.441536: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 03:51:25.441622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 03:51:25.443363: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-23 03:51:25.454321: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-23 03:51:27.672522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "# Step to install necessary libraries\n",
        "!pip uninstall community -y\n",
        "!pip install requests beautifulsoup4 networkx spacy python-louvain\n",
        "!python -m spacy download en_core_web_md\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD_VIFu-Zv2S"
      },
      "source": [
        "# Step 2: Import Libraries and Define Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mw1-i2ZzZ1JH"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import requests\n",
        "import difflib\n",
        "import spacy\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import community as community_louvain  # python-louvain library\n",
        "\n",
        "# Running the proper from statements\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from enum import Enum\n",
        "\n",
        "# Define the classes\n",
        "class ReliabilityRating(Enum):\n",
        "    UNVERIFIED = 1\n",
        "    POSSIBLY_TRUE = 2\n",
        "    LIKELY_TRUE = 3\n",
        "    VERIFIED = 4\n",
        "\n",
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.DiGraph()\n",
        "\n",
        "    def calculate_quality_score(self, reliability_rating, usage_count):\n",
        "        # Adjusted to handle string representation of Enum\n",
        "        rating_value = ReliabilityRating[reliability_rating].value if isinstance(reliability_rating, str) else reliability_rating.value\n",
        "        base_score = 10 * rating_value\n",
        "        usage_bonus = 2 * usage_count\n",
        "        return base_score + usage_bonus\n",
        "\n",
        "    def add_fact(self, fact_id, fact_statement, category, tags, date_recorded, last_updated,\n",
        "                 reliability_rating, source_id, source_title, author_creator,\n",
        "                 publication_date, url_reference, related_facts, contextual_notes,\n",
        "                 access_level, usage_count):\n",
        "        # Convert list and datetime objects to strings\n",
        "        tags_str = ', '.join(tags) if tags else ''\n",
        "        date_recorded_str = date_recorded.isoformat() if isinstance(date_recorded, datetime) else date_recorded\n",
        "        last_updated_str = last_updated.isoformat() if isinstance(last_updated, datetime) else last_updated\n",
        "        publication_date_str = publication_date.isoformat() if isinstance(publication_date, datetime) else publication_date\n",
        "\n",
        "        quality_score = self.calculate_quality_score(reliability_rating, usage_count)\n",
        "        self.graph.add_node(fact_id,\n",
        "                            fact_statement=fact_statement,\n",
        "                            category=category,\n",
        "                            tags=tags_str,\n",
        "                            date_recorded=date_recorded_str,\n",
        "                            last_updated=last_updated_str,\n",
        "                            reliability_rating=reliability_rating,\n",
        "                            quality_score=quality_score,\n",
        "                            source_id=source_id,\n",
        "                            source_title=source_title,\n",
        "                            author_creator=author_creator,\n",
        "                            publication_date=publication_date_str,\n",
        "                            url_reference=url_reference,\n",
        "                            contextual_notes=contextual_notes,\n",
        "                            access_level=access_level,\n",
        "                            usage_count=usage_count)\n",
        "\n",
        "        for related_fact_id in related_facts:\n",
        "            self.graph.add_edge(fact_id, related_fact_id)\n",
        "\n",
        "    def update_quality_score(self, fact_id):\n",
        "        if fact_id not in self.graph:\n",
        "            raise ValueError(\"Fact ID not found in the graph.\")\n",
        "        fact = self.graph.nodes[fact_id]\n",
        "        new_score = self.calculate_quality_score(fact['reliability_rating'], fact['usage_count'])\n",
        "        self.graph.nodes[fact_id]['quality_score'] = new_score\n",
        "\n",
        "    def get_fact(self, fact_id):\n",
        "        if fact_id not in self.graph:\n",
        "            raise ValueError(\"Fact ID not found in the graph.\")\n",
        "        return self.graph.nodes[fact_id]\n",
        "\n",
        "    def save_to_file(self, filename):\n",
        "        facts_to_save = []\n",
        "        for fact_id, fact_data in self.graph.nodes(data=True):\n",
        "            facts_to_save.append(fact_data)\n",
        "\n",
        "        with open(filename, 'w') as file:\n",
        "            json.dump(facts_to_save, file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn6RrXg_Z55m"
      },
      "source": [
        "# Step 3: Scrape Data from the Website"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract text from a soup object\n",
        "def extract_text(element):\n",
        "    return ' '.join(element.stripped_strings)\n",
        "\n",
        "# Generic function to find facts in common HTML structures\n",
        "def find_facts(soup):\n",
        "    facts = []\n",
        "\n",
        "    # Look for paragraphs\n",
        "    for p in soup.find_all('p'):\n",
        "        text = extract_text(p)\n",
        "        if text: facts.append(text)\n",
        "\n",
        "    # Look for headings\n",
        "    for header_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
        "        for header in soup.find_all(header_tag):\n",
        "            text = extract_text(header)\n",
        "            if text: facts.append(text)\n",
        "\n",
        "    # Look for list items\n",
        "    for li in soup.find_all('li'):\n",
        "        text = extract_text(li)\n",
        "        if text: facts.append(text)\n",
        "\n",
        "    return facts\n",
        "\n",
        "# URLs of the websites to scrape\n",
        "urls = [\"https://civichonors.com/\", \"https://www.nelslindahl.com/\"]\n",
        "\n",
        "# List to hold all facts from both websites\n",
        "all_facts = []\n",
        "\n",
        "# Iterate through each URL, scrape its content, and extract facts\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        facts = find_facts(soup)\n",
        "        all_facts.extend(facts)\n",
        "    else:\n",
        "        print(f\"Failed to retrieve content from {url}\")\n",
        "\n",
        "# all_facts now contains facts from both websites\n",
        "print(f\"Total facts extracted: {len(all_facts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmLERpN5Xy9e",
        "outputId": "18686cdb-2a01-40e1-9dca-864bd7c87288"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total facts extracted: 330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOUJY03SaBwF"
      },
      "source": [
        "# Step 4: Store Extracted Data in the KnowledgeGraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enum for Reliability Rating\n",
        "class ReliabilityRating:\n",
        "    LIKELY_TRUE = 'Likely True'\n",
        "    # ... other reliability ratings ...\n",
        "\n",
        "# KnowledgeGraph class\n",
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        self.data = []\n",
        "\n",
        "    def add_fact(self, fact_id, fact_statement, category, tags, date_recorded, last_updated,\n",
        "                 reliability_rating, source_id, source_title, author_creator, publication_date,\n",
        "                 url_reference, related_facts, contextual_notes, access_level, usage_count):\n",
        "        self.data.append({\n",
        "            'fact_id': fact_id,\n",
        "            'fact_statement': fact_statement,\n",
        "            'category': category,\n",
        "            'tags': tags,\n",
        "            'date_recorded': date_recorded,\n",
        "            'last_updated': last_updated,\n",
        "            'reliability_rating': reliability_rating,\n",
        "            'source_id': source_id,\n",
        "            'source_title': source_title,\n",
        "            'author_creator': author_creator,\n",
        "            'publication_date': publication_date,\n",
        "            'url_reference': url_reference,\n",
        "            'related_facts': related_facts,\n",
        "            'contextual_notes': contextual_notes,\n",
        "            'access_level': access_level,\n",
        "            'usage_count': usage_count\n",
        "        })\n",
        "\n",
        "    def save_to_file(self, filename):\n",
        "        with open(filename, 'w') as file:\n",
        "            json.dump(self.data, file, default=str, indent=4)\n",
        "\n",
        "# Function to extract text from a soup object\n",
        "def extract_text(element):\n",
        "    return ' '.join(element.stripped_strings)\n",
        "\n",
        "# Generic function to find facts in common HTML structures\n",
        "def find_facts(soup):\n",
        "    facts = []\n",
        "    for p in soup.find_all('p'):\n",
        "        text = extract_text(p)\n",
        "        if text: facts.append(text)\n",
        "    for header_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
        "        for header in soup.find_all(header_tag):\n",
        "            text = extract_text(header)\n",
        "            if text: facts.append(text)\n",
        "    for li in soup.find_all('li'):\n",
        "        text = extract_text(li)\n",
        "        if text: facts.append(text)\n",
        "    return facts\n",
        "\n",
        "# Function to scrape a website and return its BeautifulSoup object\n",
        "def scrape_website(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return BeautifulSoup(response.text, 'html.parser')\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during requests to {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize the KnowledgeGraph\n",
        "kg = KnowledgeGraph()\n",
        "\n",
        "# URLs of the websites to scrape\n",
        "urls = {\n",
        "    \"CivicHonors\": \"https://civichonors.com/\",\n",
        "    \"NelsLindahl\": \"https://www.nelslindahl.com/\"\n",
        "}\n",
        "\n",
        "# Add facts from each website to the KnowledgeGraph\n",
        "for source_id, url in urls.items():\n",
        "    soup = scrape_website(url)\n",
        "    if soup:\n",
        "        facts = find_facts(soup)\n",
        "        for i, fact in enumerate(facts):\n",
        "            kg.add_fact(\n",
        "                fact_id=f\"{source_id}_{i}\",\n",
        "                fact_statement=fact,\n",
        "                category=\"General\",\n",
        "                tags=[source_id, \"WebScraped\"],\n",
        "                date_recorded=datetime.now(),\n",
        "                last_updated=datetime.now(),\n",
        "                reliability_rating=ReliabilityRating.LIKELY_TRUE,\n",
        "                source_id=source_id,\n",
        "                source_title=f\"{source_id} Website\",\n",
        "                author_creator=\"Web Scraping\",\n",
        "                publication_date=datetime.now(),\n",
        "                url_reference=url,\n",
        "                related_facts=[],\n",
        "                contextual_notes=f\"Extracted from {source_id} website\",\n",
        "                access_level=\"Public\",\n",
        "                usage_count=0\n",
        "            )\n",
        "\n",
        "# Save the facts to a file\n",
        "filename = 'knowledge_graph_facts.json'\n",
        "kg.save_to_file(filename)\n",
        "print(f\"Facts saved to {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSkpzay9ZDAd",
        "outputId": "7b5630bd-b287-4aa3-eb2b-eac16347916c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Facts saved to knowledge_graph_facts.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S572InVIaIlk"
      },
      "source": [
        "# Step 5: Retrieve and Display 10 Facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8GhKDJF9aMjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94aab15-9b89-4a9d-cf87-875e439d4298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total facts extracted: 330\n",
            "Fact 1: Civic Honors\n",
            "Fact 2: Graduation with Civic Honors: Unlock the Power of  Community Opportunity\n",
            "Fact 3: This book was published in 2006. It has a formal copyright. You can buy a physical copy if you want or just read it online here. I was going to update this to be an online PDF, but it seemed like a better idea to just make it a very large single page of prose. Enjoy!\n",
            "Fact 4: Dr. Nels Lindahl, June 14, 2020, Denver, Colorado\n",
            "Fact 5: Believing in a dream like graduation with civic honors is only the first step in the process toward advocating the creation of a graduation with civic honors program.\n",
            "Fact 6: The story behind graduation with civic honors began in an academic setting during the spring semester of 2002 at the University of Kansas. I took a class from Dr. H. George Frederickson entitled Concepts of Civil Society. During the Concepts of Civil Society class, my collegiate interests focused on civic engagement. At one point during the class, Dr. Frederickson posed a question to the class about what colleges could do to get people involved within the community. Over the course of the next year thinking about the idea of civil society, I became interested in pursuing a degree in the field of public administration. Thanks to the faculty of the Public Administration Department at the University of Kansas, I decided to endeavor to enter graduate school. I would like to thank Dr. Raymond Davis for advice and guidance, Dr. Thomas Longoria for defining the importance of collaboration, and Dr. H. George Frederickson for a thoughtful introduction to the world of civil society.\n",
            "Fact 7: Graduation with civic honors was only a dream until late in 2002. Dr. Charles J. Carlsen, the president of Johnson County Community College, learned of the idea from Susan Lindahl and, along with the board of trustees and civic honors steering committee, envisioned becoming the first community college in the state of Kansas to designate a graduation with civic honors. In 2004, after Johnson County Community College developed the initial graduation with civic honors pilot program, the next step was to start spreading the graduation with civic honors message. I was honored to submit an article with Dr. Charles J. Carlsen and Susan Lindahl entitled, “Civic Honors Program at Johnson County Community College,” to the Journal for Civic Commitment for publication. The resulting publication (Carlsen, Lindahl, & Lindahl, 2004) was the first step toward globally sharing the positive message of graduation with civic honors.\n",
            "Fact 8: To realize the dream, students actually would have to graduate with civic honors. During the May 2005 graduation at Johnson County Community College, the dream became a reality when four students—including Deborah DeGrate, Carrie Donham, Chris Engle, and Jennifer Pittman-Leeper—were the first to graduate with civic honors. The story will be complete when students all over the world are graduating with civic honors.\n",
            "Fact 9: Society has experienced a true revival of public interest in civic engagement.\n",
            "Fact 10: Robert Putnam’s (2000) epic work Bowling Alone brought the idea of civic participation to the forefront of the public mind. The value of civic participation is essential to the process of value implementation in the form of civic engagement. Civic engagement is a value choice, and the implementation of that value choice is individual civic participation in the community. One of the most basic definitions of civic engagement involves thinking about how government, society, and citizens interact. In terms of how scholars discuss civic engagement, the definition takes on a benevolent feel, referencing citizen activities that benefit civil society. Strengthening civic engagement, in practice, involves building civic skills, increasing active voter participation, and using public service announcements to encourage volunteering to strengthen civil society. Watershed events like 9-11 have created a reflective sense of national interest and significantly increased the willingness of individuals to participate in civic engagement.\n"
          ]
        }
      ],
      "source": [
        "# Print the total number of facts extracted\n",
        "print(f\"Total facts extracted: {len(kg.data)}\")\n",
        "\n",
        "# Display the first 10 facts, if available\n",
        "for i in range(min(10, len(kg.data))):\n",
        "    fact = kg.data[i]['fact_statement']  # Access the fact statement directly from the data list\n",
        "    print(f\"Fact {i+1}: {fact}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceqgxEGxUxdM"
      },
      "source": [
        "# Step 6: Ensure Uniqueness of Facts in the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate_facts(knowledge_graph):\n",
        "    unique_facts = set()\n",
        "    unique_data = []\n",
        "\n",
        "    for fact_data in knowledge_graph.data:\n",
        "        fact_statement = fact_data['fact_statement']\n",
        "        if fact_statement not in unique_facts:\n",
        "            unique_facts.add(fact_statement)\n",
        "            unique_data.append(fact_data)\n",
        "\n",
        "    # Replace the original data with the unique data\n",
        "    knowledge_graph.data = unique_data\n",
        "\n",
        "# Call the function to remove duplicate facts\n",
        "remove_duplicate_facts(kg)\n",
        "\n",
        "# Optional: Print the total number of unique facts remaining\n",
        "print(f\"Total unique facts remaining: {len(kg.data)}\")\n",
        "\n",
        "# Save the facts to a file\n",
        "filename = 'unique_knowledge_graph_facts.json'\n",
        "kg.save_to_file(filename)\n",
        "print(f\"Facts saved to {filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unUjRSvCaRBH",
        "outputId": "2ae363f4-667f-4f40-9fe5-842277629905"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique facts remaining: 329\n",
            "Facts saved to unique_knowledge_graph_facts.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0ZkAAb8VCBh"
      },
      "source": [
        "# Step 7: Advanced Cleaning and Combining of Facts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def advanced_cleaning(knowledge_graph, similarity_threshold=0.8, short_fact_threshold=50):\n",
        "    # Remove short facts\n",
        "    knowledge_graph.data = [fact for fact in knowledge_graph.data if len(fact['fact_statement']) >= short_fact_threshold]\n",
        "\n",
        "    # Remove similar facts\n",
        "    unique_facts = []\n",
        "    for fact in knowledge_graph.data:\n",
        "        if not any(SequenceMatcher(None, f['fact_statement'], fact['fact_statement']).ratio() > similarity_threshold for f in unique_facts):\n",
        "            unique_facts.append(fact)\n",
        "\n",
        "    knowledge_graph.data = unique_facts\n",
        "\n",
        "# Call the function for advanced cleaning\n",
        "advanced_cleaning(kg)\n",
        "\n",
        "# Optional: Print the total number of facts after cleaning\n",
        "print(f\"Total facts after advanced cleaning: {len(kg.data)}\")\n",
        "\n",
        "# Save the facts to a file\n",
        "filename = 'advanced_knowledge_graph_facts.json'\n",
        "kg.save_to_file(filename)\n",
        "print(f\"Facts saved to {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEZ-NhKKaljQ",
        "outputId": "650d9591-9c4d-4d85-f040-496798d158d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total facts after advanced cleaning: 270\n",
            "Facts saved to advanced_knowledge_graph_facts.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EJymWj3Yv75"
      },
      "source": [
        "# Step 8: Super Aggressive Advanced Cleaning (Refined)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model (make sure to have it installed)\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def super_aggressive_cleaning(knowledge_graph, similarity_threshold=0.85):\n",
        "    processed_facts = []\n",
        "    unique_facts = []\n",
        "\n",
        "    # Pre-process each fact with spaCy\n",
        "    for fact in knowledge_graph.data:\n",
        "        doc = nlp(fact['fact_statement'])\n",
        "        processed_facts.append((fact, doc))\n",
        "\n",
        "    # Compare each fact to others for similarity\n",
        "    for fact, doc in processed_facts:\n",
        "        if not any(doc.similarity(other_doc) > similarity_threshold for _, other_doc in processed_facts if other_doc != doc):\n",
        "            unique_facts.append(fact)\n",
        "\n",
        "    knowledge_graph.data = unique_facts\n",
        "\n",
        "# Call the function for super aggressive cleaning\n",
        "super_aggressive_cleaning(kg)\n",
        "\n",
        "# Print the total number of facts after super aggressive cleaning\n",
        "print(f\"Total facts after super aggressive cleaning: {len(kg.data)}\")\n",
        "\n",
        "# Save the facts to a file\n",
        "filename = 'remaining_facts.json'\n",
        "kg.save_to_file(filename)\n",
        "print(f\"Facts saved to {filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsnqDf9xbQG3",
        "outputId": "048e67a7-4c77-4164-cc26-aec004494196"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total facts after super aggressive cleaning: 8\n",
            "Facts saved to remaining_facts.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Serialization and deserialization of the KnowledgeGraph for portability"
      ],
      "metadata": {
        "id": "nChx3f9FAFme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import networkx as nx\n",
        "\n",
        "class KnowledgeGraphPortable:\n",
        "    def __init__(self, knowledge_graph):\n",
        "        # Check if knowledge_graph is a networkx graph or a list-based structure\n",
        "        if isinstance(knowledge_graph, (nx.Graph, nx.DiGraph, nx.MultiGraph, nx.MultiDiGraph)):\n",
        "            self.graph = knowledge_graph\n",
        "        elif hasattr(knowledge_graph, 'data') and isinstance(knowledge_graph.data, list):\n",
        "            self.graph = self.convert_list_to_graph(knowledge_graph.data)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported knowledge_graph structure\")\n",
        "\n",
        "    def convert_list_to_graph(self, data_list):\n",
        "        # Convert a list of facts to a networkx graph (if necessary)\n",
        "        G = nx.DiGraph()\n",
        "        for item in data_list:\n",
        "            # Assuming each item has 'fact_id' and 'fact_statement'\n",
        "            G.add_node(item['fact_id'], **item)\n",
        "        return G\n",
        "\n",
        "    def serialize_graph_to_json(self, output_file):\n",
        "        # Convert graph to a dictionary or suitable structure\n",
        "        try:\n",
        "            graph_data = nx.node_link_data(self.graph)\n",
        "            with open(output_file, 'w') as file:\n",
        "                json.dump(graph_data, file, indent=4)\n",
        "            print(f\"Graph serialized to JSON. File saved as {output_file}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error in serializing graph to JSON: {e}\")\n",
        "            return False\n",
        "\n",
        "# Usage example\n",
        "kg_portable = KnowledgeGraphPortable(kg)\n",
        "output_file = 'serialized_kg.json'  # Specify the filename for the serialized graph\n",
        "result = kg_portable.serialize_graph_to_json(output_file)\n",
        "\n",
        "if result:\n",
        "    print(\"Serialization of Knowledge Graph completed successfully.\")\n",
        "else:\n",
        "    print(\"Serialization of Knowledge Graph failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8UErTeGb3F6",
        "outputId": "31633e97-ad6d-46e5-a111-7dad754701b0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in serializing graph to JSON: Object of type datetime is not JSON serializable\n",
            "Serialization of Knowledge Graph failed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Print a list of JSON files"
      ],
      "metadata": {
        "id": "4MBp9k0JAiXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory to search for JSON files, '.' for current directory\n",
        "directory_to_search = '.'\n",
        "\n",
        "# List all JSON files in the specified directory\n",
        "json_files = glob.glob(os.path.join(directory_to_search, '*.json'))\n",
        "\n",
        "# Print the list of JSON files\n",
        "print(\"List of JSON files created:\")\n",
        "for file in json_files:\n",
        "    print(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rsodDchAnVH",
        "outputId": "1a1c940d-ac99-4483-d6d3-233b2b43a350"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of JSON files created:\n",
            "./serialized_kg.json\n",
            "./knowledge_graph_facts.json\n",
            "./advanced_knowledge_graph_facts.json\n",
            "./remaining_facts.json\n",
            "./unique_knowledge_graph_facts.json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}