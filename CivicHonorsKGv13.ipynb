{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/CivicHonorsKGv13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLVuQtowdT4j"
      },
      "source": [
        "# Code Summary\n",
        "\n",
        "The code developed for execution in Google Colab represents a comprehensive workflow for extracting, processing, and organizing data from a website into a knowledge graph. Here's a summary of each step in the code:\n",
        "\n",
        "### Step 1: Install Necessary Libraries\n",
        "- Installs the required Python libraries: `requests` for web scraping, `beautifulsoup4` for HTML parsing, `networkx` for graph operations, and `spacy` for natural language processing.\n",
        "\n",
        "### Step 2: Import Libraries and Define Classes\n",
        "- Imports essential Python modules and defines two classes:\n",
        "  - `ReliabilityRating`: An enumeration (Enum) for categorizing the reliability of facts.\n",
        "  - `KnowledgeGraph`: A class for creating and managing a knowledge graph, including methods for adding facts, calculating quality scores, updating these scores, and retrieving facts.\n",
        "\n",
        "### Step 3: Scrape Data from the Website\n",
        "- Fetches and parses the HTML content of a specified website, extracting text data from common HTML elements like paragraphs, headings, and lists.\n",
        "\n",
        "### Step 4: Store Extracted Data in the KnowledgeGraph\n",
        "- Initializes an instance of the `KnowledgeGraph`.\n",
        "- Stores each extracted fact as a node in the graph with attributes such as fact statement, category, tags, etc.\n",
        "\n",
        "### Step 5: Retrieve and Display 10 Facts\n",
        "- Displays the first 10 facts from the knowledge graph, giving an overview of the extracted data, and prints the total number of facts extracted.\n",
        "\n",
        "### Step 6: Store the KnowledgeGraph in GEXF and GraphML Formats\n",
        "- Saves the graph in GEXF and GraphML file formats, suitable for analysis and visualization in various graph analysis tools.\n",
        "\n",
        "### Step 7: Ensure Uniqueness of Facts in the Dataset\n",
        "- Removes duplicate facts from the knowledge graph based on their statements to maintain data uniqueness.\n",
        "\n",
        "### Step 8: Advanced Cleaning and Combining of Facts\n",
        "- Implements advanced cleaning techniques, such as removing very short facts and deduplicating semantically similar facts using basic string comparison.\n",
        "\n",
        "### Step 9: Super Aggressive Advanced Cleaning (Refined)\n",
        "- Employs the `en_core_web_md` spaCy model for NLP tasks.\n",
        "- Removes facts based on complex criteria, like semantic similarity, using NLP techniques to refine the dataset's quality further.\n",
        "\n",
        "### Step 10: Store the Updated KnowledgeGraph in GEXF and GraphML Formats\n",
        "- Saves the graph again in GEXF and GraphML file formats.\n",
        "\n",
        "### Step 11: Serialization and deserialization of the KnowledgeGraph for portability.\n",
        "- This step focuses on making the Knowledge Graph portable by serializing it into a JSON format. This allows for easy storage, transfer, and reconstruction of the graph in different environments. The process includes functions for both serialization and deserialization of the graph.\n",
        "\n",
        "### Step 12: Graph Analysis Features\n",
        "- This step focuses on adding graph analysis features to the Knowledge Graph. It includes functions for calculating various centrality measures, community detection, and other relevant graph analysis metrics. These features can provide valuable insights into the structure and characteristics of the graph.\n",
        "\n",
        "This complete workflow effectively extracts information from a website, transforming it into a structured and clean dataset within a knowledge graph. It's particularly useful in scenarios where data quality, structure, and semantic richness are crucial. The inclusion of advanced NLP techniques in the later steps ensures a high-quality dataset, free from redundancies and rich in diverse information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61hozs7HZsLB"
      },
      "source": [
        "# Step 1: Install Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "clRygd5kZpFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cfeef1a-9ff9-4578-cc08-779675a2253d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: community 1.0.0b1\n",
            "Uninstalling community-1.0.0b1:\n",
            "  Successfully uninstalled community-1.0.0b1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.10/dist-packages (0.16)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2024-01-17 14:18:42.097805: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-17 14:18:42.097864: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-17 14:18:42.099734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-17 14:18:42.108171: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-17 14:18:43.381153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "source": [
        "# Completeness check passed\n",
        "!pip uninstall community -y\n",
        "!pip install requests beautifulsoup4 networkx spacy python-louvain\n",
        "!python -m spacy download en_core_web_md\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD_VIFu-Zv2S"
      },
      "source": [
        "# Step 2: Import Libraries and Define Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mw1-i2ZzZ1JH"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "# Importing necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import networkx as nx\n",
        "from datetime import datetime\n",
        "from enum import Enum\n",
        "import difflib\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import community as community_louvain  # python-louvain library\n",
        "\n",
        "class ReliabilityRating(Enum):\n",
        "    UNVERIFIED = 1\n",
        "    POSSIBLY_TRUE = 2\n",
        "    LIKELY_TRUE = 3\n",
        "    VERIFIED = 4\n",
        "\n",
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.DiGraph()\n",
        "\n",
        "    def calculate_quality_score(self, reliability_rating, usage_count):\n",
        "        # Adjusted to handle string representation of Enum\n",
        "        rating_value = ReliabilityRating[reliability_rating].value if isinstance(reliability_rating, str) else reliability_rating.value\n",
        "        base_score = 10 * rating_value\n",
        "        usage_bonus = 2 * usage_count\n",
        "        return base_score + usage_bonus\n",
        "\n",
        "    def add_fact(self, fact_id, fact_statement, category, tags, date_recorded, last_updated,\n",
        "                 reliability_rating, source_id, source_title, author_creator,\n",
        "                 publication_date, url_reference, related_facts, contextual_notes,\n",
        "                 access_level, usage_count):\n",
        "        # Convert list and datetime objects to strings\n",
        "        tags_str = ', '.join(tags) if tags else ''\n",
        "        date_recorded_str = date_recorded.isoformat() if isinstance(date_recorded, datetime) else date_recorded\n",
        "        last_updated_str = last_updated.isoformat() if isinstance(last_updated, datetime) else last_updated\n",
        "        publication_date_str = publication_date.isoformat() if isinstance(publication_date, datetime) else publication_date\n",
        "\n",
        "        quality_score = self.calculate_quality_score(reliability_rating, usage_count)\n",
        "        self.graph.add_node(fact_id,\n",
        "                            fact_statement=fact_statement,\n",
        "                            category=category,\n",
        "                            tags=tags_str,\n",
        "                            date_recorded=date_recorded_str,\n",
        "                            last_updated=last_updated_str,\n",
        "                            reliability_rating=reliability_rating,\n",
        "                            quality_score=quality_score,\n",
        "                            source_id=source_id,\n",
        "                            source_title=source_title,\n",
        "                            author_creator=author_creator,\n",
        "                            publication_date=publication_date_str,\n",
        "                            url_reference=url_reference,\n",
        "                            contextual_notes=contextual_notes,\n",
        "                            access_level=access_level,\n",
        "                            usage_count=usage_count)\n",
        "\n",
        "        for related_fact_id in related_facts:\n",
        "            self.graph.add_edge(fact_id, related_fact_id)\n",
        "\n",
        "    def update_quality_score(self, fact_id):\n",
        "        if fact_id not in self.graph:\n",
        "            raise ValueError(\"Fact ID not found in the graph.\")\n",
        "        fact = self.graph.nodes[fact_id]\n",
        "        new_score = self.calculate_quality_score(fact['reliability_rating'], fact['usage_count'])\n",
        "        self.graph.nodes[fact_id]['quality_score'] = new_score\n",
        "\n",
        "    def get_fact(self, fact_id):\n",
        "        if fact_id not in self.graph:\n",
        "            raise ValueError(\"Fact ID not found in the graph.\")\n",
        "        return self.graph.nodes[fact_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn6RrXg_Z55m"
      },
      "source": [
        "# Step 3: Scrape Data from the Website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "neHkbBnWZ_8K"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "# Importing necessary libraries\n",
        "# Fetch data from the website\n",
        "url = \"https://civichonors.com/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Function to extract text from a soup object\n",
        "def extract_text(element):\n",
        "    return ' '.join(element.stripped_strings)\n",
        "\n",
        "# Generic function to find facts in common HTML structures\n",
        "def find_facts(soup):\n",
        "    facts = []\n",
        "\n",
        "    # Look for paragraphs\n",
        "    for p in soup.find_all('p'):\n",
        "        text = extract_text(p)\n",
        "        if text: facts.append(text)\n",
        "\n",
        "    # Look for headings\n",
        "    for header_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
        "        for header in soup.find_all(header_tag):\n",
        "            text = extract_text(header)\n",
        "            if text: facts.append(text)\n",
        "\n",
        "    # Look for list items\n",
        "    for li in soup.find_all('li'):\n",
        "        text = extract_text(li)\n",
        "        if text: facts.append(text)\n",
        "\n",
        "    return facts\n",
        "\n",
        "# Extract facts using the generic function\n",
        "facts = find_facts(soup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOUJY03SaBwF"
      },
      "source": [
        "# Step 4: Store Extracted Data in the KnowledgeGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GjxxQZSeaHeR"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "# Importing necessary libraries\n",
        "# Initialize the KnowledgeGraph\n",
        "kg = KnowledgeGraph()\n",
        "\n",
        "# Store each fact in the KnowledgeGraph\n",
        "for i, fact in enumerate(facts):\n",
        "    kg.add_fact(\n",
        "        fact_id=str(i),\n",
        "        fact_statement=fact,\n",
        "        category=\"General\",\n",
        "        tags=[\"CivicHonors\", \"WebScraped\"],\n",
        "        date_recorded=datetime.now(),\n",
        "        last_updated=datetime.now(),\n",
        "        reliability_rating=ReliabilityRating.LIKELY_TRUE.name,\n",
        "        source_id=\"CivicHonors\",\n",
        "        source_title=\"Civic Honors Website\",\n",
        "        author_creator=\"Web Scraping\",\n",
        "        publication_date=datetime.now(),\n",
        "        url_reference=url,\n",
        "        related_facts=[],\n",
        "        contextual_notes=\"Extracted from Civic Honors website\",\n",
        "        access_level=\"Public\",\n",
        "        usage_count=0\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S572InVIaIlk"
      },
      "source": [
        "# Step 5: Retrieve and Display 10 Facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8GhKDJF9aMjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f0bc7f-9a02-41ce-8ee2-570344c39275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total facts extracted: 310\n",
            "Fact 1: {'fact_statement': 'Civic Honors', 'category': 'General', 'tags': 'CivicHonors, WebScraped', 'date_recorded': '2024-01-17T14:19:15.018799', 'last_updated': '2024-01-17T14:19:15.018803', 'reliability_rating': 'LIKELY_TRUE', 'quality_score': 30, 'source_id': 'CivicHonors', 'source_title': 'Civic Honors Website', 'author_creator': 'Web Scraping', 'publication_date': '2024-01-17T14:19:15.018812', 'url_reference': 'https://civichonors.com/', 'contextual_notes': 'Extracted from Civic Honors website', 'access_level': 'Public', 'usage_count': 0}\n",
            "Fact 2: {'fact_statement': 'Graduation with Civic Honors: Unlock the Power of  Community Opportunity', 'category': 'General', 'tags': 'CivicHonors, WebScraped', 'date_recorded': '2024-01-17T14:19:15.019569', 'last_updated': '2024-01-17T14:19:15.019573', 'reliability_rating': 'LIKELY_TRUE', 'quality_score': 30, 'source_id': 'CivicHonors', 'source_title': 'Civic Honors Website', 'author_creator': 'Web Scraping', 'publication_date': '2024-01-17T14:19:15.019580', 'url_reference': 'https://civichonors.com/', 'contextual_notes': 'Extracted from Civic Honors website', 'access_level': 'Public', 'usage_count': 0}\n",
            "Fact 3: {'fact_statement': 'This book was published in 2006. It has a formal copyright. You can buy a physical copy if you want or just read it online here. I was going to update this to be an online PDF, but it seemed like a better idea to just make it a very large single page of prose. Enjoy!', 'category': 'General', 'tags': 'CivicHonors, WebScraped', 'date_recorded': '2024-01-17T14:19:15.019618', 'last_updated': '2024-01-17T14:19:15.019619', 'reliability_rating': 'LIKELY_TRUE', 'quality_score': 30, 'source_id': 'CivicHonors', 'source_title': 'Civic Honors Website', 'author_creator': 'Web Scraping', 'publication_date': '2024-01-17T14:19:15.019621', 'url_reference': 'https://civichonors.com/', 'contextual_notes': 'Extracted from Civic Honors website', 'access_level': 'Public', 'usage_count': 0}\n",
            "Fact 4: {'fact_statement': 'Dr. Nels Lindahl, June 14, 2020, Denver, Colorado', 'category': 'General', 'tags': 'CivicHonors, WebScraped', 'date_recorded': '2024-01-17T14:19:15.019644', 'last_updated': '2024-01-17T14:19:15.019645', 'reliability_rating': 'LIKELY_TRUE', 'quality_score': 30, 'source_id': 'CivicHonors', 'source_title': 'Civic Honors Website', 'author_creator': 'Web Scraping', 'publication_date': '2024-01-17T14:19:15.019647', 'url_reference': 'https://civichonors.com/', 'contextual_notes': 'Extracted from Civic Honors website', 'access_level': 'Public', 'usage_count': 0}\n",
            "Fact 5: {'fact_statement': 'Believing in a dream like graduation with civic honors is only the first step in the process toward advocating the creation of a graduation with civic honors program.', 'category': 'General', 'tags': 'CivicHonors, WebScraped', 'date_recorded': '2024-01-17T14:19:15.019669', 'last_updated': '2024-01-17T14:19:15.019670', 'reliability_rating': 'LIKELY_TRUE', 'quality_score': 30, 'source_id': 'CivicHonors', 'source_title': 'Civic Honors Website', 'author_creator': 'Web Scraping', 'publication_date': '2024-01-17T14:19:15.019672', 'url_reference': 'https://civichonors.com/', 'contextual_notes': 'Extracted from Civic Honors website', 'access_level': 'Public', 'usage_count': 0}\n",
            "Fact 6: {'fact_statement': 'The story behind graduation with civic honors\\xa0began in an academic setting during the spring semester of 2002 at the University of Kansas. I took a class from Dr. H. George Frederickson\\xa0entitled Concepts of Civil Society. During the Concepts of Civil Society class, my collegiate interests focused on civic engagement.\\xa0At one point during the class, Dr. Frederickson posed a question to the class about what colleges could do to get people involved within the community. Over the course of the next year thinking about the idea of civil society, I became interested in pursuing a degree in the field of public administration. Thanks to the faculty of the Public Administration Department at the University of Kansas, I decided to endeavor to enter graduate school. I would like to thank Dr. Raymond Davis for advice and guidance, Dr. Thomas Longoria\\xa0for defining the importance of collaboration, and Dr. H. George Frederickson\\xa0for a thoughtful introduction to the world of civil society.', 'category': 'General', 'tags': 'CivicHonors, WebScraped', 'date_recorded': '2024-01-17T14:19:15.019693', 'last_updated': '2024-01-17T14:19:15.019694', 'reliability_rating': 'LIKELY_TRUE', 'quality_score': 30, 'source_id': 'CivicHonors', 'source_title': 'Civic Honors Website', 'author_creator': 'Web Scraping', 'publication_date': '2024-01-17T14:19:15.019695', 'url_reference': 'https://civichonors.com/', 'contextual_notes': 'Extracted from Civic Honors website', 'access_level': 'Public', 'usage_count': 0}\n",
            "Fact 7: {'fact_statement': 'Graduation with civic honors was only a dream until late in 2002. Dr. Charles J. Carlsen, the president\\xa0of Johnson County Community College,\\xa0learned of the idea from Susan Lindahl and, along with the board of trustees and civic honors steering committee, envisioned becoming the first community college in the state of Kansas to designate a graduation with civic honors. In 2004, after Johnson County Community College developed the initial graduation with civic honors\\xa0pilot program, the next step was to start spreading the graduation with civic honors\\xa0message. I was honored to submit an article with Dr. Charles J. Carlsen and Susan Lindahl entitled, “Civic Honors Program at Johnson County Community College,” to the Journal for Civic Commitment for publication. The resulting publication (Carlsen, Lindahl, & Lindahl, 2004) was the first step toward globally sharing the positive message of graduation with civic honors.', 'category': 'General', 'tags': 'CivicHonors, WebScraped', 'date_recorded': '2024-01-17T14:19:15.019716', 'last_updated': '2024-01-17T14:19:15.019716', 'reliability_rating': 'LIKELY_TRUE', 'quality_score': 30, 'source_id': 'CivicHonors', 'source_title': 'Civic Honors Website', 'author_creator': 'Web Scraping', 'publication_date': '2024-01-17T14:19:15.019718', 'url_reference': 'https://civichonors.com/', 'contextual_notes': 'Extracted from Civic Honors website', 'access_level': 'Public', 'usage_count': 0}\n",
            "Fact 8: {'fact_statement': 'To realize the dream, students actually would have to graduate with civic honors. During the May 2005 graduation at Johnson County Community College, the dream became a reality when four students—including Deborah DeGrate, Carrie Donham, Chris Engle, and Jennifer Pittman-Leeper—were the first to graduate with civic honors. The story will be complete when students all over the world are graduating with civic honors.', 'category': 'General', 'tags': 'CivicHonors, WebScraped', 'date_recorded': '2024-01-17T14:19:15.019736', 'last_updated': '2024-01-17T14:19:15.019737', 'reliability_rating': 'LIKELY_TRUE', 'quality_score': 30, 'source_id': 'CivicHonors', 'source_title': 'Civic Honors Website', 'author_creator': 'Web Scraping', 'publication_date': '2024-01-17T14:19:15.019739', 'url_reference': 'https://civichonors.com/', 'contextual_notes': 'Extracted from Civic Honors website', 'access_level': 'Public', 'usage_count': 0}\n",
            "Fact 9: {'fact_statement': 'Society has experienced a true revival of public interest in civic engagement.', 'category': 'General', 'tags': 'CivicHonors, WebScraped', 'date_recorded': '2024-01-17T14:19:15.019759', 'last_updated': '2024-01-17T14:19:15.019759', 'reliability_rating': 'LIKELY_TRUE', 'quality_score': 30, 'source_id': 'CivicHonors', 'source_title': 'Civic Honors Website', 'author_creator': 'Web Scraping', 'publication_date': '2024-01-17T14:19:15.019761', 'url_reference': 'https://civichonors.com/', 'contextual_notes': 'Extracted from Civic Honors website', 'access_level': 'Public', 'usage_count': 0}\n",
            "Fact 10: {'fact_statement': 'Robert Putnam’s (2000) epic work Bowling Alone brought the idea of civic participation\\xa0to the forefront of the public mind. The value of civic participation\\xa0is essential to the process of value implementation in the form of civic engagement. Civic engagement is a value choice, and the implementation of that value choice is individual civic participation\\xa0in the community. One of the most basic definitions of civic engagement\\xa0involves thinking about how government, society, and citizens interact. In terms of how scholars discuss civic engagement,\\xa0the definition takes on a benevolent feel, referencing citizen activities that benefit civil society. Strengthening civic engagement, in practice, involves building civic skills, increasing active voter participation, and using public service announcements to encourage volunteering\\xa0to strengthen civil society. Watershed events like 9-11 have created a reflective sense of national interest and significantly increased the willingness of individuals to participate in civic engagement.', 'category': 'General', 'tags': 'CivicHonors, WebScraped', 'date_recorded': '2024-01-17T14:19:15.019781', 'last_updated': '2024-01-17T14:19:15.019782', 'reliability_rating': 'LIKELY_TRUE', 'quality_score': 30, 'source_id': 'CivicHonors', 'source_title': 'Civic Honors Website', 'author_creator': 'Web Scraping', 'publication_date': '2024-01-17T14:19:15.019784', 'url_reference': 'https://civichonors.com/', 'contextual_notes': 'Extracted from Civic Honors website', 'access_level': 'Public', 'usage_count': 0}\n"
          ]
        }
      ],
      "source": [
        "# Completeness check passed\n",
        "# Print the total number of facts extracted\n",
        "total_facts = len(facts)\n",
        "print(f\"Total facts extracted: {total_facts}\")\n",
        "\n",
        "# Retrieve and display up to 10 facts\n",
        "for i in range(min(10, total_facts)):\n",
        "    fact_id = str(i)\n",
        "    try:\n",
        "        fact = kg.get_fact(fact_id)\n",
        "        print(f\"Fact {i+1}: {fact}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Fact {i+1}: {str(e)}\")\n",
        "        break  # Stop the loop if a fact ID is not found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJHI4T28a_6d"
      },
      "source": [
        "# Step 6: Store the KnowledgeGraph in GEXF and GraphML Formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dC6G0nFKbC33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e205f6f4-232a-4e6d-cc10-57a4bb064628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph stored in GEXF format at: /content/knowledge_graph.gexf\n",
            "Graph stored in GraphML format at: /content/knowledge_graph.graphml\n"
          ]
        }
      ],
      "source": [
        "# Completeness check passed\n",
        "# Store the graph in GEXF format\n",
        "gexf_path = '/content/knowledge_graph.gexf'\n",
        "nx.write_gexf(kg.graph, gexf_path)\n",
        "print(f\"Graph stored in GEXF format at: {gexf_path}\")\n",
        "\n",
        "# Store the graph in GraphML format\n",
        "graphml_path = '/content/knowledge_graph.graphml'\n",
        "nx.write_graphml(kg.graph, graphml_path)\n",
        "print(f\"Graph stored in GraphML format at: {graphml_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceqgxEGxUxdM"
      },
      "source": [
        "# Step 7: Ensure Uniqueness of Facts in the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z0ZB11K_U0Gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f40f1f-b4db-4f61-90c0-577c909b3c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 0 duplicate facts.\n",
            "Total unique facts remaining: 310\n"
          ]
        }
      ],
      "source": [
        "# Completeness check passed\n",
        "def remove_duplicate_facts(knowledge_graph):\n",
        "    unique_facts = set()\n",
        "    nodes_to_remove = []\n",
        "\n",
        "    for fact_id, fact_data in knowledge_graph.graph.nodes(data=True):\n",
        "        fact_statement = fact_data['fact_statement']\n",
        "        if fact_statement in unique_facts:\n",
        "            nodes_to_remove.append(fact_id)\n",
        "        else:\n",
        "            unique_facts.add(fact_statement)\n",
        "\n",
        "    # Remove duplicate nodes\n",
        "    for fact_id in nodes_to_remove:\n",
        "        knowledge_graph.graph.remove_node(fact_id)\n",
        "\n",
        "    print(f\"Removed {len(nodes_to_remove)} duplicate facts.\")\n",
        "\n",
        "# Call the function to remove duplicate facts\n",
        "remove_duplicate_facts(kg)\n",
        "\n",
        "# Optional: Print the total number of unique facts remaining\n",
        "print(f\"Total unique facts remaining: {kg.graph.number_of_nodes()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0ZkAAb8VCBh"
      },
      "source": [
        "# Step 8: Advanced Cleaning and Combining of Facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UoMDWAoSU06N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8afb44-07b8-4228-b3b0-5f578b801fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 5 facts based on advanced cleaning criteria.\n",
            "Total facts after advanced cleaning: 305\n"
          ]
        }
      ],
      "source": [
        "# Completeness check passed\n",
        "def advanced_cleaning(knowledge_graph, similarity_threshold=0.8, short_fact_threshold=20):\n",
        "    nodes_to_remove = []\n",
        "    facts_processed = set()\n",
        "\n",
        "    # Remove short facts\n",
        "    for fact_id, fact_data in knowledge_graph.graph.nodes(data=True):\n",
        "        if len(fact_data['fact_statement']) < short_fact_threshold:\n",
        "            nodes_to_remove.append(fact_id)\n",
        "            continue\n",
        "\n",
        "        # Check for similarity with already processed facts\n",
        "        for processed_fact in facts_processed:\n",
        "            similarity = difflib.SequenceMatcher(None, fact_data['fact_statement'], processed_fact).ratio()\n",
        "            if similarity > similarity_threshold:\n",
        "                nodes_to_remove.append(fact_id)\n",
        "                break\n",
        "\n",
        "        facts_processed.add(fact_data['fact_statement'])\n",
        "\n",
        "    # Further custom strategies can be added here\n",
        "\n",
        "    # Remove identified nodes\n",
        "    for fact_id in nodes_to_remove:\n",
        "        knowledge_graph.graph.remove_node(fact_id)\n",
        "\n",
        "    print(f\"Removed {len(nodes_to_remove)} facts based on advanced cleaning criteria.\")\n",
        "\n",
        "# Call the function for advanced cleaning\n",
        "advanced_cleaning(kg)\n",
        "\n",
        "# Optional: Print the total number of facts after cleaning\n",
        "print(f\"Total facts after advanced cleaning: {kg.graph.number_of_nodes()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EJymWj3Yv75"
      },
      "source": [
        "# Step 9: Super Aggressive Advanced Cleaning (Refined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u4SxFiWpYyRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05323a3-0537-419f-d65d-d4663e64db50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 268 facts based on super aggressive cleaning criteria.\n",
            "Total facts after super aggressive cleaning: 37\n"
          ]
        }
      ],
      "source": [
        "# Completeness check passed\n",
        "# Load the medium-sized English language model with word vectors\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def super_aggressive_cleaning(knowledge_graph, similarity_threshold=0.85):\n",
        "    nodes_to_remove = set()\n",
        "    facts_content = {}\n",
        "\n",
        "    # Pre-process and store spaCy Doc objects for each fact\n",
        "    for fact_id, fact_data in knowledge_graph.graph.nodes(data=True):\n",
        "        try:\n",
        "            doc = nlp(fact_data['fact_statement'])\n",
        "            facts_content[fact_id] = doc\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing fact ID {fact_id}: {str(e)}\")\n",
        "\n",
        "    # Compare each fact with others for semantic similarity\n",
        "    for fact_id, doc in facts_content.items():\n",
        "        # Skip if already marked for removal\n",
        "        if fact_id in nodes_to_remove:\n",
        "            continue\n",
        "\n",
        "        for other_fact_id, other_doc in facts_content.items():\n",
        "            if fact_id != other_fact_id and other_fact_id not in nodes_to_remove:\n",
        "                similarity = doc.similarity(other_doc)\n",
        "                if similarity > similarity_threshold:\n",
        "                    nodes_to_remove.add(other_fact_id)\n",
        "\n",
        "    # Remove identified nodes\n",
        "    for fact_id in nodes_to_remove:\n",
        "        knowledge_graph.graph.remove_node(fact_id)\n",
        "\n",
        "    print(f\"Removed {len(nodes_to_remove)} facts based on super aggressive cleaning criteria.\")\n",
        "\n",
        "# Call the function for super aggressive cleaning\n",
        "super_aggressive_cleaning(kg)\n",
        "\n",
        "# Print the total number of facts after super aggressive cleaning\n",
        "print(f\"Total facts after super aggressive cleaning: {kg.graph.number_of_nodes()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjCxlyHfyJn"
      },
      "source": [
        "# Step 10: Store the Updated KnowledgeGraph in GEXF and GraphML Formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YhUge9cEfwOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80e9255-dff3-4b4a-f4c0-25bdad548774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph stored in GEXF format at: /content/knowledge_graph2.gexf\n",
            "Graph stored in GraphML format at: /content/knowledge_graph2.graphml\n"
          ]
        }
      ],
      "source": [
        "# Completeness check passed\n",
        "# Store the graph in GEXF format\n",
        "gexf_path = '/content/knowledge_graph2.gexf'\n",
        "nx.write_gexf(kg.graph, gexf_path)\n",
        "print(f\"Graph stored in GEXF format at: {gexf_path}\")\n",
        "\n",
        "# Store the graph in GraphML format\n",
        "graphml_path = '/content/knowledge_graph2.graphml'\n",
        "nx.write_graphml(kg.graph, graphml_path)\n",
        "print(f\"Graph stored in GraphML format at: {graphml_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1388986"
      },
      "source": [
        "# Step 11: Making the Knowledge Graph Portable\n",
        "\n",
        "This step focuses on making the Knowledge Graph portable by serializing it into a JSON format. This allows for easy storage, transfer, and reconstruction of the graph in different environments. The process includes functions for both serialization and deserialization of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdc01998",
        "outputId": "9930a60d-3ccf-44ac-fed5-7026606fd038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph serialized and saved to path_to_graph.json\n",
            "Graph deserialized from path_to_graph.json\n",
            "Serialization and deserialization of Knowledge Graph completed successfully.\n"
          ]
        }
      ],
      "source": [
        "class KnowledgeGraphPortable:\n",
        "    def __init__(self, knowledge_graph):\n",
        "        # Assuming the NetworkX graph is stored in an attribute named 'graph' in KnowledgeGraph\n",
        "        self.graph = knowledge_graph.graph\n",
        "\n",
        "    def serialize_to_json(self, filepath):\n",
        "        # Convert graph to a dictionary or suitable structure\n",
        "        graph_data = nx.node_link_data(self.graph)\n",
        "        with open(filepath, 'w') as file:\n",
        "            json.dump(graph_data, file)\n",
        "        print(f\"Graph serialized and saved to {filepath}\")\n",
        "\n",
        "    def deserialize_from_json(self, filepath):\n",
        "        # Read the file and convert it back to a graph\n",
        "        with open(filepath, 'r') as file:\n",
        "            graph_data = json.load(file)\n",
        "        self.graph = nx.node_link_graph(graph_data)\n",
        "        print(f\"Graph deserialized from {filepath}\")\n",
        "\n",
        "# Usage\n",
        "kg_portable = KnowledgeGraphPortable(kg)\n",
        "kg_portable.serialize_to_json('path_to_graph.json')  # Serialize and save the graph\n",
        "kg_portable.deserialize_from_json('path_to_graph.json')  # Deserialize the saved graph\n",
        "print(\"Serialization and deserialization of Knowledge Graph completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "139ebae2"
      },
      "source": [
        "# Step 12: Graph Analysis Features\n",
        "\n",
        "This step focuses on adding graph analysis features to the Knowledge Graph. It includes functions for calculating various centrality measures, community detection, and other relevant graph analysis metrics. These features can provide valuable insights into the structure and characteristics of the graph."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def analyze_graph(knowledge_graph):\n",
        "    G = knowledge_graph.graph\n",
        "\n",
        "    # Remove isolated nodes\n",
        "    G.remove_nodes_from(list(nx.isolates(G)))\n",
        "    if G.number_of_nodes() == 0:\n",
        "        raise ValueError(\"The graph has no nodes after removing isolates.\")\n",
        "\n",
        "    print(\"Starting graph analysis...\")\n",
        "\n",
        "    try:\n",
        "        # Calculate various centrality measures\n",
        "        degree_centrality = nx.degree_centrality(G) if G.number_of_edges() > 0 else {}\n",
        "        betweenness_centrality = nx.betweenness_centrality(G) if G.number_of_edges() > 0 else {}\n",
        "        closeness_centrality = nx.closeness_centrality(G) if G.number_of_edges() > 0 else {}\n",
        "        print(\"Calculated centrality measures.\")\n",
        "    except ZeroDivisionError as e:\n",
        "        print(f\"Error in centrality calculations: {e}\")\n",
        "\n",
        "    try:\n",
        "        # Community detection using NetworkX's greedy modularity communities\n",
        "        communities = list(nx.community.greedy_modularity_communities(G))\n",
        "        print(\"Detected communities in the graph.\")\n",
        "\n",
        "        # Visualization\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        pos = nx.spring_layout(G)\n",
        "        for community in communities:\n",
        "            nx.draw_networkx_nodes(G, pos, community, node_size=40)\n",
        "        nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
        "        plt.title('Knowledge Graph Visualization with Communities')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in community detection: {e}\")\n",
        "\n",
        "    return {\n",
        "        'degree_centrality': degree_centrality,\n",
        "        'betweenness_centrality': betweenness_centrality,\n",
        "        'closeness_centrality': closeness_centrality,\n",
        "        'communities': communities\n",
        "    }\n",
        "\n",
        "# Usage\n",
        "try:\n",
        "    graph_analysis_results = analyze_graph(kg)\n",
        "except Exception as e:\n",
        "    print(f'Error in graph analysis: {e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Omu_iBzxT1",
        "outputId": "f0712518-46af-4456-ac64-6e2e139d5626"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in graph analysis: The graph has no nodes after removing isolates.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}