{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KnowledgeReduce Implementation for Google Colab\n",
    "\n",
    "This notebook implements the KnowledgeReduce framework for building stackable knowledge repositories. It demonstrates the core concepts from the paper, including the mapping phase, reducing phase, and knowledge stacking.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The implementation includes the following components:\n",
    "1. **Data Ingestion**: Loading and preprocessing data\n",
    "2. **Knowledge Mapping**: Entity and relationship extraction\n",
    "3. **Knowledge Reduction**: Entity resolution and graph synthesis\n",
    "4. **Graph Storage**: In-memory graph representation\n",
    "5. **Query Interface**: Tools for querying and analyzing the knowledge graph\n",
    "\n",
    "Let's start by installing the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy networkx spacy scikit-learn fuzzywuzzy python-Levenshtein matplotlib\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "First, let's define the configuration settings for our KnowledgeReduce implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "class Config:\n",
    "    # Entity resolution settings\n",
    "    SIMILARITY_THRESHOLD = 0.8  # Threshold for fuzzy matching\n",
    "    MAX_EDIT_DISTANCE = 3  # Maximum edit distance for name matching\n",
    "    \n",
    "    # Knowledge mapping settings\n",
    "    DEFAULT_ENTITY_TYPES = [\"Person\", \"Organization\", \"Location\", \"Concept\"]\n",
    "    DEFAULT_RELATIONSHIP_TYPES = [\"WORKS_FOR\", \"LOCATED_IN\", \"RELATED_TO\", \"KNOWS\"]\n",
    "    \n",
    "    # Knowledge stacking settings\n",
    "    KNOWLEDGE_LAYERS = [\"Raw\", \"Processed\", \"Abstract\"]\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Now, let's implement the data ingestion module for loading and preprocessing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Loads data from various file formats.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_csv(file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load data from a CSV file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        return pd.read_csv(file_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_json(file_path: str) -> Dict:\n",
    "        \"\"\"Load data from a JSON file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_dataframe(data: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Create a DataFrame from a list of dictionaries.\"\"\"\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"Preprocesses data for knowledge mapping.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"Clean text data by removing extra whitespace and normalizing case.\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove extra whitespace and normalize case\n",
    "        return \" \".join(text.strip().split()).lower()\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_entities(entities_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Normalize entity data by cleaning text fields and ensuring required columns.\"\"\"\n",
    "        # Ensure required columns exist\n",
    "        required_columns = ['id', 'name', 'type']\n",
    "        for col in required_columns:\n",
    "            if col not in entities_df.columns:\n",
    "                raise ValueError(f\"Required column '{col}' not found in entities data\")\n",
    "        \n",
    "        # Create a copy to avoid modifying the original\n",
    "        df = entities_df.copy()\n",
    "        \n",
    "        # Clean text fields\n",
    "        if 'name' in df.columns:\n",
    "            df['name'] = df['name'].apply(DataPreprocessor.clean_text)\n",
    "        \n",
    "        if 'description' in df.columns:\n",
    "            df['description'] = df['description'].apply(DataPreprocessor.clean_text)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_relationships(relationships_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Normalize relationship data by cleaning text fields and ensuring required columns.\"\"\"\n",
    "        # Ensure required columns exist\n",
    "        required_columns = ['source_id', 'target_id', 'type']\n",
    "        for col in required_columns:\n",
    "            if col not in relationships_df.columns:\n",
    "                raise ValueError(f\"Required column '{col}' not found in relationships data\")\n",
    "        \n",
    "        # Create a copy to avoid modifying the original\n",
    "        df = relationships_df.copy()\n",
    "        \n",
    "        # Clean text fields\n",
    "        if 'description' in df.columns:\n",
    "            df['description'] = df['description'].apply(DataPreprocessor.clean_text)\n",
    "        \n",
    "        return df\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample data for demonstration purposes.\"\"\"\n",
    "    # Create sample entities\n",
    "    entities = [\n",
    "        {\"id\": 1, \"name\": \"John Smith\", \"type\": \"Person\", \"description\": \"Software Engineer\"},\n",
    "        {\"id\": 2, \"name\": \"Jane Doe\", \"type\": \"Person\", \"description\": \"Data Scientist\"},\n",
    "        {\"id\": 3, \"name\": \"Acme Corporation\", \"type\": \"Organization\", \"description\": \"Technology company\"},\n",
    "        {\"id\": 4, \"name\": \"TechCorp\", \"type\": \"Organization\", \"description\": \"Software development firm\"},\n",
    "        {\"id\": 5, \"name\": \"Machine Learning\", \"type\": \"Concept\", \"description\": \"Field of AI\"},\n",
    "        {\"id\": 6, \"name\": \"San Francisco\", \"type\": \"Location\", \"description\": \"City in California\"},\n",
    "        {\"id\": 7, \"name\": \"John Smith\", \"type\": \"Person\", \"description\": \"CTO at TechCorp\"},  # Duplicate for resolution\n",
    "        {\"id\": 8, \"name\": \"J. Smith\", \"type\": \"Person\", \"description\": \"Engineer\"}  # Similar for resolution\n",
    "    ]\n",
    "    \n",
    "    # Create sample relationships\n",
    "    relationships = [\n",
    "        {\"source_id\": 1, \"target_id\": 3, \"type\": \"WORKS_FOR\", \"description\": \"Employed since 2020\"},\n",
    "        {\"source_id\": 2, \"target_id\": 4, \"type\": \"WORKS_FOR\", \"description\": \"Senior position\"},\n",
    "        {\"source_id\": 1, \"target_id\": 2, \"type\": \"KNOWS\", \"description\": \"Colleagues\"},\n",
    "        {\"source_id\": 1, \"target_id\": 5, \"type\": \"RELATED_TO\", \"description\": \"Expertise\"},\n",
    "        {\"source_id\": 2, \"target_id\": 5, \"type\": \"RELATED_TO\", \"description\": \"Expertise\"},\n",
    "        {\"source_id\": 3, \"target_id\": 6, \"type\": \"LOCATED_IN\", \"description\": \"Headquarters\"},\n",
    "        {\"source_id\": 7, \"target_id\": 4, \"type\": \"WORKS_FOR\", \"description\": \"Executive role\"}\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrames\n",
    "    entities_df = pd.DataFrame(entities)\n",
    "    relationships_df = pd.DataFrame(relationships)\n",
    "    \n",
    "    return entities_df, relationships_df\n",
    "\n",
    "# Create sample data\n",
    "entities_df, relationships_df = create_sample_data()\n",
    "\n",
    "# Preprocess data\n",
    "preprocessor = DataPreprocessor()\n",
    "entities_df = preprocessor.normalize_entities(entities_df)\n",
    "relationships_df = preprocessor.normalize_relationships(relationships_df)\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample Entities:\")\n",
    "display(entities_df.head(3))\n",
    "print(\"\\nSample Relationships:\")\n",
    "display(relationships_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Mapping\n",
    "\n",
    "Next, let's implement the knowledge mapping module, which extracts entities and relationships from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class EntityExtractor:\n",
    "    \"\"\"Extracts entities from text data.\"\"\"\n",
    "    \n",
    "    def __init__(self, entity_types=None):\n",
    "        \"\"\"Initialize the entity extractor.\"\"\"\n",
    "        self.entity_types = entity_types or config.DEFAULT_ENTITY_TYPES\n",
    "    \n",
    "    def extract_from_text(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract entities from text using NLP.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        entities = []\n",
    "        \n",
    "        # Extract named entities\n",
    "        for ent in doc.ents:\n",
    "            entity_type = self._map_spacy_entity_type(ent.label_)\n",
    "            if entity_type in self.entity_types:\n",
    "                entities.append({\n",
    "                    \"name\": ent.text,\n",
    "                    \"type\": entity_type,\n",
    "                    \"start\": ent.start_char,\n",
    "                    \"end\": ent.end_char\n",
    "                })\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _map_spacy_entity_type(self, spacy_type: str) -> str:\n",
    "        \"\"\"Map spaCy entity types to our entity types.\"\"\"\n",
    "        # Mapping from spaCy entity types to our types\n",
    "        mapping = {\n",
    "            \"PERSON\": \"Person\",\n",
    "            \"ORG\": \"Organization\",\n",
    "            \"GPE\": \"Location\",\n",
    "            \"LOC\": \"Location\",\n",
    "            \"PRODUCT\": \"Concept\",\n",
    "            \"WORK_OF_ART\": \"Concept\",\n",
    "            \"EVENT\": \"Concept\"\n",
    "        }\n",
    "        \n",
    "        return mapping.get(spacy_type, \"Concept\")\n",
    "    \n",
    "    def extract_from_structured_data(self, entities_df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process structured entity data.\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        for _, row in entities_df.iterrows():\n",
    "            entity = {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"name\": row[\"name\"],\n",
    "                \"type\": row[\"type\"]\n",
    "            }\n",
    "            \n",
    "            # Add any additional attributes\n",
    "            for col in entities_df.columns:\n",
    "                if col not in [\"id\", \"name\", \"type\"] and not pd.isna(row[col]):\n",
    "                    entity[col] = row[col]\n",
    "            \n",
    "            entities.append(entity)\n",
    "        \n",
    "        return entities\n",
    "\n",
    "class RelationshipExtractor:\n",
    "    \"\"\"Extracts relationships between entities.\"\"\"\n",
    "    \n",
    "    def __init__(self, relationship_types=None):\n",
    "        \"\"\"Initialize the relationship extractor.\"\"\"\n",
    "        self.relationship_types = relationship_types or config.DEFAULT_RELATIONSHIP_TYPES\n",
    "    \n",
    "    def extract_from_structured_data(self, relationships_df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process structured relationship data.\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        for _, row in relationships_df.iterrows():\n",
    "            relationship = {\n",
    "                \"source_id\": row[\"source_id\"],\n",
    "                \"target_id\": row[\"target_id\"],\n",
    "                \"type\": row[\"type\"]\n",
    "            }\n",
    "            \n",
    "            # Add any additional attributes\n",
    "            for col in relationships_df.columns:\n",
    "                if col not in [\"source_id\", \"target_id\", \"type\"] and not pd.isna(row[col]):\n",
    "                    relationship[col] = row[col]\n",
    "            \n",
    "            relationships.append(relationship)\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "class KnowledgeMapper:\n",
    "    \"\"\"Implements the mapping phase of KnowledgeReduce.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the knowledge mapper.\"\"\"\n",
    "        self.entity_extractor = EntityExtractor()\n",
    "        self.relationship_extractor = RelationshipExtractor()\n",
    "    \n",
    "    def map(self, entities_df: pd.DataFrame, relationships_df: pd.DataFrame) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "        \"\"\"Map raw data to entities and relationships.\"\"\"\n",
    "        # Process structured data\n",
    "        entities = self.entity_extractor.extract_from_structured_data(entities_df)\n",
    "        relationships = self.relationship_extractor.extract_from_structured_data(relationships_df)\n",
    "        \n",
    "        return entities, relationships\n",
    "\n",
    "# Map data to entities and relationships\n",
    "mapper = KnowledgeMapper()\n",
    "entities, relationships = mapper.map(entities_df, relationships_df)\n",
    "\n",
    "print(f\"Mapped {len(entities)} entities and {len(relationships)} relationships\")\n",
    "print(\"\\nSample mapped entities:\")\n",
    "for entity in entities[:3]:\n",
    "    print(f\"  {entity['name']} ({entity['type']})\")\n",
    "\n",
    "print(\"\\nSample mapped relationships:\")\n",
    "for rel in relationships[:3]:\n",
    "    print(f\"  {rel['source_id']} --[{rel['type']}]--> {rel['target_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Reduction\n",
    "\n",
    "Now, let's implement the knowledge reduction module, which resolves entities and synthesizes the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "class EntityResolver:\n",
    "    \"\"\"Resolves and merges duplicate entities.\"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold=None, max_edit_distance=None):\n",
    "        \"\"\"Initialize the entity resolver.\"\"\"\n",
    "        self.similarity_threshold = similarity_threshold or config.SIMILARITY_THRESHOLD\n",
    "        self.max_edit_distance = max_edit_distance or config.MAX_EDIT_DISTANCE\n",
    "    \n",
    "    def resolve(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Resolve duplicate entities by identifying and merging them.\"\"\"\n",
    "        if not entities:\n",
    "            return []\n",
    "        \n",
    "        # Group entities by type for more efficient comparison\n",
    "        entities_by_type = defaultdict(list)\n",
    "        for entity in entities:\n",
    "            entities_by_type[entity[\"type\"]].append(entity)\n",
    "        \n",
    "        resolved_entities = []\n",
    "        entity_groups = []\n",
    "        \n",
    "        # Find duplicate groups within each entity type\n",
    "        for entity_type, type_entities in entities_by_type.items():\n",
    "            # Create groups of duplicate entities\n",
    "            type_entity_groups = self._find_duplicate_groups(type_entities)\n",
    "            entity_groups.extend(type_entity_groups)\n",
    "        \n",
    "        # Merge each group of duplicates\n",
    "        for group in entity_groups:\n",
    "            merged_entity = self._merge_entities(group)\n",
    "            resolved_entities.append(merged_entity)\n",
    "        \n",
    "        return resolved_entities\n",
    "    \n",
    "    def _find_duplicate_groups(self, entities: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:\n",
    "        \"\"\"Find groups of duplicate entities.\"\"\"\n",
    "        # Initialize each entity as its own group\n",
    "        groups = [[entity] for entity in entities]\n",
    "        \n",
    "        # Merge groups if they contain similar entities\n",
    "        i = 0\n",
    "        while i < len(groups):\n",
    "            j = i + 1\n",
    "            while j < len(groups):\n",
    "                if self._groups_contain_similar_entities(groups[i], groups[j]):\n",
    "                    # Merge groups\n",
    "                    groups[i].extend(groups[j])\n",
    "                    groups.pop(j)\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    def _groups_contain_similar_entities(self, group1: List[Dict[str, Any]], group2: List[Dict[str, Any]]) -> bool:\n",
    "        \"\"\"Check if two groups contain similar entities.\"\"\"\n",
    "        for entity1 in group1:\n",
    "            for entity2 in group2:\n",
    "                if self._are_similar_entities(entity1, entity2):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def _are_similar_entities(self, entity1: Dict[str, Any], entity2: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Check if two entities are similar based on name and attributes.\"\"\"\n",
    "        # Check for exact ID match\n",
    "        if \"id\" in entity1 and \"id\" in entity2 and entity1[\"id\"] == entity2[\"id\"]:\n",
    "            return True\n",
    "        \n",
    "        # Check for name similarity\n",
    "        name1 = entity1.get(\"name\", \"\").lower()\n",
    "        name2 = entity2.get(\"name\", \"\").lower()\n",
    "        \n",
    "        # Exact name match\n",
    "        if name1 == name2 and name1:\n",
    "            return True\n",
    "        \n",
    "        # Fuzzy name match\n",
    "        similarity = fuzz.ratio(name1, name2) / 100.0\n",
    "        if similarity >= self.similarity_threshold:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _merge_entities(self, entities: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Merge a group of duplicate entities into a single entity.\"\"\"\n",
    "        if not entities:\n",
    "            return {}\n",
    "        \n",
    "        if len(entities) == 1:\n",
    "            return entities[0]\n",
    "        \n",
    "        # Start with the first entity as the base\n",
    "        merged = entities[0].copy()\n",
    "        \n",
    "        # Track all IDs that this entity represents\n",
    "        merged[\"original_ids\"] = [entities[0].get(\"id\")]\n",
    "        \n",
    "        # Merge attributes from other entities\n",
    "        for entity in entities[1:]:\n",
    "            # Add original ID\n",
    "            if \"id\" in entity:\n",
    "                merged[\"original_ids\"].append(entity[\"id\"])\n",
    "            \n",
    "            # Merge other attributes\n",
    "            for key, value in entity.items():\n",
    "                if key not in merged or not merged[key]:\n",
    "                    merged[key] = value\n",
    "                elif key == \"description\" and value and value != merged[key]:\n",
    "                    # Concatenate descriptions\n",
    "                    merged[key] = f\"{merged[key]}; {value}\"\n",
    "        \n",
    "        return merged\n",
    "\n",
    "class RelationshipAggregator:\n",
    "    \"\"\"Aggregates relationships from multiple sources.\"\"\"\n",
    "    \n",
    "    def aggregate(self, relationships: List[Dict[str, Any]], entity_id_map: Dict[Any, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Aggregate relationships, updating entity references and removing duplicates.\"\"\"\n",
    "        if not relationships:\n",
    "            return []\n",
    "        \n",
    "        # Update entity references\n",
    "        updated_relationships = []\n",
    "        for rel in relationships:\n",
    "            # Skip if source or target entity was removed during resolution\n",
    "            if rel[\"source_id\"] not in entity_id_map or rel[\"target_id\"] not in entity_id_map:\n",
    "                continue\n",
    "            \n",
    "            # Create updated relationship with resolved entity IDs\n",
    "            updated_rel = rel.copy()\n",
    "            updated_rel[\"source_id\"] = entity_id_map[rel[\"source_id\"]]\n",
    "            updated_rel[\"target_id\"] = entity_id_map[rel[\"target_id\"]]\n",
    "            \n",
    "            # Skip self-relationships (after resolution)\n",
    "            if updated_rel[\"source_id\"] == updated_rel[\"target_id\"]:\n",
    "                continue\n",
    "            \n",
    "            updated_relationships.append(updated_rel)\n",
    "        \n",
    "        # Remove duplicate relationships\n",
    "        aggregated = self._remove_duplicates(updated_relationships)\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def _remove_duplicates(self, relationships: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Remove duplicate relationships.\"\"\"\n",
    "        # Use a set to track unique relationship keys\n",
    "        unique_keys = set()\n",
    "        unique_relationships = []\n",
    "        \n",
    "        for rel in relationships:\n",
    "            # Create a key for the relationship\n",
    "            key = (rel[\"source_id\"], rel[\"target_id\"], rel[\"type\"])\n",
    "            \n",
    "            if key not in unique_keys:\n",
    "                unique_keys.add(key)\n",
    "                unique_relationships.append(rel)\n",
    "        \n",
    "        return unique_relationships\n",
    "\n",
    "class GraphSynthesizer:\n",
    "    \"\"\"Synthesizes the knowledge graph from entities and relationships.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the graph synthesizer.\"\"\"\n",
    "        self.graph = nx.DiGraph()\n",
    "    \n",
    "    def synthesize(self, entities: List[Dict[str, Any]], relationships: List[Dict[str, Any]]) -> nx.DiGraph:\n",
    "        \"\"\"Synthesize a knowledge graph from entities and relationships.\"\"\"\n",
    "        # Create a new graph\n",
    "        self.graph = nx.DiGraph()\n",
    "        \n",
    "        # Add entities as nodes\n",
    "        for entity in entities:\n",
    "            self.graph.add_node(\n",
    "                entity[\"id\"],\n",
    "                name=entity.get(\"name\", \"\"),\n",
    "                type=entity.get(\"type\", \"\"),\n",
    "                description=entity.get(\"description\", \"\"),\n",
    "                layer=entity.get(\"layer\", \"Raw\"),\n",
    "                attributes={k: v for k, v in entity.items() if k not in [\"id\", \"name\", \"type\", \"description\", \"layer\"]}\n",
    "            )\n",
    "        \n",
    "        # Add relationships as edges\n",
    "        for rel in relationships:\n",
    "            self.graph.add_edge(\n",
    "                rel[\"source_id\"],\n",
    "                rel[\"target_id\"],\n",
    "                type=rel[\"type\"],\n",
    "                description=rel.get(\"description\", \"\"),\n",
    "                attributes={k: v for k, v in rel.items() if k not in [\"source_id\", \"target_id\", \"type\", \"description\"]}\n",
    "            )\n",
    "        \n",
    "        return self.graph\n",
    "    \n",
    "    def get_graph(self) -> nx.DiGraph:\n",
    "        \"\"\"Get the synthesized knowledge graph.\"\"\"\n",
    "        return self.graph\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the knowledge graph.\"\"\"\n",
    "        stats = {\n",
    "            \"num_nodes\": self.graph.number_of_nodes(),\n",
    "            \"num_edges\": self.graph.number_of_edges(),\n",
    "            \"node_types\": defaultdict(int),\n",
    "            \"edge_types\": defaultdict(int),\n",
    "            \"avg_degree\": sum(dict(self.graph.degree()).values()) / max(1, self.graph.number_of_nodes())\n",
    "        }\n",
    "        \n",
    "        # Count node types\n",
    "        for node, data in self.graph.nodes(data=True):\n",
    "            node_type = data.get(\"type\", \"Unknown\")\n",
    "            stats[\"node_types\"][node_type] += 1\n",
    "        \n",
    "        # Count edge types\n",
    "        for _, _, data in self.graph.edges(data=True):\n",
    "            edge_type = data.get(\"type\", \"Unknown\")\n",
    "            stats[\"edge_types\"][edge_type] += 1\n",
    "        \n",
    "        return stats\n",
    "\n",
    "class KnowledgeReducer:\n",
    "    \"\"\"Implements the reducing phase of KnowledgeReduce.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the knowledge reducer.\"\"\"\n",
    "        self.entity_resolver = EntityResolver()\n",
    "        self.relationship_aggregator = RelationshipAggregator()\n",
    "        self.graph_synthesizer = GraphSynthesizer()\n",
    "    \n",
    "    def reduce(self, entities: List[Dict[str, Any]], relationships: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], nx.DiGraph]:\n",
    "        \"\"\"Reduce mapped knowledge by resolving entities, aggregating relationships, and synthesizing the graph.\"\"\"\n",
    "        # Resolve entities\n",
    "        resolved_entities = self.entity_resolver.resolve(entities)\n",
    "        \n",
    "        # Create mapping from original entity IDs to resolved entity IDs\n",
    "        entity_id_map = {}\n",
    "        for entity in resolved_entities:\n",
    "            for original_id in entity.get(\"original_ids\", [entity[\"id\"]]):\n",
    "                entity_id_map[original_id] = entity[\"id\"]\n",
    "        \n",
    "        # Aggregate relationships\n",
    "        aggregated_relationships = self.relationship_aggregator.aggregate(relationships, entity_id_map)\n",
    "        \n",
    "        # Synthesize knowledge graph\n",
    "        knowledge_graph = self.graph_synthesizer.synthesize(resolved_entities, aggregated_relationships)\n",
    "        \n",
    "        return resolved_entities, aggregated_relationships, knowledge_graph\n",
    "\n",
    "# Reduce mapped knowledge\n",
    "reducer = KnowledgeReducer()\n",
    "resolved_entities, aggregated_relationships, knowledge_graph = reducer.reduce(entities, relationships)\n",
    "\n",
    "print(f\"Reduced {len(entities)} entities to {len(resolved_entities)} entities\")\n",
    "print(f\"Reduced {len(relationships)} relationships to {len(aggregated_relationships)} relationships\")\n",
    "\n",
    "# Print graph statistics\n",
    "stats = reducer.graph_synthesizer.get_statistics()\n",
    "print(f\"\\nGraph statistics:\")\n",
    "print(f\"  Nodes: {stats['num_nodes']}\")\n",
    "print(f\"  Edges: {stats['num_edges']}\")\n",
    "print(f\"  Node types: {dict(stats['node_types'])}\")\n",
    "print(f\"  Edge types: {dict(stats['edge_types'])}\")\n",
    "print(f\"  Average degree: {stats['avg_degree']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Storage and Query Interface\n",
    "\n",
    "Now, let's implement the graph storage and query interface components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryGraphStore:\n",
    "    \"\"\"In-memory graph store using NetworkX.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the in-memory graph store.\"\"\"\n",
    "        self.graph = nx.DiGraph()\n",
    "    \n",
    "    def store_graph(self, graph: nx.DiGraph) -> None:\n",
    "        \"\"\"Store a knowledge graph.\"\"\"\n",
    "        self.graph = graph.copy()\n",
    "    \n",
    "    def get_graph(self) -> nx.DiGraph:\n",
    "        \"\"\"Get the stored knowledge graph.\"\"\"\n",
    "        return self.graph\n",
    "    \n",
    "    def get_node(self, node_id: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Get a node by ID.\"\"\"\n",
    "        if node_id in self.graph.nodes:\n",
    "            return {**{\"id\": node_id}, **self.graph.nodes[node_id]}\n",
    "        return {}\n",
    "    \n",
    "    def get_nodes_by_type(self, node_type: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get nodes by type.\"\"\"\n",
    "        nodes = []\n",
    "        for node_id, data in self.graph.nodes(data=True):\n",
    "            if data.get(\"type\") == node_type:\n",
    "                nodes.append({**{\"id\": node_id}, **data})\n",
    "        return nodes\n",
    "    \n",
    "    def get_relationships(self, source_id: Any = None, target_id: Any = None, rel_type: str = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get relationships, optionally filtered by source, target, or type.\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        for source, target, data in self.graph.edges(data=True):\n",
    "            # Apply filters\n",
    "            if source_id is not None and source != source_id:\n",
    "                continue\n",
    "            if target_id is not None and target != target_id:\n",
    "                continue\n",
    "            if rel_type is not None and data.get(\"type\") != rel_type:\n",
    "                continue\n",
    "            \n",
    "            relationships.append({\n",
    "                \"source_id\": source,\n",
    "                \"target_id\": target,\n",
    "                **data\n",
    "            })\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    def get_neighbors(self, node_id: Any, direction: str = \"both\") -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get neighboring nodes of a given node.\"\"\"\n",
    "        if node_id not in self.graph.nodes:\n",
    "            return []\n",
    "        \n",
    "        neighbors = []\n",
    "        \n",
    "        if direction in [\"out\", \"both\"]:\n",
    "            for target in self.graph.successors(node_id):\n",
    "                neighbors.append({**{\"id\": target}, **self.graph.nodes[target]})\n",
    "        \n",
    "        if direction in [\"in\", \"both\"]:\n",
    "            for source in self.graph.predecessors(node_id):\n",
    "                if {**{\"id\": source}, **self.graph.nodes[source]} not in neighbors:\n",
    "                    neighbors.append({**{\"id\": source}, **self.graph.nodes[source]})\n",
    "        \n",
    "        return neighbors\n",
    "\n",
    "class GraphQuery:\n",
    "    \"\"\"Interface for querying the knowledge graph.\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_store):\n",
    "        \"\"\"Initialize the graph query interface.\"\"\"\n",
    "        self.graph_store = graph_store\n",
    "    \n",
    "    def get_entities_by_type(self, entity_type: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get entities of a specific type.\"\"\"\n",
    "        return self.graph_store.get_nodes_by_type(entity_type)\n",
    "    \n",
    "    def get_entity_by_id(self, entity_id: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Get an entity by ID.\"\"\"\n",
    "        return self.graph_store.get_node(entity_id)\n",
    "    \n",
    "    def get_entity_by_name(self, name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get an entity by name (returns first match).\"\"\"\n",
    "        graph = self.graph_store.get_graph()\n",
    "        for node_id, data in graph.nodes(data=True):\n",
    "            if data.get(\"name\", \"\").lower() == name.lower():\n",
    "                return {**{\"id\": node_id}, **data}\n",
    "        return {}\n",
    "    \n",
    "    def get_relationships_between(self, source_id: Any, target_id: Any) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get relationships between two entities.\"\"\"\n",
    "        return self.graph_store.get_relationships(source_id=source_id, target_id=target_id)\n",
    "    \n",
    "    def get_relationships_by_type(self, rel_type: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get relationships of a specific type.\"\"\"\n",
    "        return self.graph_store.get_relationships(rel_type=rel_type)\n",
    "    \n",
    "    def get_neighbors(self, entity_id: Any, direction: str = \"both\") -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get neighboring entities of a given entity.\"\"\"\n",
    "        return self.graph_store.get_neighbors(entity_id, direction)\n",
    "    \n",
    "    def find_paths(self, source_id: Any, target_id: Any, max_length: int = 3) -> List[List[Dict[str, Any]]]:\n",
    "        \"\"\"Find paths between two entities.\"\"\"\n",
    "        graph = self.graph_store.get_graph()\n",
    "        \n",
    "        if source_id not in graph.nodes or target_id not in graph.nodes:\n",
    "            return []\n",
    "        \n",
    "        paths = []\n",
    "        \n",
    "        # Find all simple paths up to max_length\n",
    "        for path in nx.all_simple_paths(graph, source_id, target_id, cutoff=max_length):\n",
    "            entity_path = []\n",
    "            for node_id in path:\n",
    "                entity_path.append({**{\"id\": node_id}, **graph.nodes[node_id]})\n",
    "            paths.append(entity_path)\n",
    "        \n",
    "        return paths\n",
    "\n",
    "class KnowledgeAnalyzer:\n",
    "    \"\"\"Analyzes the knowledge graph to extract insights.\"\"\"\n",
    "    \n",
    "    def __init__(self, graph_store):\n",
    "        \"\"\"Initialize the knowledge analyzer.\"\"\"\n",
    "        self.graph_store = graph_store\n",
    "    \n",
    "    def get_central_entities(self, top_n: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get the most central entities based on degree centrality.\"\"\"\n",
    "        graph = self.graph_store.get_graph()\n",
    "        \n",
    "        # Calculate degree centrality\n",
    "        centrality = nx.degree_centrality(graph)\n",
    "        \n",
    "        # Sort entities by centrality\n",
    "        sorted_entities = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        \n",
    "        # Convert to entity dictionaries with centrality scores\n",
    "        central_entities = []\n",
    "        for node_id, score in sorted_entities:\n",
    "            entity = {**{\"id\": node_id}, **graph.nodes[node_id], \"centrality\": score}\n",
    "            central_entities.append(entity)\n",
    "        \n",
    "        return central_entities\n",
    "    \n",
    "    def generate_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a summary of the knowledge graph.\"\"\"\n",
    "        graph = self.graph_store.get_graph()\n",
    "        \n",
    "        # Count entities by type\n",
    "        entity_types = defaultdict(int)\n",
    "        for _, data in graph.nodes(data=True):\n",
    "            entity_type = data.get(\"type\", \"Unknown\")\n",
    "            entity_types[entity_type] += 1\n",
    "        \n",
    "        # Count relationships by type\n",
    "        relationship_types = defaultdict(int)\n",
    "        for _, _, data in graph.edges(data=True):\n",
    "            rel_type = data.get(\"type\", \"Unknown\")\n",
    "            relationship_types[rel_type] += 1\n",
    "        \n",
    "        # Calculate average number of relationships per entity\n",
    "        avg_degree = sum(dict(graph.degree()).values()) / max(1, graph.number_of_nodes())\n",
    "        \n",
    "        # Get central entities\n",
    "        central_entities = self.get_central_entities(5)\n",
    "        \n",
    "        return {\n",
    "            \"entity_statistics\": {\n",
    "                \"total_entities\": graph.number_of_nodes(),\n",
    "                \"entity_types\": dict(entity_types),\n",
    "                \"avg_relationships\": avg_degree\n",
    "            },\n",
    "            \"relationship_statistics\": {\n",
    "                \"total_relationships\": graph.number_of_edges(),\n",
    "                \"relationship_types\": dict(relationship_types)\n",
    "            },\n",
    "            \"central_entities\": central_entities\n",
    "        }\n",
    "\n",
    "# Store the graph\n",
    "graph_store = InMemoryGraphStore()\n",
    "graph_store.store_graph(knowledge_graph)\n",
    "\n",
    "# Create query interface\n",
    "query = GraphQuery(graph_store)\n",
    "\n",
    "# Test queries\n",
    "print(\"\\nEntities by type:\")\n",
    "for entity_type in set(entity['type'] for entity in resolved_entities):\n",
    "    entities_of_type = query.get_entities_by_type(entity_type)\n",
    "    print(f\"  {entity_type}: {len(entities_of_type)} entities\")\n",
    "\n",
    "# Find entity by name\n",
    "john = query.get_entity_by_name(\"john smith\")\n",
    "if john:\n",
    "    print(f\"\\nFound entity: {john['name']} ({john['id']})\")\n",
    "    \n",
    "    # Get neighbors\n",
    "    neighbors = query.get_neighbors(john['id'])\n",
    "    print(f\"\\nNeighbors of {john['name']}:\")\n",
    "    for neighbor in neighbors:\n",
    "        print(f\"  {neighbor['name']} ({neighbor['type']})\")\n",
    "\n",
    "# Create analyzer\n",
    "analyzer = KnowledgeAnalyzer(graph_store)\n",
    "\n",
    "# Get central entities\n",
    "print(\"\\nMost central entities:\")\n",
    "central = analyzer.get_central_entities(3)\n",
    "for entity in central:\n",
    "    print(f\"  {entity['name']} (centrality: {entity['centrality']:.3f})\")\n",
    "\n",
    "# Generate summary\n",
    "summary = analyzer.generate_summary()\n",
    "print(f\"\\nKnowledge Graph Summary:\")\n",
    "print(f\"  Entities: {summary['entity_statistics']['total_entities']}\")\n",
    "print(f\"  Relationships: {summary['relationship_statistics']['total_relationships']}\")\n",
    "print(f\"  Entity types: {summary['entity_statistics']['entity_types']}\")\n",
    "print(f\"  Relationship types: {summary['relationship_statistics']['relationship_types']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Stacking\n",
    "\n",
    "Now, let's implement the knowledge stacking mechanism, which organizes knowledge in hierarchical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_knowledge_stacking(graph_store):\n",
    "    \"\"\"Implement knowledge stacking by assigning entities to layers.\"\"\"\n",
    "    print(\"\\n=== Knowledge Stacking Implementation ===\\n\")\n",
    "    \n",
    "    # Get the knowledge graph\n",
    "    graph = graph_store.get_graph()\n",
    "    \n",
    "    # Create knowledge layers\n",
    "    layers = config.KNOWLEDGE_LAYERS\n",
    "    print(f\"Creating {len(layers)} knowledge layers: {', '.join(layers)}\")\n",
    "    \n",
    "    # Assign entities to layers\n",
    "    for node_id, data in graph.nodes(data=True):\n",
    "        # Assign layer based on entity type\n",
    "        if data.get(\"type\") == \"Concept\":\n",
    "            # Concepts go to the Abstract layer\n",
    "            graph.nodes[node_id][\"layer\"] = \"Abstract\"\n",
    "        elif data.get(\"type\") in [\"Person\", \"Organization\"]:\n",
    "            # People and organizations go to the Processed layer\n",
    "            graph.nodes[node_id][\"layer\"] = \"Processed\"\n",
    "        else:\n",
    "            # Everything else goes to the Raw layer\n",
    "            graph.nodes[node_id][\"layer\"] = \"Raw\"\n",
    "    \n",
    "    # Count entities by layer\n",
    "    layer_counts = {}\n",
    "    for layer in layers:\n",
    "        count = sum(1 for _, data in graph.nodes(data=True) if data.get(\"layer\") == layer)\n",
    "        layer_counts[layer] = count\n",
    "    \n",
    "    print(\"\\nEntities by layer:\")\n",
    "    for layer, count in layer_counts.items():\n",
    "        print(f\"  {layer}: {count} entities\")\n",
    "    \n",
    "    # Update the graph in the store\n",
    "    graph_store.store_graph(graph)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def demonstrate_cross_layer_query(graph_store, query):\n",
    "    \"\"\"Demonstrate cross-layer querying capabilities.\"\"\"\n",
    "    print(\"\\n=== Cross-Layer Query Demonstration ===\\n\")\n",
    "    \n",
    "    # Find concepts (Abstract layer) related to people (Processed layer)\n",
    "    print(\"Concepts related to people:\")\n",
    "    \n",
    "    # Get all people\n",
    "    people = query.get_entities_by_type(\"Person\")\n",
    "    \n",
    "    # For each person, find related concepts\n",
    "    for person in people:\n",
    "        neighbors = query.get_neighbors(person[\"id\"])\n",
    "        concepts = [n for n in neighbors if n.get(\"type\") == \"Concept\"]\n",
    "        \n",
    "        if concepts:\n",
    "            print(f\"  {person['name']} is related to concepts:\")\n",
    "            for concept in concepts:\n",
    "                print(f\"    - {concept['name']}\")\n",
    "\n",
    "# Implement knowledge stacking\n",
    "stacked_graph = implement_knowledge_stacking(graph_store)\n",
    "\n",
    "# Demonstrate cross-layer querying\n",
    "demonstrate_cross_layer_query(graph_store, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Finally, let's visualize the knowledge graph to better understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def visualize_knowledge_graph(graph_store):\n",
    "    \"\"\"Visualize the knowledge graph.\"\"\"\n",
    "    graph = graph_store.get_graph()\n",
    "    \n",
    "    # Create a larger figure\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Define node colors by type\n",
    "    type_colors = {\n",
    "        \"Person\": \"skyblue\",\n",
    "        \"Organization\": \"lightgreen\",\n",
    "        \"Location\": \"salmon\",\n",
    "        \"Concept\": \"gold\"\n",
    "    }\n",
    "    \n",
    "    # Define node colors by layer\n",
    "    layer_colors = {\n",
    "        \"Raw\": \"lightgray\",\n",
    "        \"Processed\": \"lightblue\",\n",
    "        \"Abstract\": \"lightyellow\"\n",
    "    }\n",
    "    \n",
    "    # Define edge colors by type\n",
    "    edge_colors = {\n",
    "        \"WORKS_FOR\": \"blue\",\n",
    "        \"LOCATED_IN\": \"green\",\n",
    "        \"KNOWS\": \"red\",\n",
    "        \"RELATED_TO\": \"purple\"\n",
    "    }\n",
    "    \n",
    "    # Create node color map\n",
    "    node_colors = []\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        node_type = data.get(\"type\", \"Unknown\")\n",
    "        node_colors.append(type_colors.get(node_type, \"gray\"))\n",
    "    \n",
    "    # Create edge color map\n",
    "    edge_colors_list = []\n",
    "    for _, _, data in graph.edges(data=True):\n",
    "        edge_type = data.get(\"type\", \"Unknown\")\n",
    "        edge_colors_list.append(edge_colors.get(edge_type, \"gray\"))\n",
    "    \n",
    "    # Create node labels\n",
    "    node_labels = {}\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        node_labels[node] = data.get(\"name\", str(node))\n",
    "    \n",
    "    # Use spring layout for node positioning\n",
    "    pos = nx.spring_layout(graph, seed=42)\n",
    "    \n",
    "    # Draw the graph\n",
    "    nx.draw_networkx_nodes(graph, pos, node_color=node_colors, node_size=500, alpha=0.8)\n",
    "    nx.draw_networkx_edges(graph, pos, edge_color=edge_colors_list, width=2, alpha=0.7, arrowsize=15)\n",
    "    nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=10, font_weight=\"bold\")\n",
    "    \n",
    "    # Add legend for node types\n",
    "    node_type_patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=type_name)\n",
    "                         for type_name, color in type_colors.items()]\n",
    "    plt.legend(handles=node_type_patches, title=\"Entity Types\", loc=\"upper left\")\n",
    "    \n",
    "    plt.title(\"KnowledgeReduce Knowledge Graph\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a second visualization showing layers\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create node color map by layer\n",
    "    node_layer_colors = []\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        node_layer = data.get(\"layer\", \"Raw\")\n",
    "        node_layer_colors.append(layer_colors.get(node_layer, \"gray\"))\n",
    "    \n",
    "    # Draw the graph with layer colors\n",
    "    nx.draw_networkx_nodes(graph, pos, node_color=node_layer_colors, node_size=500, alpha=0.8)\n",
    "    nx.draw_networkx_edges(graph, pos, edge_color=\"gray\", width=2, alpha=0.7, arrowsize=15)\n",
    "    nx.draw_networkx_labels(graph, pos, labels=node_labels, font_size=10, font_weight=\"bold\")\n",
    "    \n",
    "    # Add legend for layers\n",
    "    layer_patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=layer_name)\n",
    "                     for layer_name, color in layer_colors.items()]\n",
    "    plt.legend(handles=layer_patches, title=\"Knowledge Layers\", loc=\"upper left\")\n",
    "    \n",
    "    plt.title(\"KnowledgeReduce Stackable Knowledge Layers\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the knowledge graph\n",
    "visualize_knowledge_graph(graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the implementation of the KnowledgeReduce framework for building stackable knowledge repositories. We've covered:\n",
    "\n",
    "1. **Data Ingestion**: Loading and preprocessing data from various sources\n",
    "2. **Knowledge Mapping**: Extracting entities and relationships from data\n",
    "3. **Knowledge Reduction**: Resolving entities and synthesizing the knowledge graph\n",
    "4. **Knowledge Stacking**: Organizing knowledge in hierarchical layers\n",
    "5. **Query and Analysis**: Querying and analyzing the knowledge graph\n",
    "6. **Visualization**: Visualizing the knowledge graph structure\n",
    "\n",
    "The KnowledgeReduce framework provides a powerful approach to building knowledge repositories with stackable knowledge capabilities. This implementation demonstrates the core concepts and can be extended for more complex use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
