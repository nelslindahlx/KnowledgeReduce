{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqApLjsH2xVQRapMDdflAY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/CivicHonorsGraphML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Overview\n",
        "\n",
        "The revised Python code is developed for Google Colab and focuses on extracting, analyzing, and structuring data from a webpage into a knowledge graph, which is subsequently saved in GraphML format. The update addresses a specific issue related to data types compatibility with the GraphML format. Here's an updated summary of the code and its workflow:\n",
        "\n",
        "### 1. **Library Installation and Imports**\n",
        "   - The script begins by installing necessary NLP-related Python libraries (`nltk`, `spacy`, `textblob`).\n",
        "   - It imports libraries for web scraping (`requests`, `BeautifulSoup`), graph creation (`networkx`), and standard utilities (`nltk`, `string`, `Counter`).\n",
        "   - NLTK resources are downloaded for text processing tasks, and Spacy's English model is loaded for NLP functions.\n",
        "\n",
        "### 2. **Function Definitions**\n",
        "   - `fetch_webpage_content(url)`: Retrieves the HTML content from a given URL. It includes error handling for HTTP errors.\n",
        "   - `parse_webpage_content(content)`: Parses HTML content to extract text, then performs entity recognition using Spacy, sentiment analysis using TextBlob, and keyword extraction with NLTK, filtering out stopwords and punctuation.\n",
        "   - `knowledge_reduce(entities, sentiment, keyword_freq)`: Constructs a knowledge graph where nodes represent entities and keywords. The function has been updated to ensure all data attributes are converted to strings, a requirement for GraphML compatibility. It includes the overall sentiment of the text.\n",
        "   - `save_knowledge_graph_graphml(graph, file_name)`: Saves the knowledge graph in GraphML format, addressing the data type compatibility issue by ensuring all node and edge attributes are in a supported format.\n",
        "\n",
        "### 3. **Execution Flow**\n",
        "   - **Fetching Webpage Content**: The script fetches content from \"https://civichonors.com/\".\n",
        "   - **Parsing Webpage Content**: The content is then parsed to extract entities, sentiments, and keywords.\n",
        "   - **Creating and Saving the Knowledge Graph**: The script creates a knowledge graph from the extracted data. The updated script ensures all graph attributes are in a format compatible with GraphML, and the graph is saved as a GraphML file.\n",
        "\n",
        "This updated script is particularly useful for educational and analytical purposes in a Google Colab environment, allowing for an interactive process in web scraping, natural language processing, and knowledge graph construction. The final output, a GraphML file, provides a structured, visual representation of the content's interconnected elements, suitable for analysis or visualization in graph software tools.\n"
      ],
      "metadata": {
        "id": "HFaRbPURu4Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install and Import Libraries"
      ],
      "metadata": {
        "id": "9T5gKUD_uXm8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MX-CRX6uUDp",
        "outputId": "0c035019-35fc-449c-8d66-072ff778b94b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk spacy textblob\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import networkx as nx\n",
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Define Functions"
      ],
      "metadata": {
        "id": "bZ1ixAZouazY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_webpage_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        raise Exception(f\"HTTP Error: {e}\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error fetching webpage: {e}\")\n",
        "\n",
        "def parse_webpage_content(content):\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "    text_content = soup.get_text()\n",
        "\n",
        "    doc = nlp(text_content)\n",
        "    entities = [(ent.text.strip(), ent.label_) for ent in doc.ents if ent.text.strip()]\n",
        "\n",
        "    blob = TextBlob(text_content)\n",
        "    sentiment = blob.sentiment\n",
        "\n",
        "    words = [word.lower() for word in nltk.word_tokenize(text_content)\n",
        "             if word.lower() not in stopwords.words('english')\n",
        "             and word not in string.punctuation]\n",
        "    keyword_freq = Counter(words).most_common(10)\n",
        "\n",
        "    return entities, sentiment, keyword_freq\n",
        "\n",
        "def knowledge_reduce(entities, sentiment, keyword_freq):\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    for entity, type in entities:\n",
        "        graph.add_node(entity, type=str(type), label=str(entity))\n",
        "\n",
        "    for word, freq in keyword_freq:\n",
        "        graph.add_node(word, type='keyword', frequency=str(freq), label=str(word))\n",
        "\n",
        "    # Convert sentiment to a string representation\n",
        "    sentiment_data = f\"Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\"\n",
        "    graph.graph['sentiment'] = sentiment_data\n",
        "\n",
        "    # Add edge creation logic here\n",
        "    for entity, _ in entities:\n",
        "        for word, _ in keyword_freq:\n",
        "            if word in entity:\n",
        "                graph.add_edge(entity, word)\n",
        "\n",
        "    return graph\n",
        "\n",
        "def save_knowledge_graph_graphml(graph, file_name):\n",
        "    nx.write_graphml(graph, file_name)"
      ],
      "metadata": {
        "id": "a3_HM_yNucgJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Fetch Webpage Content"
      ],
      "metadata": {
        "id": "n12KSl1fueNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://civichonors.com/\"\n",
        "try:\n",
        "    content = fetch_webpage_content(url)\n",
        "    print(\"Webpage content fetched successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching webpage: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzWGNdTqujpa",
        "outputId": "b5f279f6-4a47-42ff-f9a2-9339a4053c98"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webpage content fetched successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Parse the Webpage Content"
      ],
      "metadata": {
        "id": "4uiD2FG0umBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    entities, sentiment, keywords = parse_webpage_content(content)\n",
        "    print(\"Webpage content parsed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing webpage content: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1QR_dmzunFK",
        "outputId": "ca21c66e-acc2-4859-c9a5-2c3a57fae3cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webpage content parsed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Create and Save the Knowledge Graph"
      ],
      "metadata": {
        "id": "R0hkjXNDuo-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    graph = knowledge_reduce(entities, sentiment, keywords)\n",
        "    save_knowledge_graph_graphml(graph, '/content/knowledge_graph.graphml')\n",
        "    print(\"Knowledge graph created and saved in GraphML format successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating or saving knowledge graph: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UL0iiRIurZx",
        "outputId": "2e3de638-0843-4cec-e863-3a48d7ccbbc5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge graph created and saved in GraphML format successfully.\n"
          ]
        }
      ]
    }
  ]
}