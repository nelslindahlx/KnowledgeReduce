{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVFlSEIgDfuADT7izf95VS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/CivicHonorsGraphML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Overview\n",
        "\n",
        "The provided Python code is structured to run in Google Colab and is designed to fetch, process, and visualize data from a specified webpage. It particularly focuses on constructing a knowledge graph from the webpage's content, which is then saved in GraphML format. Here's a breakdown of the code's main components and workflow:\n",
        "\n",
        "### 1. **Library Installation and Imports**\n",
        "   - The code starts by installing necessary Python libraries (`nltk`, `spacy`, `textblob`) that are used for natural language processing tasks.\n",
        "   - Essential libraries for web scraping (`requests`, `BeautifulSoup`), graph creation and manipulation (`networkx`), and standard utilities (`string`, `Counter` from `collections`) are imported.\n",
        "   - NLTK resources for tasks like tokenization and stopwords removal are downloaded, and Spacy's English model is loaded for NLP tasks.\n",
        "\n",
        "### 2. **Function Definitions**\n",
        "   - `fetch_webpage_content(url)`: This function fetches the HTML content of a webpage using the `requests` library. It includes error handling to manage HTTP errors effectively.\n",
        "   - `parse_webpage_content(content)`: Parses the HTML to extract text, performs entity recognition using Spacy, sentiment analysis using TextBlob, and keyword extraction with NLTK. It filters out stopwords and punctuation to identify the most frequent keywords.\n",
        "   - `knowledge_reduce(entities, sentiment, keyword_freq)`: Constructs a knowledge graph using NetworkX. Nodes in the graph represent entities and keywords, each annotated with specific attributes. The graph also incorporates the overall sentiment of the text. Custom logic can be added to define how nodes are connected.\n",
        "   - `save_knowledge_graph_graphml(graph, file_name)`: Saves the generated knowledge graph in the GraphML format using NetworkX's `write_graphml` method. GraphML is a versatile format compatible with various graph analysis tools.\n",
        "\n",
        "### 3. **Execution Flow**\n",
        "   - **Fetching Webpage Content**: The script begins by retrieving the content from \"https://civichonors.com/\".\n",
        "   - **Parsing Webpage Content**: The fetched content is then parsed to extract entities, sentiments, and keywords.\n",
        "   - **Creating and Saving the Knowledge Graph**: The script constructs a knowledge graph from the parsed data and saves it as a GraphML file. This file can be used for further analysis or visualization in graph software.\n",
        "\n",
        "This script is particularly useful in educational and data analysis contexts, allowing for an interactive process in web content scraping, natural language processing, and knowledge graph construction. The final output, a GraphML file, provides a structured and visual representation of the content's interconnected elements, making it suitable for further analysis in graph visualization tools.\n"
      ],
      "metadata": {
        "id": "HFaRbPURu4Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install and Import Libraries"
      ],
      "metadata": {
        "id": "9T5gKUD_uXm8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MX-CRX6uUDp",
        "outputId": "0c035019-35fc-449c-8d66-072ff778b94b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk spacy textblob\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import networkx as nx\n",
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Define Functions"
      ],
      "metadata": {
        "id": "bZ1ixAZouazY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_webpage_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        raise Exception(f\"HTTP Error: {e}\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error fetching webpage: {e}\")\n",
        "\n",
        "def parse_webpage_content(content):\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "    text_content = soup.get_text()\n",
        "\n",
        "    doc = nlp(text_content)\n",
        "    entities = [(ent.text.strip(), ent.label_) for ent in doc.ents if ent.text.strip()]\n",
        "\n",
        "    blob = TextBlob(text_content)\n",
        "    sentiment = blob.sentiment\n",
        "\n",
        "    words = [word.lower() for word in nltk.word_tokenize(text_content)\n",
        "             if word.lower() not in stopwords.words('english')\n",
        "             and word not in string.punctuation]\n",
        "    keyword_freq = Counter(words).most_common(10)\n",
        "\n",
        "    return entities, sentiment, keyword_freq\n",
        "\n",
        "def knowledge_reduce(entities, sentiment, keyword_freq):\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    for entity, type in entities:\n",
        "        graph.add_node(entity, type=str(type), label=str(entity))\n",
        "\n",
        "    for word, freq in keyword_freq:\n",
        "        graph.add_node(word, type='keyword', frequency=str(freq), label=str(word))\n",
        "\n",
        "    # Convert sentiment to a string representation\n",
        "    sentiment_data = f\"Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\"\n",
        "    graph.graph['sentiment'] = sentiment_data\n",
        "\n",
        "    # Add edge creation logic here\n",
        "    for entity, _ in entities:\n",
        "        for word, _ in keyword_freq:\n",
        "            if word in entity:\n",
        "                graph.add_edge(entity, word)\n",
        "\n",
        "    return graph\n",
        "\n",
        "def save_knowledge_graph_graphml(graph, file_name):\n",
        "    nx.write_graphml(graph, file_name)"
      ],
      "metadata": {
        "id": "a3_HM_yNucgJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Fetch Webpage Content"
      ],
      "metadata": {
        "id": "n12KSl1fueNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://civichonors.com/\"\n",
        "try:\n",
        "    content = fetch_webpage_content(url)\n",
        "    print(\"Webpage content fetched successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching webpage: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzWGNdTqujpa",
        "outputId": "b5f279f6-4a47-42ff-f9a2-9339a4053c98"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webpage content fetched successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Parse the Webpage Content"
      ],
      "metadata": {
        "id": "4uiD2FG0umBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    entities, sentiment, keywords = parse_webpage_content(content)\n",
        "    print(\"Webpage content parsed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing webpage content: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1QR_dmzunFK",
        "outputId": "ca21c66e-acc2-4859-c9a5-2c3a57fae3cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webpage content parsed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Create and Save the Knowledge Graph"
      ],
      "metadata": {
        "id": "R0hkjXNDuo-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    graph = knowledge_reduce(entities, sentiment, keywords)\n",
        "    save_knowledge_graph_graphml(graph, '/content/knowledge_graph.graphml')\n",
        "    print(\"Knowledge graph created and saved in GraphML format successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating or saving knowledge graph: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UL0iiRIurZx",
        "outputId": "2e3de638-0843-4cec-e863-3a48d7ccbbc5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge graph created and saved in GraphML format successfully.\n"
          ]
        }
      ]
    }
  ]
}