{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CivicHonorsKGv20: Enhanced Knowledge Graph with Advanced Link Collection and Page Search\n",
    "\n",
    "This notebook is an improved version of CivicHonorsKGv19, featuring enhanced link collection and page search capabilities.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a knowledge graph for Civic Honors content with the following steps:\n",
    "\n",
    "1. **Install and Import Libraries**: Set up the required dependencies\n",
    "2. **Define Knowledge Graph Class**: Create a flexible KG structure\n",
    "3. **Define Enhanced Knowledge Reduction Class**: Implement advanced reduction techniques\n",
    "4. **Define Advanced Link Collection Class**: Implement comprehensive link collection\n",
    "5. **Define Advanced Page Search Class**: Implement intelligent page searching\n",
    "6. **Scrape Website Data**: Collect information from relevant websites and follow links\n",
    "7. **Populate Knowledge Graph**: Extract and structure facts\n",
    "8. **Retrieve and Display Facts**: View the extracted knowledge\n",
    "9. **Ensure Uniqueness**: Remove duplicate facts\n",
    "10. **Advanced Cleaning**: Apply semantic similarity for redundancy reduction\n",
    "11. **Enhanced Knowledge Reduction**: Apply transformer-based models and hierarchical clustering\n",
    "12. **Serialization**: Save and load the knowledge graph\n",
    "\n",
    "The main improvements in this version are the enhanced link collection and page search capabilities that allow the knowledge graph to discover and incorporate information from a broader range of relevant sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests beautifulsoup4 difflib spacy sentence-transformers scikit-learn networkx\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "import enum\n",
    "import networkx as nx\n",
    "from difflib import SequenceMatcher\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib.parse\n",
    "import time\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from collections import deque, defaultdict\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Any, Optional, Set, Union, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Knowledge Graph Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReliabilityRating(enum.Enum):\n",
    "    UNKNOWN = \"Unknown\"\n",
    "    UNLIKELY_FALSE = \"Unlikely False\"\n",
    "    POSSIBLY_FALSE = \"Possibly False\"\n",
    "    POSSIBLY_TRUE = \"Possibly True\"\n",
    "    LIKELY_TRUE = \"Likely True\"\n",
    "    VERIFIED_TRUE = \"Verified True\"\n",
    "\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        \n",
    "    def add_fact(self, \n",
    "                fact_id=None, \n",
    "                fact_statement=None, \n",
    "                category=\"General\", \n",
    "                tags=None, \n",
    "                date_recorded=None, \n",
    "                last_updated=None, \n",
    "                reliability_rating=ReliabilityRating.UNKNOWN, \n",
    "                source_id=None, \n",
    "                source_title=None, \n",
    "                author_creator=None, \n",
    "                publication_date=None, \n",
    "                url_reference=None, \n",
    "                related_facts=None, \n",
    "                contextual_notes=None, \n",
    "                access_level=\"Public\", \n",
    "                usage_count=0):\n",
    "        \n",
    "        if date_recorded is None:\n",
    "            date_recorded = datetime.datetime.now()\n",
    "        \n",
    "        if last_updated is None:\n",
    "            last_updated = datetime.datetime.now()\n",
    "        \n",
    "        if tags is None:\n",
    "            tags = []\n",
    "        \n",
    "        if related_facts is None:\n",
    "            related_facts = []\n",
    "            \n",
    "        fact = {\n",
    "            \"fact_id\": fact_id,\n",
    "            \"fact_statement\": fact_statement,\n",
    "            \"category\": category,\n",
    "            \"tags\": tags,\n",
    "            \"date_recorded\": date_recorded,\n",
    "            \"last_updated\": last_updated,\n",
    "            \"reliability_rating\": reliability_rating,\n",
    "            \"source_id\": source_id,\n",
    "            \"source_title\": source_title,\n",
    "            \"author_creator\": author_creator,\n",
    "            \"publication_date\": publication_date,\n",
    "            \"url_reference\": url_reference,\n",
    "            \"related_facts\": related_facts,\n",
    "            \"contextual_notes\": contextual_notes,\n",
    "            \"access_level\": access_level,\n",
    "            \"usage_count\": usage_count\n",
    "        }\n",
    "        \n",
    "        self.data.append(fact)\n",
    "        return fact\n",
    "    \n",
    "    def update_quality_score(self, fact_id, new_score):\n",
    "        for fact in self.data:\n",
    "            if fact[\"fact_id\"] == fact_id:\n",
    "                fact[\"quality_score\"] = new_score\n",
    "                fact[\"last_updated\"] = datetime.datetime.now()\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def save_to_file(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            # Convert datetime objects to strings for JSON serialization\n",
    "            serializable_data = []\n",
    "            for fact in self.data:\n",
    "                fact_copy = fact.copy()\n",
    "                if isinstance(fact_copy[\"date_recorded\"], datetime.datetime):\n",
    "                    fact_copy[\"date_recorded\"] = fact_copy[\"date_recorded\"].isoformat()\n",
    "                if isinstance(fact_copy[\"last_updated\"], datetime.datetime):\n",
    "                    fact_copy[\"last_updated\"] = fact_copy[\"last_updated\"].isoformat()\n",
    "                if isinstance(fact_copy[\"publication_date\"], datetime.datetime):\n",
    "                    fact_copy[\"publication_date\"] = fact_copy[\"publication_date\"].isoformat()\n",
    "                if isinstance(fact_copy[\"reliability_rating\"], ReliabilityRating):\n",
    "                    fact_copy[\"reliability_rating\"] = fact_copy[\"reliability_rating\"].value\n",
    "                serializable_data.append(fact_copy)\n",
    "            \n",
    "            json.dump(serializable_data, f, indent=2)\n",
    "    \n",
    "    def load_from_file(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Convert string representations back to objects\n",
    "            for fact in data:\n",
    "                if \"date_recorded\" in fact and fact[\"date_recorded\"]:\n",
    "                    fact[\"date_recorded\"] = datetime.datetime.fromisoformat(fact[\"date_recorded\"])\n",
    "                if \"last_updated\" in fact and fact[\"last_updated\"]:\n",
    "                    fact[\"last_updated\"] = datetime.datetime.fromisoformat(fact[\"last_updated\"])\n",
    "                if \"publication_date\" in fact and fact[\"publication_date\"]:\n",
    "                    fact[\"publication_date\"] = datetime.datetime.fromisoformat(fact[\"publication_date\"])\n",
    "                if \"reliability_rating\" in fact:\n",
    "                    fact[\"reliability_rating\"] = ReliabilityRating(fact[\"reliability_rating\"])\n",
    "            \n",
    "            self.data = data\n",
    "    \n",
    "    def get_facts_by_tag(self, tag):\n",
    "        return [fact for fact in self.data if tag in fact[\"tags\"]]\n",
    "    \n",
    "    def get_facts_by_category(self, category):\n",
    "        return [fact for fact in self.data if fact[\"category\"] == category]\n",
    "    \n",
    "    def get_facts_by_source(self, source_id):\n",
    "        return [fact for fact in self.data if fact[\"source_id\"] == source_id]\n",
    "    \n",
    "    def get_fact_by_id(self, fact_id):\n",
    "        for fact in self.data:\n",
    "            if fact[\"fact_id\"] == fact_id:\n",
    "                return fact\n",
    "        return None\n",
    "    \n",
    "    def update_fact(self, fact_id, **kwargs):\n",
    "        for fact in self.data:\n",
    "            if fact[\"fact_id\"] == fact_id:\n",
    "                for key, value in kwargs.items():\n",
    "                    if key in fact:\n",
    "                        fact[key] = value\n",
    "                fact[\"last_updated\"] = datetime.datetime.now()\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def delete_fact(self, fact_id):\n",
    "        for i, fact in enumerate(self.data):\n",
    "            if fact[\"fact_id\"] == fact_id:\n",
    "                del self.data[i]\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_all_tags(self):\n",
    "        tags = set()\n",
    "        for fact in self.data:\n",
    "            tags.update(fact[\"tags\"])\n",
    "        return list(tags)\n",
    "    \n",
    "    def get_all_categories(self):\n",
    "        return list(set(fact[\"category\"] for fact in self.data))\n",
    "    \n",
    "    def get_all_sources(self):\n",
    "        return list(set(fact[\"source_id\"] for fact in self.data if fact[\"source_id\"]))\n",
    "    \n",
    "    def get_fact_count(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Enhanced Knowledge Reduction Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedKnowledgeReduction:\n",
    "    def __init__(self, \n",
    "                 transformer_model='all-MiniLM-L6-v2',\n",
    "                 spacy_model='en_core_web_md',\n",
    "                 similarity_threshold=0.85,\n",
    "                 cluster_distance_threshold=0.5,\n",
    "                 importance_threshold=0.6):\n",
    "        self.transformer_model = transformer_model\n",
    "        self.spacy_model = spacy_model\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.cluster_distance_threshold = cluster_distance_threshold\n",
    "        self.importance_threshold = importance_threshold\n",
    "        \n",
    "        # Load models\n",
    "        self.sentence_transformer = SentenceTransformer(transformer_model)\n",
    "        self.nlp = spacy.load(spacy_model)\n",
    "        \n",
    "        # Initialize graph for entity disambiguation\n",
    "        self.graph = nx.Graph()\n",
    "    \n",
    "    def reduce_knowledge(self, knowledge_graph):\n",
    "        # Step 1: Extract fact statements\n",
    "        facts = [fact[\"fact_statement\"] for fact in knowledge_graph.data]\n",
    "        fact_ids = [fact[\"fact_id\"] for fact in knowledge_graph.data]\n",
    "        \n",
    "        # Step 2: Compute embeddings\n",
    "        embeddings = self.sentence_transformer.encode(facts)\n",
    "        \n",
    "        # Step 3: Perform hierarchical clustering\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=self.cluster_distance_threshold,\n",
    "\n",
    "            linkage='average'\n",
    "        ).fit(embeddings)\n",
    "        \n",
    "        # Step 4: Extract entities and build entity graph\n",
    "        self._build_entity_graph(facts, fact_ids)\n",
    "        \n",
    "        # Step 5: Calculate importance scores\n",
    "        importance_scores = self._calculate_importance_scores(knowledge_graph)\n",
    "        \n",
    "        # Step 6: Select representative facts from each cluster\n",
    "        clusters = {}\n",
    "        for i, cluster_id in enumerate(clustering.labels_):\n",
    "            if cluster_id not in clusters:\n",
    "                clusters[cluster_id] = []\n",
    "            clusters[cluster_id].append(i)\n",
    "        \n",
    "        # Step 7: Create reduced knowledge graph\n",
    "        reduced_kg = KnowledgeGraph()\n",
    "        \n",
    "        for cluster_id, fact_indices in clusters.items():\n",
    "            # If cluster has only one fact, keep it\n",
    "            if len(fact_indices) == 1:\n",
    "                idx = fact_indices[0]\n",
    "                if importance_scores[idx] >= self.importance_threshold:\n",
    "                    reduced_kg.data.append(knowledge_graph.data[idx])\n",
    "                continue\n",
    "            \n",
    "            # Find the most important fact in the cluster\n",
    "            most_important_idx = max(fact_indices, key=lambda i: importance_scores[i])\n",
    "            \n",
    "            # Only keep if it meets the importance threshold\n",
    "            if importance_scores[most_important_idx] >= self.importance_threshold:\n",
    "                # Add the most important fact\n",
    "                most_important_fact = knowledge_graph.data[most_important_idx].copy()\n",
    "                \n",
    "                # Add related facts\n",
    "                related_facts = [fact_ids[i] for i in fact_indices if i != most_important_idx]\n",
    "                most_important_fact[\"related_facts\"] = related_facts\n",
    "                \n",
    "                reduced_kg.data.append(most_important_fact)\n",
    "        \n",
    "        return reduced_kg\n",
    "    \n",
    "    def _build_entity_graph(self, facts, fact_ids):\n",
    "        self.graph.clear()\n",
    "        \n",
    "        # Extract entities from each fact\n",
    "        for i, fact in enumerate(facts):\n",
    "            doc = self.nlp(fact)\n",
    "            \n",
    "            # Add fact node\n",
    "            fact_node = f\"fact_{fact_ids[i]}\"\n",
    "            self.graph.add_node(fact_node, type=\"fact\", text=fact)\n",
    "            \n",
    "            # Add entity nodes and edges\n",
    "            for ent in doc.ents:\n",
    "                entity_node = f\"entity_{ent.text}_{ent.label_}\"\n",
    "                if not self.graph.has_node(entity_node):\n",
    "                    self.graph.add_node(entity_node, type=\"entity\", text=ent.text, label=ent.label_)\n",
    "                \n",
    "                self.graph.add_edge(fact_node, entity_node, weight=1.0)\n",
    "    \n",
    "    def _calculate_importance_scores(self, knowledge_graph):\n",
    "        importance_scores = []\n",
    "        \n",
    "        for fact in knowledge_graph.data:\n",
    "            # Base score\n",
    "            score = 0.5\n",
    "            \n",
    "            # Adjust based on reliability rating\n",
    "            if fact[\"reliability_rating\"] == ReliabilityRating.VERIFIED_TRUE:\n",
    "                score += 0.3\n",
    "            elif fact[\"reliability_rating\"] == ReliabilityRating.LIKELY_TRUE:\n",
    "                score += 0.2\n",
    "            elif fact[\"reliability_rating\"] == ReliabilityRating.POSSIBLY_TRUE:\n",
    "                score += 0.1\n",
    "            elif fact[\"reliability_rating\"] == ReliabilityRating.POSSIBLY_FALSE:\n",
    "                score -= 0.1\n",
    "            elif fact[\"reliability_rating\"] == ReliabilityRating.UNLIKELY_FALSE:\n",
    "                score -= 0.2\n",
    "            \n",
    "            # Adjust based on centrality in entity graph\n",
    "            fact_node = f\"fact_{fact['fact_id']}\"\n",
    "            if self.graph.has_node(fact_node):\n",
    "                # Use degree centrality as a measure of importance\n",
    "                centrality = nx.degree_centrality(self.graph)[fact_node]\n",
    "                score += centrality * 0.2\n",
    "            \n",
    "            # Adjust based on usage count\n",
    "            if fact[\"usage_count\"] > 0:\n",
    "                score += min(0.2, fact[\"usage_count\"] * 0.02)\n",
    "            \n",
    "            # Ensure score is between 0 and 1\n",
    "            score = max(0.0, min(1.0, score))\n",
    "            \n",
    "            importance_scores.append(score)\n",
    "        \n",
    "        return importance_scores\n",
    "    \n",
    "    def save_graph(self, filename):\n",
    "        # Explicitly set edges=\"links\" to avoid future compatibility warnings\n",
    "        data = nx.node_link_data(self.graph, edges=\"links\")\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def load_graph(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        # Explicitly set edges=\"links\" to avoid future compatibility warnings\n",
    "        self.graph = nx.node_link_graph(data, edges=\"links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Advanced Link Collection Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkCollector:\n",
    "    \"\"\"Class for collecting links from websites with advanced features.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 start_urls: List[str] = None,\n",
    "                 allowed_domains: List[str] = None,\n",
    "                 max_depth: int = 2,\n",
    "                 respect_robots_txt: bool = True,\n",
    "                 rate_limit: float = 1.0,\n",
    "                 user_agent: str = 'CivicHonorsKG/1.0',\n",
    "                 max_urls: int = 100,\n",
    "                 link_filters: List[Callable[[str], bool]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the LinkCollector.\n",
    "        \n",
    "        Args:\n",
    "            start_urls: List of URLs to start crawling from\n",
    "            allowed_domains: List of domains to restrict crawling to\n",
    "            max_depth: Maximum depth to crawl (0 = only start_urls)\n",
    "            respect_robots_txt: Whether to respect robots.txt files\n",
    "            rate_limit: Minimum time between requests in seconds\n",
    "            user_agent: User agent string to use for requests\n",
    "            max_urls: Maximum number of URLs to collect\n",
    "            link_filters: List of filter functions that take a URL and return True if it should be followed\n",
    "        \"\"\"\n",
    "        self.start_urls = start_urls or []\n",
    "        self.allowed_domains = allowed_domains or []\n",
    "        self.max_depth = max_depth\n",
    "        self.respect_robots_txt = respect_robots_txt\n",
    "        self.rate_limit = rate_limit\n",
    "        self.user_agent = user_agent\n",
    "        self.max_urls = max_urls\n",
    "        self.link_filters = link_filters or []\n",
    "        \n",
    "        # Internal state\n",
    "        self.url_queue = deque()  # Queue of (url, depth) tuples\n",
    "        self.visited_urls = set()  # Set of visited URLs\n",
    "        self.robots_parsers = {}  # Cache of RobotFileParser objects\n",
    "        self.last_request_time = 0  # Time of last request\n",
    "        self.collected_pages = {}  # Dictionary of {url: {'html': html, 'soup': soup, 'links': links, 'metadata': metadata}}\n",
    "        \n",
    "        # Initialize queue with start URLs\n",
    "        for url in self.start_urls:\n",
    "            self.url_queue.append((self._normalize_url(url), 0))\n",
    "    \n",
    "    def _normalize_url(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize a URL to prevent duplicates.\n",
    "        \n",
    "        Args:\n",
    "            url: URL to normalize\n",
    "            \n",
    "        Returns:\n",
    "            Normalized URL\n",
    "        \"\"\"\n",
    "        # Parse the URL\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        \n",
    "        # Normalize the path\n",
    "        path = parsed.path\n",
    "        if not path:\n",
    "            path = '/'\n",
    "        \n",
    "        # Remove trailing slash except for root\n",
    "        if path != '/' and path.endswith('/'):\n",
    "            path = path[:-1]\n",
    "        \n",
    "        # Reconstruct the URL without fragments and with default ports removed\n",
    "        normalized = urllib.parse.urlunparse((\n",
    "            parsed.scheme,\n",
    "            parsed.netloc,\n",
    "            path,\n",
    "            parsed.params,\n",
    "            parsed.query,\n",
    "            ''  # Remove fragment\n",
    "        ))\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def _get_domain(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the domain from a URL.\n",
    "        \n",
    "        Args:\n",
    "            url: URL to extract domain from\n",
    "            \n",
    "        Returns:\n",
    "            Domain name\n",
    "        \"\"\"\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return parsed.netloc\n",
    "    \n",
    "    def _is_allowed_domain(self, url: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a URL's domain is in the allowed domains list.\n",
    "        \n",
    "        Args:\n",
    "            url: URL to check\n",
    "            \n",
    "        Returns:\n",
    "            True if domain is allowed or no restrictions set\n",
    "        \"\"\"\n",
    "        if not self.allowed_domains:\n",
    "            return True\n",
    "        \n",
    "        domain = self._get_domain(url)\n",
    "        \n",
    "        for allowed_domain in self.allowed_domains:\n",
    "            if domain == allowed_domain or domain.endswith('.' + allowed_domain):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _can_fetch(self, url: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a URL can be fetched according to robots.txt.\n",
    "        \n",
    "        Args:\n",
    "            url: URL to check\n",
    "            \n",
    "        Returns:\n",
    "            True if URL can be fetched\n",
    "        \"\"\"\n",
    "        if not self.respect_robots_txt:\n",
    "            return True\n",
    "        \n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        scheme = parsed.scheme\n",
    "        \n",
    "        # Get or create robots parser for this domain\n",
    "        if domain not in self.robots_parsers:\n",
    "            robots_url = f\"{scheme}://{domain}/robots.txt\"\n",
    "            parser = RobotFileParser()\n",
    "            parser.set_url(robots_url)\n",
    "            try:\n",
    "                parser.read()\n",
    "                self.robots_parsers[domain] = parser\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading robots.txt for {domain}: {e}\")\n",
    "                # If we can't read robots.txt, assume we can fetch\n",
    "                return True\n",
    "        \n",
    "        # Check if we can fetch this URL\n",
    "        return self.robots_parsers[domain].can_fetch(self.user_agent, url)\n",
    "    \n",
    "    def _apply_rate_limit(self):\n",
    "        \"\"\"Apply rate limiting between requests.\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last_request = current_time - self.last_request_time\n",
    "        \n",
    "        if time_since_last_request < self.rate_limit:\n",
    "            time.sleep(self.rate_limit - time_since_last_request)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "    \n",
    "    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract all links from a BeautifulSoup object.\n",
    "        \n",
    "        Args:\n",
    "            soup: BeautifulSoup object\n",
    "            base_url: Base URL for resolving relative links\n",
    "            \n",
    "        Returns:\n",
    "            List of absolute URLs\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        \n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            \n",
    "            # Skip empty links, javascript, and mailto links\n",
    "            if not href or href.startswith(('javascript:', 'mailto:', 'tel:')):\n",
    "                continue\n",
    "            \n",
    "            # Resolve relative URLs\n",
    "            absolute_url = urllib.parse.urljoin(base_url, href)\n",
    "            \n",
    "            # Normalize the URL\n",
    "            normalized_url = self._normalize_url(absolute_url)\n",
    "            \n",
    "            # Apply custom filters\n",
    "            if all(filter_func(normalized_url) for filter_func in self.link_filters):\n",
    "                links.append(normalized_url)\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    def _extract_metadata(self, soup: BeautifulSoup, url: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract metadata from a page.\n",
    "        \n",
    "        Args:\n",
    "            soup: BeautifulSoup object\n",
    "            url: URL of the page\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of metadata\n",
    "        \"\"\"\n",
    "        metadata = {\n",
    "            'title': None,\n",
    "            'description': None,\n",
    "            'keywords': None,\n",
    "            'author': None,\n",
    "            'published_date': None,\n",
    "        }\n",
    "        \n",
    "        # Extract title\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            metadata['title'] = title_tag.get_text().strip()\n",
    "        \n",
    "        # Extract meta tags\n",
    "        for meta in soup.find_all('meta'):\n",
    "            name = meta.get('name', '').lower()\n",
    "            property = meta.get('property', '').lower()\n",
    "            content = meta.get('content', '')\n",
    "            \n",
    "            if name == 'description' or property == 'og:description':\n",
    "                metadata['description'] = content\n",
    "            elif name == 'keywords':\n",
    "                metadata['keywords'] = content\n",
    "            elif name == 'author':\n",
    "                metadata['author'] = content\n",
    "            elif name == 'article:published_time' or property == 'article:published_time':\n",
    "                metadata['published_date'] = content\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def fetch_url(self, url: str) -> Tuple[Optional[str], Optional[BeautifulSoup]]:\n",
    "        \"\"\"\n",
    "        Fetch a URL and return its HTML content and BeautifulSoup object.\n",
    "        \n",
    "        Args:\n",
    "            url: URL to fetch\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (html_content, soup_object) or (None, None) if fetch failed\n",
    "        \"\"\"\n",
    "        print(f\"Fetching URL: {url}\")\n",
    "        \n",
    "        # Apply rate limiting\n",
    "        self._apply_rate_limit()\n",
    "        \n",
    "        # Check robots.txt\n",
    "        if not self._can_fetch(url):\n",
    "            print(f\"Skipping URL {url} (disallowed by robots.txt)\")\n",
    "            return None, None\n",
    "        \n",
    "        # Fetch the URL\n",
    "        try:\n",
    "            headers = {'User-Agent': self.user_agent}\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            return html, soup\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def collect_links(self) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Collect links starting from the start_urls.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of collected pages\n",
    "        \"\"\"\n",
    "        while self.url_queue and len(self.visited_urls) < self.max_urls:\n",
    "            # Get the next URL and depth from the queue\n",
    "            url, depth = self.url_queue.popleft()\n",
    "            \n",
    "            # Skip if already visited\n",
    "            if url in self.visited_urls:\n",
    "                continue\n",
    "            \n",
    "            # Skip if not in allowed domains\n",
    "            if not self._is_allowed_domain(url):\n",
    "                continue\n",
    "            \n",
    "            # Mark as visited\n",
    "            self.visited_urls.add(url)\n",
    "            \n",
    "            # Fetch the URL\n",
    "            html, soup = self.fetch_url(url)\n",
    "            \n",
    "            if not html or not soup:\n",
    "                continue\n",
    "            \n",
    "            # Extract links\n",
    "            links = self._extract_links(soup, url)\n",
    "            \n",
    "            # Extract metadata\n",
    "            metadata = self._extract_metadata(soup, url)\n",
    "            \n",
    "            # Store the page\n",
    "            self.collected_pages[url] = {\n",
    "                'html': html,\n",
    "                'soup': soup,\n",
    "                'links': links,\n",
    "                'metadata': metadata,\n",
    "                'depth': depth\n",
    "            }\n",
    "            \n",
    "            # If we haven't reached max depth, add links to queue\n",
    "            if depth < self.max_depth:\n",
    "                for link in links:\n",
    "                    if link not in self.visited_urls:\n",
    "                        self.url_queue.append((link, depth + 1))\n",
    "            \n",
    "            print(f\"Collected {len(self.visited_urls)} URLs, {len(self.url_queue)} in queue\")\n",
    "        \n",
    "        return self.collected_pages\n",
    "    \n",
    "    def get_collected_urls(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of collected URLs.\n",
    "        \n",
    "        Returns:\n",
    "            List of URLs\n",
    "        \"\"\"\n",
    "        return list(self.collected_pages.keys())\n",
    "    \n",
    "    def get_page_content(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get content for a specific URL.\n",
    "        \n",
    "        Args:\n",
    "            url: URL to get content for\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with page content or None if not found\n",
    "        \"\"\"\n",
    "        return self.collected_pages.get(url)\n",
    "\n",
    "# Example filter functions\n",
    "def filter_pdf_urls(url: str) -> bool:\n",
    "    \"\"\"Filter out PDF URLs.\"\"\"\n",
    "    return not url.lower().endswith('.pdf')\n",
    "\n",
    "def filter_image_urls(url: str) -> bool:\n",
    "    \"\"\"Filter out image URLs.\"\"\"\n",
    "    return not url.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg'))\n",
    "\n",
    "def filter_by_keywords(url: str, keywords: List[str]) -> bool:\n",
    "    \"\"\"Filter URLs by keywords in the URL.\"\"\"\n",
    "    url_lower = url.lower()\n",
    "    return any(keyword.lower() in url_lower for keyword in keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Advanced Page Search Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageSearcher:\n",
    "    \"\"\"Class for searching and extracting information from web pages with advanced features.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 spacy_model: str = 'en_core_web_md',\n",
    "                 transformer_model: str = 'all-MiniLM-L6-v2',\n",
    "                 min_content_length: int = 20,\n",
    "                 relevance_threshold: float = 0.5,\n",
    "                 context_window: int = 2):\n",
    "        \"\"\"\n",
    "        Initialize the PageSearcher.\n",
    "        \n",
    "        Args:\n",
    "            spacy_model: Name of spaCy model to use\n",
    "            transformer_model: Name of sentence transformer model to use\n",
    "            min_content_length: Minimum length of content to consider\n",
    "            relevance_threshold: Minimum relevance score for content to be included\n",
    "            context_window: Number of surrounding elements to include as context\n",
    "        \"\"\"\n",
    "        self.min_content_length = min_content_length\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.context_window = context_window\n",
    "        \n",
    "        # Load NLP models\n",
    "        print(f\"Loading spaCy model: {spacy_model}\")\n",
    "        self.nlp = spacy.load(spacy_model)\n",
    "        \n",
    "        print(f\"Loading sentence transformer model: {transformer_model}\")\n",
    "        self.transformer = SentenceTransformer(transformer_model)\n",
    "        \n",
    "        # Initialize storage for extracted information\n",
    "        self.extracted_facts = []\n",
    "        self.entity_index = defaultdict(list)  # Maps entity -> list of fact indices\n",
    "        self.keyword_index = defaultdict(list)  # Maps keyword -> list of fact indices\n",
    "        self.url_to_facts = defaultdict(list)  # Maps URL -> list of fact indices\n",
    "        \n",
    "    def extract_text_with_context(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract text from HTML elements with surrounding context.\n",
    "        \n",
    "        Args:\n",
    "            soup: BeautifulSoup object\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with text and context\n",
    "        \"\"\"\n",
    "        elements = []\n",
    "        \n",
    "        # Get all elements that might contain content\n",
    "        content_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div', 'span', 'article', 'section'])\n",
    "        \n",
    "        for i, element in enumerate(content_elements):\n",
    "            text = element.get_text().strip()\n",
    "            \n",
    "            # Skip empty or very short elements\n",
    "            if not text or len(text) < self.min_content_length:\n",
    "                continue\n",
    "            \n",
    "            # Get context (surrounding elements)\n",
    "            start_idx = max(0, i - self.context_window)\n",
    "            end_idx = min(len(content_elements), i + self.context_window + 1)\n",
    "            \n",
    "            context_elements = content_elements[start_idx:i] + content_elements[i+1:end_idx]\n",
    "            context = [elem.get_text().strip() for elem in context_elements if elem.get_text().strip()]\n",
    "            \n",
    "            elements.append({\n",
    "                'text': text,\n",
    "                'element_type': element.name,\n",
    "                'context': context,\n",
    "                'html': str(element)\n",
    "            })\n",
    "        \n",
    "        return elements\n",
    "    \n",
    "    def extract_structured_data(self, soup: BeautifulSoup) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract structured data like tables and lists.\n",
    "        \n",
    "        Args:\n",
    "            soup: BeautifulSoup object\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with structured data\n",
    "        \"\"\"\n",
    "        structured_data = []\n",
    "        \n",
    "        # Extract tables\n",
    "        tables = soup.find_all('table')\n",
    "        for table in tables:\n",
    "            rows = []\n",
    "            headers = []\n",
    "            \n",
    "            # Extract headers\n",
    "            th_elements = table.find_all('th')\n",
    "            if th_elements:\n",
    "                headers = [th.get_text().strip() for th in th_elements]\n",
    "            \n",
    "            # Extract rows\n",
    "            for tr in table.find_all('tr'):\n",
    "                cells = [td.get_text().strip() for td in tr.find_all(['td', 'th'])]\n",
    "                if cells and not all(cell == '' for cell in cells):\n",
    "                    rows.append(cells)\n",
    "            \n",
    "            if rows:\n",
    "                structured_data.append({\n",
    "                    'type': 'table',\n",
    "                    'headers': headers,\n",
    "                    'rows': rows,\n",
    "                    'html': str(table)\n",
    "                })\n",
    "        \n",
    "        # Extract lists\n",
    "        for list_tag in soup.find_all(['ul', 'ol']):\n",
    "            items = [li.get_text().strip() for li in list_tag.find_all('li')]\n",
    "            if items:\n",
    "                structured_data.append({\n",
    "                    'type': 'list',\n",
    "                    'list_type': list_tag.name,\n",
    "                    'items': items,\n",
    "                    'html': str(list_tag)\n",
    "                })\n",
    "        \n",
    "        return structured_data\n",
    "    \n",
    "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Extract named entities from text using spaCy.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to extract entities from\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping entity types to lists of entities\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        entities = defaultdict(list)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "        \n",
    "        return dict(entities)\n",
    "    \n",
    "    def calculate_relevance_score(self, text: str, query_embedding: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate relevance score of text to a query.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to score\n",
    "            query_embedding: Embedding of the query\n",
    "            \n",
    "        Returns:\n",
    "            Relevance score between 0 and 1\n",
    "        \"\"\"\n",
    "        text_embedding = self.transformer.encode([text])[0]\n",
    "        similarity = cosine_similarity([query_embedding], [text_embedding])[0][0]\n",
    "        return float(similarity)\n",
    "    \n",
    "    def search_by_keywords(self, pages: Dict[str, Dict], keywords: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search pages for specific keywords.\n",
    "        \n",
    "        Args:\n",
    "            pages: Dictionary of pages from LinkCollector\n",
    "            keywords: List of keywords to search for\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with search results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for url, page_data in pages.items():\n",
    "            soup = page_data['soup']\n",
    "            \n",
    "            # Get all text elements\n",
    "            elements = self.extract_text_with_context(soup)\n",
    "            \n",
    "            for element in elements:\n",
    "                text = element['text'].lower()\n",
    "                \n",
    "                # Check if any keyword is in the text\n",
    "                matches = [keyword for keyword in keywords if keyword.lower() in text]\n",
    "                \n",
    "                if matches:\n",
    "                    results.append({\n",
    "                        'url': url,\n",
    "                        'text': element['text'],\n",
    "                        'element_type': element['element_type'],\n",
    "                        'context': element['context'],\n",
    "                        'matched_keywords': matches,\n",
    "                        'html': element['html']\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_by_regex(self, pages: Dict[str, Dict], pattern: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search pages using regular expression pattern.\n",
    "        \n",
    "        Args:\n",
    "            pages: Dictionary of pages from LinkCollector\n",
    "            pattern: Regular expression pattern\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with search results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        regex = re.compile(pattern)\n",
    "        \n",
    "        for url, page_data in pages.items():\n",
    "            soup = page_data['soup']\n",
    "            \n",
    "            # Get all text elements\n",
    "            elements = self.extract_text_with_context(soup)\n",
    "            \n",
    "            for element in elements:\n",
    "                text = element['text']\n",
    "                \n",
    "                # Find all matches\n",
    "                matches = regex.findall(text)\n",
    "                \n",
    "                if matches:\n",
    "                    results.append({\n",
    "                        'url': url,\n",
    "                        'text': element['text'],\n",
    "                        'element_type': element['element_type'],\n",
    "                        'context': element['context'],\n",
    "                        'regex_matches': matches,\n",
    "                        'html': element['html']\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_by_css_selector(self, pages: Dict[str, Dict], css_selector: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search pages using CSS selector.\n",
    "        \n",
    "        Args:\n",
    "            pages: Dictionary of pages from LinkCollector\n",
    "            css_selector: CSS selector string\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with search results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for url, page_data in pages.items():\n",
    "            soup = page_data['soup']\n",
    "            \n",
    "            # Find elements matching the CSS selector\n",
    "            matching_elements = soup.select(css_selector)\n",
    "            \n",
    "            for element in matching_elements:\n",
    "                text = element.get_text().strip()\n",
    "                \n",
    "                if text and len(text) >= self.min_content_length:\n",
    "                    results.append({\n",
    "                        'url': url,\n",
    "                        'text': text,\n",
    "                        'element_type': element.name,\n",
    "                        'css_selector': css_selector,\n",
    "                        'html': str(element)\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def semantic_search(self, pages: Dict[str, Dict], query: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform semantic search on pages using sentence transformers.\n",
    "        \n",
    "        Args:\n",
    "            pages: Dictionary of pages from LinkCollector\n",
    "            query: Search query\n",
    "            top_k: Number of top results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with search results, sorted by relevance\n",
    "        \"\"\"\n",
    "        # Encode the query\n",
    "        query_embedding = self.transformer.encode([query])[0]\n",
    "        \n",
    "        all_elements = []\n",
    "        \n",
    "        # Extract text elements from all pages\n",
    "        for url, page_data in pages.items():\n",
    "            soup = page_data['soup']\n",
    "            elements = self.extract_text_with_context(soup)\n",
    "            \n",
    "            for element in elements:\n",
    "                all_elements.append({\n",
    "                    'url': url,\n",
    "                    'text': element['text'],\n",
    "                    'element_type': element['element_type'],\n",
    "                    'context': element['context'],\n",
    "                    'html': element['html']\n",
    "                })\n",
    "        \n",
    "        # Calculate relevance scores\n",
    "        for element in all_elements:\n",
    "            element['relevance_score'] = self.calculate_relevance_score(element['text'], query_embedding)\n",
    "        \n",
    "        # Sort by relevance score and take top_k\n",
    "        results = sorted(all_elements, key=lambda x: x['relevance_score'], reverse=True)[:top_k]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def extract_facts_from_pages(self, pages: Dict[str, Dict], topics: List[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract facts from pages with optional topic filtering.\n",
    "        \n",
    "        Args:\n",
    "            pages: Dictionary of pages from LinkCollector\n",
    "            topics: Optional list of topics to filter by\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with extracted facts\n",
    "        \"\"\"\n",
    "        # Reset storage\n",
    "        self.extracted_facts = []\n",
    "        self.entity_index = defaultdict(list)\n",
    "        self.keyword_index = defaultdict(list)\n",
    "        self.url_to_facts = defaultdict(list)\n",
    "        \n",
    "        # Encode topics if provided\n",
    "        topic_embeddings = None\n",
    "        if topics:\n",
    "            topic_embeddings = self.transformer.encode(topics)\n",
    "        \n",
    "        for url, page_data in pages.items():\n",
    "            soup = page_data['soup']\n",
    "            metadata = page_data['metadata']\n",
    "            \n",
    "            # Extract text elements\n",
    "            elements = self.extract_text_with_context(soup)\n",
    "            \n",
    "            # Extract structured data\n",
    "            structured_data = self.extract_structured_data(soup)\n",
    "            \n",
    "            # Process text elements\n",
    "            for element in elements:\n",
    "                text = element['text']\n",
    "                \n",
    "                # Skip if too short\n",
    "                if len(text) < self.min_content_length:\n",
    "                    continue\n",
    "                \n",
    "                # Check relevance to topics if provided\n",
    "                if topic_embeddings is not None:\n",
    "                    text_embedding = self.transformer.encode([text])[0]\n",
    "                    similarities = cosine_similarity([text_embedding], topic_embeddings)[0]\n",
    "                    max_similarity = max(similarities)\n",
    "                    \n",
    "                    if max_similarity < self.relevance_threshold:\n",
    "                        continue\n",
    "                    \n",
    "                    topic_relevance = {topics[i]: float(similarities[i]) for i in range(len(topics))}\n",
    "                else:\n",
    "                    topic_relevance = {}\n",
    "                    max_similarity = 1.0\n",
    "                \n",
    "                # Extract entities\n",
    "                entities = self.extract_entities(text)\n",
    "                \n",
    "                # Create fact\n",
    "                fact = {\n",
    "                    'text': text,\n",
    "                    'url': url,\n",
    "                    'element_type': element['element_type'],\n",
    "                    'context': element['context'],\n",
    "                    'entities': entities,\n",
    "                    'topic_relevance': topic_relevance,\n",
    "                    'relevance_score': max_similarity,\n",
    "                    'metadata': metadata,\n",
    "                    'html': element['html']\n",
    "                }\n",
    "                \n",
    "                # Add to storage\n",
    "                fact_idx = len(self.extracted_facts)\n",
    "                self.extracted_facts.append(fact)\n",
    "                self.url_to_facts[url].append(fact_idx)\n",
    "                \n",
    "                # Index entities\n",
    "                for entity_type, entity_list in entities.items():\n",
    "                    for entity in entity_list:\n",
    "                        self.entity_index[entity].append(fact_idx)\n",
    "                \n",
    "                # Index keywords (simple approach - split by spaces and remove punctuation)\n",
    "                words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "                for word in words:\n",
    "                    if len(word) > 3:  # Skip very short words\n",
    "                        self.keyword_index[word].append(fact_idx)\n",
    "            \n",
    "            # Process structured data\n",
    "            for data in structured_data:\n",
    "                if data['type'] == 'table':\n",
    "                    # For tables, create a fact for each row\n",
    "                    headers = data['headers']\n",
    "                    for row in data['rows']:\n",
    "                        if headers and len(headers) == len(row):\n",
    "                            # Create a text representation of the row\n",
    "                            row_text = '. '.join([f\"{headers[i]}: {row[i]}\" for i in range(len(headers))])\n",
    "                        else:\n",
    "                            row_text = '. '.join(row)\n",
    "                        \n",
    "                        # Skip if too short\n",
    "                        if len(row_text) < self.min_content_length:\n",
    "                            continue\n",
    "                        \n",
    "                        # Extract entities\n",
    "                        entities = self.extract_entities(row_text)\n",
    "                        \n",
    "                        # Create fact\n",
    "                        fact = {\n",
    "                            'text': row_text,\n",
    "                            'url': url,\n",
    "                            'element_type': 'table_row',\n",
    "                            'context': [],\n",
    "                            'entities': entities,\n",
    "                            'topic_relevance': {},\n",
    "                            'relevance_score': 1.0,\n",
    "                            'metadata': metadata,\n",
    "                            'structured_data': {\n",
    "                                'type': 'table_row',\n",
    "                                'headers': headers,\n",
    "                                'values': row\n",
    "                            },\n",
    "                            'html': data['html']\n",
    "                        }\n",
    "                        \n",
    "                        # Add to storage\n",
    "                        fact_idx = len(self.extracted_facts)\n",
    "                        self.extracted_facts.append(fact)\n",
    "                        self.url_to_facts[url].append(fact_idx)\n",
    "                        \n",
    "                        # Index entities\n",
    "                        for entity_type, entity_list in entities.items():\n",
    "                            for entity in entity_list:\n",
    "                                self.entity_index[entity].append(fact_idx)\n",
    "                \n",
    "                elif data['type'] == 'list':\n",
    "                    # For lists, create a fact for the entire list\n",
    "                    list_text = '. '.join(data['items'])\n",
    "                    \n",
    "                    # Skip if too short\n",
    "                    if len(list_text) < self.min_content_length:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract entities\n",
    "                    entities = self.extract_entities(list_text)\n",
    "                    \n",
    "                    # Create fact\n",
    "                    fact = {\n",
    "                        'text': list_text,\n",
    "                        'url': url,\n",
    "                        'element_type': f\"{data['list_type']}_list\",\n",
    "                        'context': [],\n",
    "                        'entities': entities,\n",
    "                        'topic_relevance': {},\n",
    "                        'relevance_score': 1.0,\n",
    "                        'metadata': metadata,\n",
    "                        'structured_data': {\n",
    "                            'type': 'list',\n",
    "                            'list_type': data['list_type'],\n",
    "                            'items': data['items']\n",
    "                        },\n",
    "                        'html': data['html']\n",
    "                    }\n",
    "                    \n",
    "                    # Add to storage\n",
    "                    fact_idx = len(self.extracted_facts)\n",
    "                    self.extracted_facts.append(fact)\n",
    "                    self.url_to_facts[url].append(fact_idx)\n",
    "                    \n",
    "                    # Index entities\n",
    "                    for entity_type, entity_list in entities.items():\n",
    "                        for entity in entity_list:\n",
    "                            self.entity_index[entity].append(fact_idx)\n",
    "        \n",
    "        return self.extracted_facts\n",
    "    \n",
    "    def search_facts_by_entity(self, entity: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search extracted facts by entity.\n",
    "        \n",
    "        Args:\n",
    "            entity: Entity to search for\n",
    "            \n",
    "        Returns:\n",
    "            List of facts containing the entity\n",
    "        \"\"\"\n",
    "        fact_indices = self.entity_index.get(entity, [])\n",
    "        return [self.extracted_facts[idx] for idx in fact_indices]\n",
    "    \n",
    "    def search_facts_by_keyword(self, keyword: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search extracted facts by keyword.\n",
    "        \n",
    "        Args:\n",
    "            keyword: Keyword to search for\n",
    "            \n",
    "        Returns:\n",
    "            List of facts containing the keyword\n",
    "        \"\"\"\n",
    "        keyword = keyword.lower()\n",
    "        fact_indices = self.keyword_index.get(keyword, [])\n",
    "        return [self.extracted_facts[idx] for idx in fact_indices]\n",
    "    \n",
    "    def get_facts_by_url(self, url: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get all facts extracted from a specific URL.\n",
    "        \n",
    "        Args:\n",
    "            url: URL to get facts for\n",
    "            \n",
    "        Returns:\n",
    "            List of facts from the URL\n",
    "        \"\"\"\n",
    "        fact_indices = self.url_to_facts.get(url, [])\n",
    "        return [self.extracted_facts[idx] for idx in fact_indices]\n",
    "    \n",
    "    def find_related_facts(self, fact_idx: int, threshold: float = 0.7) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Find facts related to a given fact based on semantic similarity.\n",
    "        \n",
    "        Args:\n",
    "            fact_idx: Index of the fact to find related facts for\n",
    "            threshold: Minimum similarity threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples (fact_idx, similarity_score)\n",
    "        \"\"\"\n",
    "        if fact_idx >= len(self.extracted_facts):\n",
    "            return []\n",
    "        \n",
    "        fact = self.extracted_facts[fact_idx]\n",
    "        fact_embedding = self.transformer.encode([fact['text']])[0]\n",
    "        \n",
    "        related = []\n",
    "        \n",
    "        # Encode all other facts\n",
    "        for i, other_fact in enumerate(self.extracted_facts):\n",
    "            if i == fact_idx:\n",
    "                continue\n",
    "            \n",
    "            other_embedding = self.transformer.encode([other_fact['text']])[0]\n",
    "            similarity = cosine_similarity([fact_embedding], [other_embedding])[0][0]\n",
    "            \n",
    "            if similarity >= threshold:\n",
    "                related.append((i, float(similarity)))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        related.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return related\n",
    "    \n",
    "    def classify_content(self, text: str, categories: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Classify content into predefined categories using semantic similarity.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to classify\n",
    "            categories: List of category names\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping categories to confidence scores\n",
    "        \"\"\"\n",
    "        # Encode text and categories\n",
    "        text_embedding = self.transformer.encode([text])[0]\n",
    "        category_embeddings = self.transformer.encode(categories)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity([text_embedding], category_embeddings)[0]\n",
    "        \n",
    "        # Create result dictionary\n",
    "        result = {categories[i]: float(similarities[i]) for i in range(len(categories))}\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Scrape Website Data with Advanced Link Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LinkCollector with improved capabilities\n",
    "collector = LinkCollector(\n",
    "    start_urls=[\n",
    "        \"https://civichonors.com/\",\n",
    "        \"https://www.nelslindahl.com/\"\n",
    "    ],\n",
    "    allowed_domains=[\"civichonors.com\", \"nelslindahl.com\"],\n",
    "    max_depth=1,  # Follow links one level deep\n",
    "    respect_robots_txt=True,\n",
    "    rate_limit=1.0,  # Wait 1 second between requests\n",
    "    max_urls=10,  # Limit to 10 URLs for demonstration\n",
    "    link_filters=[filter_pdf_urls, filter_image_urls]  # Filter out PDFs and images\n",
    ")\n",
    "\n",
    "# Collect links and pages\n",
    "pages = collector.collect_links()\n",
    "\n",
    "# Print summary of collected pages\n",
    "print(f\"\\nCollected {len(pages)} pages:\")\n",
    "for url in pages.keys():\n",
    "    print(f\"  - {url}\")\n",
    "    \n",
    "    # Print number of links found on this page\n",
    "    links = pages[url]['links']\n",
    "    print(f\"    Found {len(links)} links on this page\")\n",
    "    \n",
    "    # Print metadata\n",
    "    metadata = pages[url]['metadata']\n",
    "    print(f\"    Title: {metadata.get('title')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Populate Knowledge Graph with Advanced Page Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PageSearcher\n",
    "searcher = PageSearcher()\n",
    "\n",
    "# Extract facts from collected pages with topic filtering\n",
    "facts = searcher.extract_facts_from_pages(pages, topics=[\"civic honors\", \"community service\", \"volunteering\"])\n",
    "\n",
    "print(f\"Extracted {len(facts)} facts from {len(pages)} pages\")\n",
    "\n",
    "# Initialize the KnowledgeGraph\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "# Add extracted facts to the knowledge graph\n",
    "for i, fact in enumerate(facts):\n",
    "    # Create a unique ID for the fact\n",
    "    fact_id = f\"fact_{i}\"\n",
    "    \n",
    "    # Get the source ID from the URL\n",
    "    url = fact['url']\n",
    "    domain = urllib.parse.urlparse(url).netloc\n",
    "    source_id = domain.replace('www.', '').split('.')[0].capitalize()\n",
    "    \n",
    "    # Get metadata\n",
    "    metadata = fact['metadata']\n",
    "    \n",
    "    # Add the fact to the knowledge graph\n",
    "    kg.add_fact(\n",
    "        fact_id=fact_id,\n",
    "        fact_statement=fact['text'],\n",
    "        category=\"General\",\n",
    "        tags=[source_id, \"WebScraped\"],\n",
    "        date_recorded=datetime.datetime.now(),\n",
    "        last_updated=datetime.datetime.now(),\n",
    "        reliability_rating=ReliabilityRating.LIKELY_TRUE,\n",
    "        source_id=source_id,\n",
    "        source_title=metadata.get('title', f\"{source_id} Website\"),\n",
    "        author_creator=metadata.get('author', \"Web Scraping\"),\n",
    "        publication_date=datetime.datetime.now(),\n",
    "        url_reference=url,\n",
    "        related_facts=[],\n",
    "        contextual_notes=f\"Extracted from {source_id} website\",\n",
    "        access_level=\"Public\",\n",
    "        usage_count=0\n",
    "    )\n",
    "\n",
    "print(f\"Added {kg.get_fact_count()} facts to the knowledge graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Retrieve and Display Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of facts from the knowledge graph\n",
    "sample_size = min(5, kg.get_fact_count())\n",
    "print(f\"Sample of {sample_size} facts from the knowledge graph:\\n\")\n",
    "\n",
    "for i in range(sample_size):\n",
    "    fact = kg.data[i]\n",
    "    print(f\"Fact ID: {fact['fact_id']}\")\n",
    "    print(f\"Statement: {fact['fact_statement']}\")\n",
    "    print(f\"Source: {fact['source_id']}\")\n",
    "    print(f\"URL: {fact['url_reference']}\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Ensure Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(knowledge_graph, similarity_threshold=0.9):\n",
    "    \"\"\"Remove duplicate facts from the knowledge graph based on string similarity.\"\"\"\n",
    "    unique_facts = []\n",
    "    duplicate_count = 0\n",
    "    \n",
    "    for fact in knowledge_graph.data:\n",
    "        is_duplicate = False\n",
    "        for unique_fact in unique_facts:\n",
    "            similarity = SequenceMatcher(None, fact[\"fact_statement\"], unique_fact[\"fact_statement\"]).ratio()\n",
    "            if similarity >= similarity_threshold:\n",
    "                is_duplicate = True\n",
    "                duplicate_count += 1\n",
    "                break\n",
    "        \n",
    "        if not is_duplicate:\n",
    "            unique_facts.append(fact)\n",
    "    \n",
    "    # Create a new knowledge graph with unique facts\n",
    "    unique_kg = KnowledgeGraph()\n",
    "    unique_kg.data = unique_facts\n",
    "    \n",
    "    return unique_kg, duplicate_count\n",
    "\n",
    "# Remove duplicates\n",
    "unique_kg, duplicate_count = remove_duplicates(kg)\n",
    "\n",
    "print(f\"Removed {duplicate_count} duplicate facts\")\n",
    "print(f\"Original knowledge graph: {kg.get_fact_count()} facts\")\n",
    "print(f\"Unique knowledge graph: {unique_kg.get_fact_count()} facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Advanced Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_knowledge_graph(knowledge_graph, similarity_threshold=0.85):\n",
    "    \"\"\"Clean the knowledge graph by removing semantically similar facts.\"\"\"\n",
    "    # Load the sentence transformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Extract fact statements\n",
    "    facts = [fact[\"fact_statement\"] for fact in knowledge_graph.data]\n",
    "    \n",
    "    # Compute embeddings\n",
    "    embeddings = model.encode(facts)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Find groups of similar facts\n",
    "    groups = []\n",
    "    processed = set()\n",
    "    \n",
    "    for i in range(len(facts)):\n",
    "        if i in processed:\n",
    "            continue\n",
    "        \n",
    "        group = [i]\n",
    "        processed.add(i)\n",
    "        \n",
    "        for j in range(i+1, len(facts)):\n",
    "            if j in processed:\n",
    "                continue\n",
    "            \n",
    "            if similarity_matrix[i, j] >= similarity_threshold:\n",
    "                group.append(j)\n",
    "                processed.add(j)\n",
    "        \n",
    "        groups.append(group)\n",
    "    \n",
    "    # Create a new knowledge graph with one fact from each group\n",
    "    cleaned_kg = KnowledgeGraph()\n",
    "    \n",
    "    for group in groups:\n",
    "        # Choose the fact with the highest reliability rating\n",
    "        best_fact_idx = group[0]\n",
    "        best_rating = knowledge_graph.data[best_fact_idx][\"reliability_rating\"]\n",
    "        \n",
    "        for idx in group[1:]:\n",
    "            rating = knowledge_graph.data[idx][\"reliability_rating\"]\n",
    "            if rating.value > best_rating.value:\n",
    "                best_fact_idx = idx\n",
    "                best_rating = rating\n",
    "        \n",
    "        cleaned_kg.data.append(knowledge_graph.data[best_fact_idx])\n",
    "    \n",
    "    return cleaned_kg, len(knowledge_graph.data) - len(cleaned_kg.data)\n",
    "\n",
    "# Clean the knowledge graph\n",
    "cleaned_kg, removed_count = clean_knowledge_graph(unique_kg)\n",
    "\n",
    "print(f\"Removed {removed_count} semantically similar facts\")\n",
    "print(f\"Unique knowledge graph: {unique_kg.get_fact_count()} facts\")\n",
    "print(f\"Cleaned knowledge graph: {cleaned_kg.get_fact_count()} facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Enhanced Knowledge Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the EnhancedKnowledgeReduction\n",
    "reducer = EnhancedKnowledgeReduction()\n",
    "\n",
    "# Reduce the knowledge graph\n",
    "reduced_kg = reducer.reduce_knowledge(cleaned_kg)\n",
    "\n",
    "print(f\"Cleaned knowledge graph: {cleaned_kg.get_fact_count()} facts\")\n",
    "print(f\"Reduced knowledge graph: {reduced_kg.get_fact_count()} facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the knowledge graph to a file\n",
    "reduced_kg.save_to_file('civic_honors_kg.json')\n",
    "print(\"Knowledge graph saved to civic_honors_kg.json\")\n",
    "\n",
    "# Save the entity graph\n",
    "reducer.save_graph('entity_graph.json')\n",
    "print(\"Entity graph saved to entity_graph.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated an enhanced knowledge graph for Civic Honors content with advanced link collection and page search capabilities. The improvements allow the knowledge graph to discover and incorporate information from a broader range of relevant sources, rather than just the two predefined URLs in the original implementation.\n",
    "\n",
    "Key improvements include:\n",
    "\n",
    "1. **Advanced Link Collection**: The LinkCollector class provides comprehensive link collection capabilities including URL normalization, depth control, domain filtering, robots.txt compliance, rate limiting, and metadata extraction.\n",
    "\n",
    "2. **Intelligent Page Searching**: The PageSearcher class provides advanced search and extraction capabilities including keyword search, semantic search, structured data extraction, entity recognition, and content classification.\n",
    "\n",
    "3. **Enhanced Knowledge Reduction**: The EnhancedKnowledgeReduction class provides advanced techniques for reducing redundancy and improving the quality of the knowledge graph.\n",
    "\n",
    "These improvements significantly enhance the knowledge graph's ability to collect, search, and process information from relevant sources, resulting in a more comprehensive and accurate representation of Civic Honors content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
