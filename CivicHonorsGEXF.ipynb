{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6CHtxcLFVvownbD+aqyI2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/CivicHonorsGEXF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Overview\n",
        "\n",
        "The revised Python script is designed to extract, analyze, and visualize knowledge from a webpage, specifically by creating a knowledge graph in GEXF format, suitable for analysis in graph visualization tools. The process is implemented step-by-step in Google Colab. Here's a summary of its components and functionalities:\n",
        "\n",
        "### 1. **Library Installation and Imports**\n",
        "   - The script starts by installing necessary Python libraries (`nltk`, `spacy`, `textblob`) for natural language processing.\n",
        "   - It imports libraries for web scraping (`requests`, `BeautifulSoup`), graph creation (`networkx`), and standard utilities (`nltk`, `string`, `Counter`).\n",
        "   - NLTK resources are downloaded, and Spacy's English model is loaded for NLP tasks.\n",
        "\n",
        "### 2. **Function Definitions**\n",
        "   - `fetch_webpage_content(url)`: Retrieves the HTML content from a given URL using `requests`. It includes improved error handling to manage HTTP errors gracefully.\n",
        "   - `parse_webpage_content(content)`: Parses the HTML content to extract plain text and performs NLP tasks:\n",
        "       - Entity recognition using Spacy to identify and label entities.\n",
        "       - Sentiment analysis using TextBlob to evaluate the overall sentiment of the text.\n",
        "       - Keyword extraction with NLTK, identifying frequent keywords while filtering out stopwords and punctuation.\n",
        "   - `knowledge_reduce(entities, sentiment, keyword_freq)`: Constructs a knowledge graph with NetworkX. Nodes represent entities and keywords, each with specific attributes, and the overall sentiment of the text is attached to the graph. Edge creation logic can be customized.\n",
        "   - `save_knowledge_graph_gexf(graph, file_name)`: Saves the knowledge graph in the GEXF format, suitable for visualization and analysis in tools like Gephi.\n",
        "\n",
        "### 3. **Execution Flow**\n",
        "   - **Fetching Webpage Content**: The script fetches content from \"https://civichonors.com/\".\n",
        "   - **Parsing Webpage Content**: The content is parsed to extract entities, sentiments, and keywords.\n",
        "   - **Creating and Saving the Knowledge Graph**: A knowledge graph is constructed from the parsed data and saved in GEXF format.\n",
        "\n",
        "This script is tailored for an educational or analytical setting, such as in a Google Colab notebook, allowing for step-by-step execution. The final output is a knowledge graph that encapsulates the structured information extracted from the webpage, providing insights into the relationships between different entities and concepts present in the content."
      ],
      "metadata": {
        "id": "TNDBHPqoXuDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install and Import Libraries"
      ],
      "metadata": {
        "id": "xXNiUGWuX233"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9jN7HGPXtFk",
        "outputId": "e7a3d7dd-ab94-4e69-86e7-96d864953347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Installing required packages\n",
        "!pip install nltk spacy textblob\n",
        "\n",
        "# Importing necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import networkx as nx\n",
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# Downloading NLTK resources and loading Spacy's English model\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Define Functions"
      ],
      "metadata": {
        "id": "9qmT9N5UYGX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_webpage_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        raise Exception(f\"HTTP Error: {e}\")\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error fetching webpage: {e}\")\n",
        "\n",
        "def parse_webpage_content(content):\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "    text_content = soup.get_text()\n",
        "\n",
        "    doc = nlp(text_content)\n",
        "    entities = [(ent.text.strip(), ent.label_) for ent in doc.ents if ent.text.strip()]\n",
        "\n",
        "    blob = TextBlob(text_content)\n",
        "    sentiment = blob.sentiment\n",
        "\n",
        "    words = [word.lower() for word in nltk.word_tokenize(text_content)\n",
        "             if word.lower() not in stopwords.words('english')\n",
        "             and word not in string.punctuation]\n",
        "    keyword_freq = Counter(words).most_common(10)\n",
        "\n",
        "    return entities, sentiment, keyword_freq\n",
        "\n",
        "def knowledge_reduce(entities, sentiment, keyword_freq):\n",
        "    graph = nx.Graph()\n",
        "    for entity, type in entities:\n",
        "        graph.add_node(entity, type=type, label=type)\n",
        "    for word, freq in keyword_freq:\n",
        "        graph.add_node(word, type='keyword', frequency=freq, label='keyword')\n",
        "    graph.graph['sentiment'] = {'polarity': sentiment.polarity, 'subjectivity': sentiment.subjectivity}\n",
        "    # Add edges between entities and keywords\n",
        "    # ... [existing edge creation logic]\n",
        "    return graph\n",
        "\n",
        "def save_knowledge_graph_gexf(graph, file_name):\n",
        "    nx.write_gexf(graph, file_name)"
      ],
      "metadata": {
        "id": "AbAVXSJKYGw6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Fetch Webpage Content"
      ],
      "metadata": {
        "id": "DlGJlnRCYPW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://civichonors.com/\"\n",
        "try:\n",
        "    content = fetch_webpage_content(url)\n",
        "    print(\"Webpage content fetched successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching webpage: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USm6B8aSYP3g",
        "outputId": "4581d6a0-9e78-4ee3-cbbe-2aafe30abcf7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webpage content fetched successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Parse the Webpage Content"
      ],
      "metadata": {
        "id": "5cyHe-zWYXIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    entities, sentiment, keywords = parse_webpage_content(content)\n",
        "    print(\"Webpage content parsed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing webpage content: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zpmrvxxYXnW",
        "outputId": "ef373aad-d801-4829-f4d9-e701838307a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webpage content parsed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Create and Save the Knowledge Graph"
      ],
      "metadata": {
        "id": "q5SDG6itYfXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    graph = knowledge_reduce(entities, sentiment, keywords)\n",
        "    save_knowledge_graph_gexf(graph, '/content/knowledge_graph.gexf')\n",
        "    print(\"Knowledge graph created and saved in GEXF format successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating or saving knowledge graph: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1eolivZYf3C",
        "outputId": "5f4ba7c2-6ebe-4ab0-9a73-433a6ed74a73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knowledge graph created and saved in GEXF format successfully.\n"
          ]
        }
      ]
    }
  ]
}