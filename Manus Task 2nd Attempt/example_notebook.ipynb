"""
Example notebook demonstrating the KnowledgeReduce framework.

This notebook shows how to use the KnowledgeReduce framework to build
a knowledge graph from web sources, clean it, and analyze the results.
"""

# %% [markdown]
# # KnowledgeReduce Example
# 
# This notebook demonstrates how to use the KnowledgeReduce framework to:
# 1. Extract facts from web sources
# 2. Build a knowledge graph
# 3. Clean and deduplicate data
# 4. Analyze and visualize the results
# 5. Serialize the graph for portability

# %% [markdown]
# ## Setup
# 
# First, let's install the required dependencies:

# %%
# Uncomment to install dependencies
# !pip install networkx requests beautifulsoup4 spacy
# !python -m spacy download en_core_web_md

# %% [markdown]
# ## Import Libraries

# %%
import os
import json
import networkx as nx
import matplotlib.pyplot as plt
from datetime import datetime

# Import KnowledgeReduce components
from knowledge_reduce.graph import ReliabilityRating, KnowledgeGraph
from knowledge_reduce.extraction import (
    extract_facts_from_url,
    extract_facts_from_multiple_urls,
    populate_knowledge_graph_from_urls
)
from knowledge_reduce.processing import (
    remove_duplicate_facts,
    advanced_cleaning,
    semantic_cleaning,
    clean_knowledge_graph
)
from knowledge_reduce.utils import (
    serialize_knowledge_graph,
    deserialize_knowledge_graph
)
from knowledge_reduce.main import (
    build_knowledge_graph_from_urls,
    save_knowledge_graph,
    load_knowledge_graph
)

# %% [markdown]
# ## Define Data Sources
# 
# We'll extract facts from a few websites:

# %%
# Define URLs to scrape
urls = {
    "CivicHonors": "https://civichonors.com/",
    "NelsLindahl": "https://www.nelslindahl.com/"
}

print(f"Data sources: {', '.join(urls.keys())}")

# %% [markdown]
# ## Extract Facts and Build Knowledge Graph
# 
# Now we'll extract facts from these websites and build our knowledge graph:

# %%
# Method 1: High-level function (recommended for most use cases)
print("Building knowledge graph using high-level function...")
kg, stats = build_knowledge_graph_from_urls(
    urls,
    category="Example",
    tags=["Example", "WebScraped"],
    reliability_rating=ReliabilityRating.LIKELY_TRUE,
    clean=True  # Automatically clean the data
)

# Print statistics
print("\nKnowledge Graph Statistics:")
print(f"- Initial facts extracted: {stats['initial_count']}")
print(f"- Duplicates removed: {stats['duplicates_removed']}")
print(f"- Short facts removed: {stats['short_removed']}")
print(f"- Similar facts removed: {stats['similar_removed']}")
print(f"- Semantically similar facts removed: {stats['semantic_removed']}")
print(f"- Final fact count: {stats['final_count']}")

# %% [markdown]
# ## Method 2: Step-by-Step Approach
# 
# Alternatively, we can build the knowledge graph step by step for more control:

# %%
# Create a new knowledge graph
kg2 = KnowledgeGraph()

# Extract facts from URLs
print("\nExtracting facts step by step...")
for source_id, url in urls.items():
    facts, _, source_title = extract_facts_from_url(url, source_id)
    print(f"Extracted {len(facts)} facts from {source_id}")
    
    # Add facts to knowledge graph
    for i, fact in enumerate(facts):
        kg2.add_fact(
            fact_id=f"{source_id}_{i}",
            fact_statement=fact,
            category="Example",
            tags=["Example", "WebScraped", source_id],
            date_recorded=datetime.now(),
            reliability_rating=ReliabilityRating.LIKELY_TRUE,
            source_id=source_id,
            source_title=source_title,
            url_reference=url
        )

print(f"Total facts added: {len(kg2.get_all_facts())}")

# %% [markdown]
# ## Clean the Knowledge Graph
# 
# Let's clean the knowledge graph to remove duplicates and low-quality facts:

# %%
# Method 2 (continued): Clean the knowledge graph step by step
print("\nCleaning knowledge graph step by step...")

# Remove duplicates
duplicates = remove_duplicate_facts(kg2)
print(f"Removed {duplicates} duplicate facts")

# Remove short facts and similar facts
short_removed, similar_removed = advanced_cleaning(
    kg2,
    similarity_threshold=0.8,
    short_fact_threshold=50
)
print(f"Removed {short_removed} short facts")
print(f"Removed {similar_removed} similar facts")

# Remove semantically similar facts
semantic_removed = semantic_cleaning(kg2, similarity_threshold=0.85)
print(f"Removed {semantic_removed} semantically similar facts")

print(f"Final fact count: {len(kg2.get_all_facts())}")

# %% [markdown]
# ## Analyze the Knowledge Graph
# 
# Let's examine some of the facts in our knowledge graph:

# %%
# Get all facts
facts = kg.get_all_facts()

# Print a sample of facts
print("\nSample facts:")
for i, fact in enumerate(facts[:5]):
    print(f"{i+1}. {fact['fact_statement'][:100]}...")
    print(f"   Category: {fact['category']}")
    print(f"   Tags: {fact['tags']}")
    print(f"   Reliability: {fact['reliability_rating']}")
    print()

# %% [markdown]
# ## Visualize the Knowledge Graph
# 
# Let's create a simple visualization of our knowledge graph:

# %%
# Create a simplified graph for visualization
G = nx.DiGraph()

# Add nodes for each fact
for fact in facts[:20]:  # Limit to 20 facts for clarity
    G.add_node(fact['fact_id'], label=fact['fact_statement'][:30])
    
    # Add edges for related facts if they exist
    for related_id in fact.get('related_facts', []):
        if related_id in G:
            G.add_edge(fact['fact_id'], related_id)

# Visualize
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G, seed=42)
nx.draw(G, pos, with_labels=False, node_size=300, node_color="skyblue", alpha=0.8)

# Add labels
labels = nx.get_node_attributes(G, 'label')
nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)

plt.title("Knowledge Graph Visualization")
plt.axis('off')
plt.tight_layout()
plt.show()

# %% [markdown]
# ## Serialize the Knowledge Graph
# 
# Finally, let's save our knowledge graph for later use:

# %%
# Create output directory if it doesn't exist
os.makedirs("output", exist_ok=True)

# Save knowledge graph
output_file = "output/example_knowledge_graph.json"
success = save_knowledge_graph(kg, output_file)

if success:
    print(f"\nKnowledge graph saved to {output_file}")
else:
    print("\nFailed to save knowledge graph")

# %% [markdown]
# ## Load the Knowledge Graph
# 
# We can also load a previously saved knowledge graph:

# %%
# Load knowledge graph
loaded_kg = load_knowledge_graph(output_file)

if loaded_kg is not None:
    print(f"\nSuccessfully loaded knowledge graph with {len(loaded_kg.get_all_facts())} facts")
else:
    print("\nFailed to load knowledge graph")

# %% [markdown]
# ## Conclusion
# 
# In this notebook, we've demonstrated how to:
# 
# 1. Extract facts from web sources
# 2. Build a knowledge graph with rich metadata
# 3. Clean and deduplicate the data
# 4. Analyze and visualize the knowledge graph
# 5. Serialize and deserialize the graph for portability
# 
# The KnowledgeReduce framework provides a flexible and powerful way to build knowledge graphs from diverse data sources.
