{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/CivicHonorsKGv12_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLVuQtowdT4j"
      },
      "source": [
        "# Code Summary\n",
        "\n",
        "The code developed for execution in Google Colab represents a comprehensive workflow for extracting, processing, and organizing data from a website into a knowledge graph. Here's a summary of each step in the code:\n",
        "\n",
        "### Step 1: Install Necessary Libraries\n",
        "- Installs the required Python libraries: `requests` for web scraping, `beautifulsoup4` for HTML parsing, `networkx` for graph operations, and `spacy` for natural language processing.\n",
        "\n",
        "### Step 2: Import Libraries and Define Classes\n",
        "- Imports essential Python modules and defines two classes:\n",
        "  - `ReliabilityRating`: An enumeration (Enum) for categorizing the reliability of facts.\n",
        "  - `KnowledgeGraph`: A class for creating and managing a knowledge graph, including methods for adding facts, calculating quality scores, updating these scores, and retrieving facts.\n",
        "\n",
        "### Step 3: Scrape Data from the Website\n",
        "- Fetches and parses the HTML content of a specified website, extracting text data from common HTML elements like paragraphs, headings, and lists.\n",
        "\n",
        "### Step 4: Store Extracted Data in the KnowledgeGraph\n",
        "- Initializes an instance of the `KnowledgeGraph`.\n",
        "- Stores each extracted fact as a node in the graph with attributes such as fact statement, category, tags, etc.\n",
        "\n",
        "### Step 5: Retrieve and Display 10 Facts\n",
        "- Displays the first 10 facts from the knowledge graph, giving an overview of the extracted data, and prints the total number of facts extracted.\n",
        "\n",
        "### Step 6: Store the KnowledgeGraph in GEXF and GraphML Formats\n",
        "- Saves the graph in GEXF and GraphML file formats, suitable for analysis and visualization in various graph analysis tools.\n",
        "\n",
        "### Step 7: Ensure Uniqueness of Facts in the Dataset\n",
        "- Removes duplicate facts from the knowledge graph based on their statements to maintain data uniqueness.\n",
        "\n",
        "### Step 8: Advanced Cleaning and Combining of Facts\n",
        "- Implements advanced cleaning techniques, such as removing very short facts and deduplicating semantically similar facts using basic string comparison.\n",
        "\n",
        "### Step 9: Super Aggressive Advanced Cleaning (Refined)\n",
        "- Employs the `en_core_web_md` spaCy model for NLP tasks.\n",
        "- Removes facts based on complex criteria, like semantic similarity, using NLP techniques to refine the dataset's quality further.\n",
        "\n",
        "### Step 10: Store the Updated KnowledgeGraph in GEXF and GraphML Formats\n",
        "- Saves the graph again in GEXF and GraphML file formats.\n",
        "\n",
        "### Step 11: Serialization and deserialization of the KnowledgeGraph for portability.\n",
        "- This step focuses on making the Knowledge Graph portable by serializing it into a JSON format. This allows for easy storage, transfer, and reconstruction of the graph in different environments. The process includes functions for both serialization and deserialization of the graph.\n",
        "\n",
        "This complete workflow effectively extracts information from a website, transforming it into a structured and clean dataset within a knowledge graph. It's particularly useful in scenarios where data quality, structure, and semantic richness are crucial. The inclusion of advanced NLP techniques in the later steps ensures a high-quality dataset, free from redundancies and rich in diverse information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61hozs7HZsLB"
      },
      "source": [
        "# Step 1: Install Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clRygd5kZpFj"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "!pip install requests beautifulsoup4 networkx spacy\n",
        "!python -m spacy download en_core_web_md\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD_VIFu-Zv2S"
      },
      "source": [
        "# Step 2: Import Libraries and Define Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw1-i2ZzZ1JH"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "# Importing necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import networkx as nx\n",
        "import json\n",
        "from datetime import datetime\n",
        "from enum import Enum\n",
        "import difflib\n",
        "import spacy\n",
        "\n",
        "class ReliabilityRating(Enum):\n",
        "    UNVERIFIED = 1\n",
        "    POSSIBLY_TRUE = 2\n",
        "    LIKELY_TRUE = 3\n",
        "    VERIFIED = 4\n",
        "\n",
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.DiGraph()\n",
        "\n",
        "    def calculate_quality_score(self, reliability_rating, usage_count):\n",
        "        # Adjusted to handle string representation of Enum\n",
        "        rating_value = ReliabilityRating[reliability_rating].value if isinstance(reliability_rating, str) else reliability_rating.value\n",
        "        base_score = 10 * rating_value\n",
        "        usage_bonus = 2 * usage_count\n",
        "        return base_score + usage_bonus\n",
        "\n",
        "    def add_fact(self, fact_id, fact_statement, category, tags, date_recorded, last_updated,\n",
        "                 reliability_rating, source_id, source_title, author_creator,\n",
        "                 publication_date, url_reference, related_facts, contextual_notes,\n",
        "                 access_level, usage_count):\n",
        "        # Convert list and datetime objects to strings\n",
        "        tags_str = ', '.join(tags) if tags else ''\n",
        "        date_recorded_str = date_recorded.isoformat() if isinstance(date_recorded, datetime) else date_recorded\n",
        "        last_updated_str = last_updated.isoformat() if isinstance(last_updated, datetime) else last_updated\n",
        "        publication_date_str = publication_date.isoformat() if isinstance(publication_date, datetime) else publication_date\n",
        "\n",
        "        quality_score = self.calculate_quality_score(reliability_rating, usage_count)\n",
        "        self.graph.add_node(fact_id,\n",
        "                            fact_statement=fact_statement,\n",
        "                            category=category,\n",
        "                            tags=tags_str,\n",
        "                            date_recorded=date_recorded_str,\n",
        "                            last_updated=last_updated_str,\n",
        "                            reliability_rating=reliability_rating,\n",
        "                            quality_score=quality_score,\n",
        "                            source_id=source_id,\n",
        "                            source_title=source_title,\n",
        "                            author_creator=author_creator,\n",
        "                            publication_date=publication_date_str,\n",
        "                            url_reference=url_reference,\n",
        "                            contextual_notes=contextual_notes,\n",
        "                            access_level=access_level,\n",
        "                            usage_count=usage_count)\n",
        "\n",
        "        for related_fact_id in related_facts:\n",
        "            self.graph.add_edge(fact_id, related_fact_id)\n",
        "\n",
        "    def update_quality_score(self, fact_id):\n",
        "        if fact_id not in self.graph:\n",
        "            raise ValueError(\"Fact ID not found in the graph.\")\n",
        "        fact = self.graph.nodes[fact_id]\n",
        "        new_score = self.calculate_quality_score(fact['reliability_rating'], fact['usage_count'])\n",
        "        self.graph.nodes[fact_id]['quality_score'] = new_score\n",
        "\n",
        "    def get_fact(self, fact_id):\n",
        "        if fact_id not in self.graph:\n",
        "            raise ValueError(\"Fact ID not found in the graph.\")\n",
        "        return self.graph.nodes[fact_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn6RrXg_Z55m"
      },
      "source": [
        "# Step 3: Scrape Data from the Website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neHkbBnWZ_8K"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "# Importing necessary libraries\n",
        "# Fetch data from the website\n",
        "url = \"https://civichonors.com/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Function to extract text from a soup object\n",
        "def extract_text(element):\n",
        "    return ' '.join(element.stripped_strings)\n",
        "\n",
        "# Generic function to find facts in common HTML structures\n",
        "def find_facts(soup):\n",
        "    facts = []\n",
        "\n",
        "    # Look for paragraphs\n",
        "    for p in soup.find_all('p'):\n",
        "        text = extract_text(p)\n",
        "        if text: facts.append(text)\n",
        "\n",
        "    # Look for headings\n",
        "    for header_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
        "        for header in soup.find_all(header_tag):\n",
        "            text = extract_text(header)\n",
        "            if text: facts.append(text)\n",
        "\n",
        "    # Look for list items\n",
        "    for li in soup.find_all('li'):\n",
        "        text = extract_text(li)\n",
        "        if text: facts.append(text)\n",
        "\n",
        "    return facts\n",
        "\n",
        "# Extract facts using the generic function\n",
        "facts = find_facts(soup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOUJY03SaBwF"
      },
      "source": [
        "# Step 4: Store Extracted Data in the KnowledgeGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjxxQZSeaHeR"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "# Importing necessary libraries\n",
        "# Initialize the KnowledgeGraph\n",
        "kg = KnowledgeGraph()\n",
        "\n",
        "# Store each fact in the KnowledgeGraph\n",
        "for i, fact in enumerate(facts):\n",
        "    kg.add_fact(\n",
        "        fact_id=str(i),\n",
        "        fact_statement=fact,\n",
        "        category=\"General\",\n",
        "        tags=[\"CivicHonors\", \"WebScraped\"],\n",
        "        date_recorded=datetime.now(),\n",
        "        last_updated=datetime.now(),\n",
        "        reliability_rating=ReliabilityRating.LIKELY_TRUE.name,\n",
        "        source_id=\"CivicHonors\",\n",
        "        source_title=\"Civic Honors Website\",\n",
        "        author_creator=\"Web Scraping\",\n",
        "        publication_date=datetime.now(),\n",
        "        url_reference=url,\n",
        "        related_facts=[],\n",
        "        contextual_notes=\"Extracted from Civic Honors website\",\n",
        "        access_level=\"Public\",\n",
        "        usage_count=0\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S572InVIaIlk"
      },
      "source": [
        "# Step 5: Retrieve and Display 10 Facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GhKDJF9aMjW"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "# Print the total number of facts extracted\n",
        "total_facts = len(facts)\n",
        "print(f\"Total facts extracted: {total_facts}\")\n",
        "\n",
        "# Retrieve and display up to 10 facts\n",
        "for i in range(min(10, total_facts)):\n",
        "    fact_id = str(i)\n",
        "    try:\n",
        "        fact = kg.get_fact(fact_id)\n",
        "        print(f\"Fact {i+1}: {fact}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Fact {i+1}: {str(e)}\")\n",
        "        break  # Stop the loop if a fact ID is not found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJHI4T28a_6d"
      },
      "source": [
        "# Step 6: Store the KnowledgeGraph in GEXF and GraphML Formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC6G0nFKbC33"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "# Store the graph in GEXF format\n",
        "gexf_path = '/content/knowledge_graph.gexf'\n",
        "nx.write_gexf(kg.graph, gexf_path)\n",
        "print(f\"Graph stored in GEXF format at: {gexf_path}\")\n",
        "\n",
        "# Store the graph in GraphML format\n",
        "graphml_path = '/content/knowledge_graph.graphml'\n",
        "nx.write_graphml(kg.graph, graphml_path)\n",
        "print(f\"Graph stored in GraphML format at: {graphml_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceqgxEGxUxdM"
      },
      "source": [
        "# Step 7: Ensure Uniqueness of Facts in the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0ZB11K_U0Gc"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "def remove_duplicate_facts(knowledge_graph):\n",
        "    unique_facts = set()\n",
        "    nodes_to_remove = []\n",
        "\n",
        "    for fact_id, fact_data in knowledge_graph.graph.nodes(data=True):\n",
        "        fact_statement = fact_data['fact_statement']\n",
        "        if fact_statement in unique_facts:\n",
        "            nodes_to_remove.append(fact_id)\n",
        "        else:\n",
        "            unique_facts.add(fact_statement)\n",
        "\n",
        "    # Remove duplicate nodes\n",
        "    for fact_id in nodes_to_remove:\n",
        "        knowledge_graph.graph.remove_node(fact_id)\n",
        "\n",
        "    print(f\"Removed {len(nodes_to_remove)} duplicate facts.\")\n",
        "\n",
        "# Call the function to remove duplicate facts\n",
        "remove_duplicate_facts(kg)\n",
        "\n",
        "# Optional: Print the total number of unique facts remaining\n",
        "print(f\"Total unique facts remaining: {kg.graph.number_of_nodes()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0ZkAAb8VCBh"
      },
      "source": [
        "# Step 8: Advanced Cleaning and Combining of Facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoMDWAoSU06N"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "def advanced_cleaning(knowledge_graph, similarity_threshold=0.8, short_fact_threshold=20):\n",
        "    nodes_to_remove = []\n",
        "    facts_processed = set()\n",
        "\n",
        "    # Remove short facts\n",
        "    for fact_id, fact_data in knowledge_graph.graph.nodes(data=True):\n",
        "        if len(fact_data['fact_statement']) < short_fact_threshold:\n",
        "            nodes_to_remove.append(fact_id)\n",
        "            continue\n",
        "\n",
        "        # Check for similarity with already processed facts\n",
        "        for processed_fact in facts_processed:\n",
        "            similarity = difflib.SequenceMatcher(None, fact_data['fact_statement'], processed_fact).ratio()\n",
        "            if similarity > similarity_threshold:\n",
        "                nodes_to_remove.append(fact_id)\n",
        "                break\n",
        "\n",
        "        facts_processed.add(fact_data['fact_statement'])\n",
        "\n",
        "    # Further custom strategies can be added here\n",
        "\n",
        "    # Remove identified nodes\n",
        "    for fact_id in nodes_to_remove:\n",
        "        knowledge_graph.graph.remove_node(fact_id)\n",
        "\n",
        "    print(f\"Removed {len(nodes_to_remove)} facts based on advanced cleaning criteria.\")\n",
        "\n",
        "# Call the function for advanced cleaning\n",
        "advanced_cleaning(kg)\n",
        "\n",
        "# Optional: Print the total number of facts after cleaning\n",
        "print(f\"Total facts after advanced cleaning: {kg.graph.number_of_nodes()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EJymWj3Yv75"
      },
      "source": [
        "# Step 9: Super Aggressive Advanced Cleaning (Refined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4SxFiWpYyRt"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "# Load the medium-sized English language model with word vectors\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def super_aggressive_cleaning(knowledge_graph, similarity_threshold=0.85):\n",
        "    nodes_to_remove = set()\n",
        "    facts_content = {}\n",
        "\n",
        "    # Pre-process and store spaCy Doc objects for each fact\n",
        "    for fact_id, fact_data in knowledge_graph.graph.nodes(data=True):\n",
        "        try:\n",
        "            doc = nlp(fact_data['fact_statement'])\n",
        "            facts_content[fact_id] = doc\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing fact ID {fact_id}: {str(e)}\")\n",
        "\n",
        "    # Compare each fact with others for semantic similarity\n",
        "    for fact_id, doc in facts_content.items():\n",
        "        # Skip if already marked for removal\n",
        "        if fact_id in nodes_to_remove:\n",
        "            continue\n",
        "\n",
        "        for other_fact_id, other_doc in facts_content.items():\n",
        "            if fact_id != other_fact_id and other_fact_id not in nodes_to_remove:\n",
        "                similarity = doc.similarity(other_doc)\n",
        "                if similarity > similarity_threshold:\n",
        "                    nodes_to_remove.add(other_fact_id)\n",
        "\n",
        "    # Remove identified nodes\n",
        "    for fact_id in nodes_to_remove:\n",
        "        knowledge_graph.graph.remove_node(fact_id)\n",
        "\n",
        "    print(f\"Removed {len(nodes_to_remove)} facts based on super aggressive cleaning criteria.\")\n",
        "\n",
        "# Call the function for super aggressive cleaning\n",
        "super_aggressive_cleaning(kg)\n",
        "\n",
        "# Print the total number of facts after super aggressive cleaning\n",
        "print(f\"Total facts after super aggressive cleaning: {kg.graph.number_of_nodes()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjCxlyHfyJn"
      },
      "source": [
        "# Step 10: Store the Updated KnowledgeGraph in GEXF and GraphML Formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhUge9cEfwOu"
      },
      "outputs": [],
      "source": [
        "# Completeness check passed\n",
        "# Store the graph in GEXF format\n",
        "gexf_path = '/content/knowledge_graph2.gexf'\n",
        "nx.write_gexf(kg.graph, gexf_path)\n",
        "print(f\"Graph stored in GEXF format at: {gexf_path}\")\n",
        "\n",
        "# Store the graph in GraphML format\n",
        "graphml_path = '/content/knowledge_graph2.graphml'\n",
        "nx.write_graphml(kg.graph, graphml_path)\n",
        "print(f\"Graph stored in GraphML format at: {graphml_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1388986"
      },
      "source": [
        "## Step 11: Making the Knowledge Graph Portable\n",
        "\n",
        "This step focuses on making the Knowledge Graph portable by serializing it into a JSON format. This allows for easy storage, transfer, and reconstruction of the graph in different environments. The process includes functions for both serialization and deserialization of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdc01998"
      },
      "outputs": [],
      "source": [
        "class KnowledgeGraphPortable:\n",
        "    def __init__(self, knowledge_graph):\n",
        "        # Assuming the NetworkX graph is stored in an attribute named 'graph' in KnowledgeGraph\n",
        "        self.graph = knowledge_graph.graph\n",
        "\n",
        "    def serialize_to_json(self, filepath):\n",
        "        # Convert graph to a dictionary or suitable structure\n",
        "        graph_data = nx.node_link_data(self.graph)\n",
        "        with open(filepath, 'w') as file:\n",
        "            json.dump(graph_data, file)\n",
        "        print(f\"Graph serialized and saved to {filepath}\")\n",
        "\n",
        "    def deserialize_from_json(self, filepath):\n",
        "        # Read the file and convert it back to a graph\n",
        "        with open(filepath, 'r') as file:\n",
        "            graph_data = json.load(file)\n",
        "        self.graph = nx.node_link_graph(graph_data)\n",
        "        print(f\"Graph deserialized from {filepath}\")\n",
        "\n",
        "# Usage\n",
        "kg_portable = KnowledgeGraphPortable(kg)\n",
        "kg_portable.serialize_to_json('path_to_graph.json')  # Serialize and save the graph\n",
        "kg_portable.deserialize_from_json('path_to_graph.json')  # Deserialize the saved graph\n",
        "print(\"Serialization and deserialization of Knowledge Graph completed successfully.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}