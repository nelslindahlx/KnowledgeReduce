{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# <a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/Enhanced_CivicHonorsKG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# # Enhanced Code Summary\n",
    "# \n",
    "# This notebook is an advanced version of the original CivicHonorsKGv18 notebook, with significant enhancements to the web crawling and knowledge reduction capabilities. The workflow is outlined in detailed steps:\n",
    "# \n",
    "# ### Step 1: Install Necessary Libraries\n",
    "# - Installation of Python libraries including `requests`, `beautifulsoup4`, `networkx`, `spacy`, `scikit-learn`, and `sentence-transformers` for advanced NLP and web crawling.\n",
    "# - Code to restart the runtime\n",
    "# \n",
    "# ### Step 2: Import Libraries and Define Classes\n",
    "# - Incorporation of essential Python modules and establishment of foundational classes:\n",
    "#   - `ReliabilityRating`: An enumeration for classifying information reliability.\n",
    "#   - `KnowledgeGraph`: A versatile class for constructing and managing a knowledge graph.\n",
    "#   - `RecursiveCrawler`: A new class for advanced web crawling with robots.txt handling, rate limiting, and recursive link following.\n",
    "#   - `EnhancedKnowledgeReduction`: A new class implementing advanced knowledge reduction algorithms.\n",
    "# \n",
    "# ### Step 3: Advanced Web Crawling\n",
    "# - Implementation of a recursive crawler that can follow links across multiple pages.\n",
    "# - Respect for robots.txt rules and implementation of rate limiting to avoid overloading servers.\n",
    "# - URL frontier management for efficient crawling across multiple domains.\n",
    "# \n",
    "# ### Step 4: Populate the KnowledgeGraph\n",
    "# - Initialization and population of the `KnowledgeGraph` instance from crawled content.\n",
    "# - Systematic incorporation of facts with comprehensive attributes.\n",
    "# \n",
    "# ### Step 5: Display Extracted Facts\n",
    "# - Presentation of facts from the knowledge graph with statistics on the crawling process.\n",
    "# \n",
    "# ### Step 6: Advanced Knowledge Reduction\n",
    "# - Implementation of multiple similarity calculation methods (spaCy, TF-IDF, transformers).\n",
    "# - Hierarchical clustering for better fact grouping.\n",
    "# - Entity-based knowledge reduction to eliminate redundant information.\n",
    "# - Multi-stage reduction pipeline for more effective deduplication.\n",
    "# \n",
    "# ### Step 7: Serialize the KnowledgeGraph for Portability\n",
    "# - Rendering the Knowledge Graph portable via serialization into JSON format with sharding.\n",
    "# \n",
    "# This enhanced pipeline significantly improves the ability to crawl and process multiple webpages while respecting web etiquette standards, and provides more sophisticated knowledge reduction capabilities for higher quality insights.\n",
    "\n",
    "# # Step 1: Install Necessary Libraries & restart the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_ipython().system('pip install requests beautifulsoup4 networkx spacy scikit-learn sentence-transformers')\n",
    "get_ipython().system('python -m spacy download en_core_web_md')\n",
    "\n",
    "\n",
    "# # Step 2: Import Libraries and Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import time\n",
    "import enum\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class ReliabilityRating(enum.Enum):\n",
    "    LOW = 1\n",
    "    MEDIUM = 2\n",
    "    HIGH = 3\n",
    "\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.fact_count = 0\n",
    "    \n",
    "    def add_fact(self, fact_statement, source_id, source_name, reliability=ReliabilityRating.MEDIUM, \n",
    "                 category=\"General\", tags=None, quality_score=None):\n",
    "        \"\"\"Add a fact to the knowledge graph.\"\"\"\n",
    "        if tags is None:\n",
    "            tags = []\n",
    "        \n",
    "        # Calculate quality score if not provided\n",
    "        if quality_score is None:\n",
    "            quality_score = self._compute_quality_score(fact_statement, reliability)\n",
    "        \n",
    "        fact = {\n",
    "            'fact_id': self.fact_count,\n",
    "            'fact_statement': fact_statement,\n",
    "            'source_id': source_id,\n",
    "            'source_name': source_name,\n",
    "            'reliability': reliability,\n",
    "            'category': category,\n",
    "            'tags': tags,\n",
    "            'quality_score': quality_score\n",
    "        }\n",
    "        \n",
    "        self.data.append(fact)\n",
    "        self.fact_count += 1\n",
    "        return fact['fact_id']\n",
    "    \n",
    "    def _compute_quality_score(self, fact_statement, reliability):\n",
    "        \"\"\"Compute quality score based on fact length and reliability.\"\"\"\n",
    "        # Length component: longer facts (up to a point) are considered higher quality\n",
    "        length = len(fact_statement)\n",
    "        length_score = min(length / 200, 1.0)  # Cap at 1.0 for facts longer than 200 chars\n",
    "        \n",
    "        # Reliability component: convert enum to numerical value\n",
    "        reliability_value = reliability.value / 3.0  # Normalize to [0.33, 1.0]\n",
    "        \n",
    "        # Combine scores (equal weight)\n",
    "        return (length_score + reliability_value) / 2.0\n",
    "    \n",
    "    def update_quality_scores(self):\n",
    "        \"\"\"Update quality scores for all facts.\"\"\"\n",
    "        for fact in self.data:\n",
    "            fact['quality_score'] = self._compute_quality_score(\n",
    "                fact['fact_statement'], fact['reliability'])\n",
    "    \n",
    "    def get_facts_by_quality(self, min_score=0.0, max_facts=None):\n",
    "        \"\"\"Get facts filtered by minimum quality score, sorted by quality.\"\"\"\n",
    "        filtered_facts = [f for f in self.data if f['quality_score'] >= min_score]\n",
    "        sorted_facts = sorted(filtered_facts, key=lambda x: x['quality_score'], reverse=True)\n",
    "        \n",
    "        if max_facts is not None:\n",
    "            return sorted_facts[:max_facts]\n",
    "        return sorted_facts\n",
    "    \n",
    "    def get_facts_by_source(self, source_id):\n",
    "        \"\"\"Get facts from a specific source.\"\"\"\n",
    "        return [f for f in self.data if f['source_id'] == source_id]\n",
    "    \n",
    "    def get_facts_by_category(self, category):\n",
    "        \"\"\"Get facts from a specific category.\"\"\"\n",
    "        return [f for f in self.data if f['category'] == category]\n",
    "    \n",
    "    def get_facts_by_tag(self, tag):\n",
    "        \"\"\"Get facts with a specific tag.\"\"\"\n",
    "        return [f for f in self.data if tag in f['tags']]\n",
    "\n",
    "\n",
    "# # Step 3: Define RecursiveCrawler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RecursiveCrawler:\n",
    "    def __init__(self, start_urls, max_depth=2, max_pages_per_domain=20, respect_robots=True, \n",
    "                 rate_limit=1.0, user_agent=\"KnowledgeReduceCrawler/1.0\"):\n",
    "        \"\"\"\n",
    "        Initialize the recursive crawler.\n",
    "        \n",
    "        Args:\n",
    "            start_urls (list): List of URLs to start crawling from\n",
    "            max_depth (int): Maximum depth to crawl\n",
    "            max_pages_per_domain (int): Maximum pages to crawl per domain\n",
    "            respect_robots (bool): Whether to respect robots.txt\n",
    "            rate_limit (float): Minimum time between requests to the same domain in seconds\n",
    "            user_agent (str): User agent string to use for requests\n",
    "        \"\"\"\n",
    "        self.start_urls = start_urls\n",
    "        self.max_depth = max_depth\n",
    "        self.max_pages_per_domain = max_pages_per_domain\n",
    "        self.respect_robots = respect_robots\n",
    "        self.rate_limit = rate_limit\n",
    "        self.user_agent = user_agent\n",
    "        \n",
    "        # Data structures for crawler management\n",
    "        self.visited_urls = set()\n",
    "        self.url_frontier = []  # URLs to visit\n",
    "        self.domain_last_access = {}  # Domain -> timestamp of last access\n",
    "        self.robots_parsers = {}  # Domain -> robots parser\n",
    "        self.results = []  # Crawled pages content\n",
    "    \n",
    "    def _get_robots_parser(self, url):\n",
    "        \"\"\"Get robots.txt parser for a domain.\"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        \n",
    "        if domain in self.robots_parsers:\n",
    "            return self.robots_parsers[domain]\n",
    "        \n",
    "        # Create new parser\n",
    "        robots_parser = RobotFileParser()\n",
    "        robots_parser.set_url(f\"{domain}/robots.txt\")\n",
    "        \n",
    "        try:\n",
    "            robots_parser.read()\n",
    "            self.robots_parsers[domain] = robots_parser\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading robots.txt for {domain}: {e}\")\n",
    "            # If we can't read robots.txt, create an empty parser that allows everything\n",
    "            robots_parser = RobotFileParser()\n",
    "            self.robots_parsers[domain] = robots_parser\n",
    "        \n",
    "        return robots_parser\n",
    "    \n",
    "    def _can_fetch(self, url):\n",
    "        \"\"\"Check if we can fetch a URL according to robots.txt.\"\"\"\n",
    "        if not self.respect_robots:\n",
    "            return True\n",
    "        \n",
    "        robots_parser = self._get_robots_parser(url)\n",
    "        return robots_parser.can_fetch(self.user_agent, url)\n",
    "    \n",
    "    def _respect_rate_limits(self, domain):\n",
    "        \"\"\"Respect rate limits for a domain by sleeping if necessary.\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if domain in self.domain_last_access:\n",
    "            elapsed = current_time - self.domain_last_access[domain]\n",
    "            if elapsed < self.rate_limit:\n",
    "                time.sleep(self.rate_limit - elapsed)\n",
    "        \n",
    "        self.domain_last_access[domain] = time.time()\n",
    "    \n",
    "    def _add_to_frontier(self, url, depth):\n",
    "        \"\"\"Add URL to frontier if it meets criteria.\"\"\"\n",
    "        if url in self.visited_urls:\n",
    "            return\n",
    "        \n",
    "        parsed_url = urlparse(url)\n",
    "        domain = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        \n",
    "        # Check domain page count\n",
    "        domain_count = sum(1 for visited_url in self.visited_urls \n",
    "                          if urlparse(visited_url).netloc == parsed_url.netloc)\n",
    "        \n",
    "        if domain_count >= self.max_pages_per_domain:\n",
    "            return\n",
    "        \n",
    "        # Check robots.txt\n",
    "        if not self._can_fetch(url):\n",
    "            return\n",
    "        \n",
    "        # Add to frontier\n",
    "        self.url_frontier.append((url, depth))\n",
    "    \n",
    "    def _extract_links(self, soup, base_url):\n",
    "        \"\"\"Extract links from a BeautifulSoup object.\"\"\"\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            # Convert relative URLs to absolute\n",
    "            absolute_url = urljoin(base_url, href)\n",
    "            # Filter out non-HTTP/HTTPS URLs and fragments\n",
    "            parsed = urlparse(absolute_url)\n",
    "            if parsed.scheme in ('http', 'https') and not parsed.fragment:\n",
    "                links.append(absolute_url)\n",
    "        return links\n",
    "    \n",
    "    def _extract_text(self, soup):\n",
    "        \"\"\"Extract text content from a BeautifulSoup object.\"\"\"\n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        # Get text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        \n",
    "        # Break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        \n",
    "        # Drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def crawl(self):\n",
    "        \"\"\"Start the crawling process.\"\"\"\n",
    "        # Initialize frontier with start URLs\n",
    "        for url in self.start_urls:\n",
    "            self._add_to_frontier(url, 0)\n",
    "        \n",
    "        # Process frontier until empty or max pages reached\n",
    "        while self.url_frontier and len(self.visited_urls) < self.max_pages_per_domain * len(set(urlparse(u).netloc for u, _ in self.url_frontier if u not in self.visited_urls)):\n",
    "            # Get next URL from frontier\n",
    "            url, depth = self.url_frontier.pop(0)\n",
    "            \n",
    "            if url in self.visited_urls:\n",
    "                continue\n",
    "            \n",
    "            # Mark as visited\n",
    "            self.visited_urls.add(url)\n",
    "            \n",
    "            # Respect rate limits\n",
    "            domain = urlparse(url).netloc\n",
    "            self._respect_rate_limits(domain)\n",
    "            \n",
    "            # Fetch page\n",
    "            try:\n",
    "                headers = {'User-Agent': self.user_agent}\n",
    "                response = requests.get(url, headers=headers, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Parse content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Store result\n",
    "                self.results.append({\n",
    "                    'url': url,\n",
    "                    'depth': depth,\n",
    "                    'soup': soup,\n",
    "                    'text_content': self._extract_text(soup),\n",
    "                    'title': soup.title.text if soup.title else url\n",
    "                })\n",
    "                \n",
    "                # If not at max depth, extract and add links to frontier\n",
    "                if depth < self.max_depth:\n",
    "                    links = self._extract_links(soup, url)\n",
    "                    for link in links:\n",
    "                        self._add_to_frontier(link, depth + 1)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error crawling {url}: {e}\")\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "\n",
    "# # Step 4: Define EnhancedKnowledgeReduction Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EnhancedKnowledgeReduction:\n",
    "    def __init__(self, use_transformer=True, transformer_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the knowledge reduction system.\n",
    "        \n",
    "        Args:\n",
    "            use_transformer (bool): Whether to use transformer models for embeddings\n",
    "            transformer_model (str): Transformer model to use for embeddings\n",
    "        \"\"\"\n",
    "        # Load spaCy model\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        \n",
    "        # Set up transformer model if requested\n",
    "        self.use_transformer = use_transformer\n",
    "        if use_transformer:\n",
    "            self.transformer = SentenceTransformer(transformer_model)\n",
    "        \n",
    "        # Similarity calculation methods\n",
    "        self.similarity_methods = {\n",
    "            \"spacy\": self._spacy_similarity,\n",
    "            \"tfidf\": self._tfidf_similarity,\n",
    "            \"transformer\": self._transformer_similarity,\n",
    "            \"hybrid\": self._hybrid_similarity\n",
    "        }\n",
    "    \n",
    "    def _spacy_similarity(self, text1, text2):\n",
    "        \"\"\"Calculate similarity using spaCy.\"\"\"\n",
    "        doc1 = self.nlp(text1)\n",
    "        doc2 = self.nlp(text2)\n",
    "        return doc1.similarity(doc2)\n",
    "    \n",
    "    def _tfidf_similarity(self, texts):\n",
    "        \"\"\"Calculate TF-IDF similarity matrix for a list of texts.\"\"\"\n",
    "        # Create TF-IDF vectorizer\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        return cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    def _transformer_similarity(self, texts):\n",
    "        \"\"\"Calculate similarity using transformer embeddings.\"\"\"\n",
    "        if not self.use_transformer:\n",
    "            raise ValueError(\"Transformer model not initialized\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.transformer.encode(texts)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        return cosine_similarity(embeddings)\n",
    "    \n",
    "    def _hybrid_similarity(self, texts, weights={\"spacy\": 0.3, \"tfidf\": 0.3, \"transformer\": 0.4}):\n",
    "        \"\"\"Calculate similarity using a weighted combination of methods.\"\"\"\n",
    "        similarity_matrices = {}\n",
    "        \n",
    "        # Calculate similarity using each method\n",
    "        if \"spacy\" in weights:\n",
    "            spacy_sim = np.zeros((len(texts), len(texts)))\n",
    "            for i in range(len(texts)):\n",
    "                for j in range(i, len(texts)):\n",
    "                    sim = self._spacy_similarity(texts[i], texts[j])\n",
    "                    spacy_sim[i, j] = sim\n",
    "                    spacy_sim[j, i] = sim\n",
    "            similarity_matrices[\"spacy\"] = spacy_sim\n",
    "        \n",
    "        if \"tfidf\" in weights:\n",
    "            similarity_matrices[\"tfidf\"] = self._tfidf_similarity(texts)\n",
    "        \n",
    "        if \"transformer\" in weights and self.use_transformer:\n",
    "            similarity_matrices[\"transformer\"] = self._transformer_similarity(texts)\n",
    "        \n",
    "        # Combine similarity matrices using weights\n",
    "        combined_sim = np.zeros((len(texts), len(texts)))\n",
    "        weight_sum = sum(weights.values())\n",
    "        \n",
    "        for method, weight in weights.items():\n",
    "            if method in similarity_matrices:\n",
    "                combined_sim += (weight / weight_sum) * similarity_matrices[method]\n",
    "        \n",
    "        return combined_sim\n",
    "    \n",
    "    def _find_central_fact(self, indices, similarity_matrix):\n",
    "        \"\"\"Find the most central fact in a cluster.\"\"\"\n",
    "        # Calculate centrality as the sum of similarities to other facts in the cluster\n",
    "        centrality = {}\n",
    "        for i in indices:\n",
    "            centrality[i] = sum(similarity_matrix[i, j] for j in indices if i != j)\n",
    "        \n",
    "        # Return the fact with highest centrality\n",
    "        return max(centrality.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    def reduce_knowledge(self, facts, method=\"hybrid\", threshold=0.85, min_cluster_size=2):\n",
    "        \"\"\"\n",
    "        Reduce knowledge by clustering similar facts.\n",
    "        \n",
    "        Args:\n",
    "            facts (list): List of fact statements\n",
    "            method (str): Similarity method to use\n",
    "            threshold (float): Similarity threshold for clustering\n",
    "            min_cluster_size (int): Minimum cluster size to consider\n",
    "            \n",
    "        Returns:\n",
    "            list: List of representative facts after reduction\n",
    "        \"\"\"\n",
    "        if len(facts) <= 1:\n",
    "            return facts\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        if method == \"hybrid\":\n",
    "            similarity_matrix = self._hybrid_similarity(facts)\n",
    "        elif method in self.similarity_methods:\n",
    "            similarity_matrix = self.similarity_methods[method](facts)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown similarity method: {method}\")\n",
    "        \n",
    "        # Apply hierarchical clustering\n",
    "        # Convert similarity to distance\n",
    "        distance_matrix = 1 - similarity_matrix\n",
    "        \n",
    "        # Apply clustering\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=1 - threshold,\n",
    "            affinity='precomputed',\n",
    "            linkage='average'\n",
    "        ).fit(distance_matrix)\n",
    "        \n",
    "        # Get cluster labels\n",
    "        labels = clustering.labels_\n",
    "        \n",
    "        # Group facts by cluster\n",
    "        clusters = {}\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(i)\n",
    "        \n",
    "        # Select representative fact from each cluster\n",
    "        representative_facts = []\n",
    "        for label, indices in clusters.items():\n",
    "            if len(indices) < min_cluster_size:\n",
    "                # Keep all facts from small clusters\n",
    "                for idx in indices:\n",
    "                    representative_facts.append(facts[idx])\n",
    "            else:\n",
    "                # Select the most central fact as representative\n",
    "                central_idx = self._find_central_fact(indices, similarity_matrix)\n",
    "                representative_facts.append(facts[central_idx])\n",
    "        \n",
    "        return representative_facts\n",
    "    \n",
    "    def entity_based_reduction(self, facts, threshold=0.85):\n",
    "        \"\"\"\n",
    "        Reduce knowledge based on entity overlap.\n",
    "        \n",
    "        Args:\n",
    "            facts (list): List of fact statements\n",
    "            threshold (float): Similarity threshold for entity overlap\n",
    "            \n",
    "        Returns:\n",
    "            list: List of facts after entity-based reduction\n",
    "        \"\"\"\n",
    "        # Process facts with spaCy to extract entities\n",
    "        docs = list(self.nlp.pipe(facts))\n",
    "        \n",
    "        # Create entity sets for each fact\n",
    "        entity_sets = []\n",
    "        for doc in docs:\n",
    "            entities = set()\n",
    "            for ent in doc.ents:\n",
    "                entities.add((ent.text, ent.label_))\n",
    "            entity_sets.append(entities)\n",
    "        \n",
    "        # Calculate entity overlap\n",
    "        unique_facts = []\n",
    "        unique_indices = []\n",
    "        \n",
    "        for i, (fact, entities) in enumerate(zip(facts, entity_sets)):\n",
    "            # Skip if no entities\n",
    "            if not entities:\n",
    "                unique_facts.append(fact)\n",
    "                unique_indices.append(i)\n",
    "                continue\n",
    "            \n",
    "            # Check if similar to any existing unique fact\n",
    "            is_unique = True\n",
    "            for j in unique_indices:\n",
    "                # Calculate Jaccard similarity of entity sets\n",
    "                overlap = len(entities.intersection(entity_sets[j]))\n",
    "                union = len(entities.union(entity_sets[j]))\n",
    "                \n",
    "                if union > 0 and overlap / union > threshold:\n",
    "                    is_unique = False\n",
    "                    break\n",
    "            \n",
    "            if is_unique:\n",
    "                unique_facts.append(fact)\n",
    "                unique_indices.append(i)\n",
    "        \n",
    "        return unique_facts\n",
    "    \n",
    "    def multi_stage_reduction(self, facts, stages=None):\n",
    "        \"\"\"\n",
    "        Apply multi-stage knowledge reduction.\n",
    "        \n",
    "        Args:\n",
    "            facts (list): List of fact statements\n",
    "            stages (list): List of reduction stages to apply\n",
    "            \n",
    "        Returns:\n",
    "            list: List of facts after multi-stage reduction\n",
    "        \"\"\"\n",
    "        if stages is None:\n",
    "            stages = [\n",
    "                {\"method\": \"length_filter\", \"min_length\": 50},\n",
    "                {\"method\": \"entity_based\", \"threshold\": 0.8},\n",
    "                {\"method\": \"hybrid\", \"threshold\": 0.85}\n",
    "            ]\n",
    "        \n",
    "        reduced_facts = facts\n",
    "        \n",
    "        for stage in stages:\n",
    "            method = stage[\"method\"]\n",
    "            \n",
    "            if method == \"length_filter\":\n",
    "                min_length = stage.get(\"min_length\", 50)\n",
    "                reduced_facts = [f for f in reduced_facts if len(f) >= min_length]\n",
    "            \n",
    "            elif method == \"entity_based\":\n",
    "                threshold = stage.get(\"threshold\", 0.8)\n",
    "                reduced_facts = self.entity_based_reduction(reduced_facts, threshold)\n",
    "            \n",
    "            elif method in self.similarity_methods or method == \"hybrid\":\n",
    "                threshold = stage.get(\"threshold\", 0.85)\n",
    "                min_cluster_size = stage.get(\"min_cluster_size\", 2)\n",
    "                reduced_facts = self.reduce_knowledge(\n",
    "                    reduced_facts, method=method, \n",
    "                    threshold=threshold, min_cluster_size=min_cluster_size\n",
    "                )\n",
    "            \n",
    "            print(f\"After {method} reduction: {len(reduced_facts)} facts remaining\")\n",
    "        \n",
    "        return reduced_facts\n",
    "\n",
    "\n",
    "# # Step 5: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76276867",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_text_into_facts(text, min_length=30):\n",
    "    \"\"\"\n",
    "    Process text content into facts.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text content to process\n",
    "        min_length (int): Minimum length for a fact\n",
    "        \n",
    "    Returns:\n",
    "        list: List of facts\n",
    "    \"\"\"\n",
    "    # Split text into sentences (simple approach)\n",
    "    sentences = [s.strip() for s in text.replace('\\n', ' ').split('.') if s.strip()]\n",
    "    \n",
    "    # Combine short sentences\n",
    "    facts = []\n",
    "    current_fact = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_fact) + len(sentence) <= min_length:\n",
    "            current_fact += \" \" + sentence if current_fact else sentence\n",
    "        else:\n",
    "            if current_fact:\n",
    "                facts.append(current_fact + \".\")\n",
    "            current_fact = sentence\n",
    "    \n",
    "    # Add the last fact if not empty\n",
    "    if current_fact:\n",
    "        facts.append(current_fact + \".\")\n",
    "    \n",
    "    return facts\n",
    "\n",
    "def populate_knowledge_graph_from_crawler(kg, crawler_results):\n",
    "    \"\"\"\n",
    "    Populate knowledge graph from crawler results.\n",
    "    \n",
    "    Args:\n",
    "        kg: KnowledgeGraph instance\n",
    "        crawler_results: Results from the RecursiveCrawler\n",
    "    \"\"\"\n",
    "    for result in crawler_results:\n",
    "        url = result['url']\n",
    "        title = result['title']\n",
    "        text_content = result['text_content']\n",
    "        \n",
    "        # Process text content into facts\n",
    "        facts = process_text_into_facts(text_content)\n",
    "        \n",
    "        # Add facts to knowledge graph\n",
    "        for fact in facts:\n",
    "            kg.add_fact(\n",
    "                fact_statement=fact,\n",
    "                source_id=url,\n",
    "                source_name=title,\n",
    "                reliability=ReliabilityRating.MEDIUM,  # Default rating\n",
    "                category=\"Web Content\",\n",
    "                tags=[\"crawled\", f\"depth_{result['depth']}\"]\n",
    "            )\n",
    "\n",
    "def apply_knowledge_reduction(kg, reducer=None):\n",
    "    \"\"\"\n",
    "    Apply knowledge reduction to a knowledge graph.\n",
    "    \n",
    "    Args:\n",
    "        kg: KnowledgeGraph instance\n",
    "        reducer: EnhancedKnowledgeReduction instance (created if None)\n",
    "    \n",
    "    Returns:\n",
    "        KnowledgeGraph: New knowledge graph with reduced facts\n",
    "    \"\"\"\n",
    "    if reducer is None:\n",
    "        reducer = EnhancedKnowledgeReduction()\n",
    "    \n",
    "    # Extract facts from knowledge graph\n",
    "    facts = [fact['fact_statement'] for fact in kg.data]\n",
    "    \n",
    "    # Apply multi-stage reduction\n",
    "    reduced_facts = reducer.multi_stage_reduction(facts)\n",
    "    \n",
    "    # Create new knowledge graph with reduced facts\n",
    "    reduced_kg = KnowledgeGraph()\n",
    "    \n",
    "    # Add reduced facts to new knowledge graph\n",
    "    for fact_statement in reduced_facts:\n",
    "        # Find original fact in knowledge graph\n",
    "        for fact in kg.data:\n",
    "            if fact['fact_statement'] == fact_statement:\n",
    "                # Copy fact to new knowledge graph\n",
    "                reduced_kg.add_fact(\n",
    "                    fact_statement=fact['fact_statement'],\n",
    "                    source_id=fact['source_id'],\n",
    "                    source_name=fact['source_name'],\n",
    "                    reliability=fact['reliability'],\n",
    "                    category=fact['category'],\n",
    "                    tags=fact['tags']\n",
    "                )\n",
    "                break\n",
    "    \n",
    "    return reduced_kg\n",
    "\n",
    "\n",
    "# # Step 6: Advanced Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the crawler with multiple start URLs\n",
    "start_urls = [\n",
    "    \"https://civichonors.com/\",\n",
    "    \"https://www.nelslindahl.com/\",\n",
    "    \"https://en.wikipedia.org/wiki/Knowledge_graph\",\n",
    "    \"https://en.wikipedia.org/wiki/Web_crawler\"\n",
    "]\n",
    "\n",
    "# Create the crawler with custom settings\n",
    "crawler = RecursiveCrawler(\n",
    "    start_urls=start_urls,\n",
    "    max_depth=2,  # Follow links up to 2 levels deep\n",
    "    max_pages_per_domain=5,  # Limit to 5 pages per domain\n",
    "    respect_robots=True,  # Respect robots.txt\n",
    "    rate_limit=1.0,  # Wait at least 1 second between requests to the same domain\n",
    "    user_agent=\"KnowledgeReduceCrawler/1.0\"\n",
    ")\n",
    "\n",
    "# Start crawling\n",
    "print(\"Starting crawling process...\")\n",
    "results = crawler.crawl()\n",
    "print(f\"Crawling complete. Visited {len(crawler.visited_urls)} URLs, collected {len(results)} pages.\")\n",
    "\n",
    "# Print statistics about the crawled pages\n",
    "domains = {}\n",
    "depths = {}\n",
    "\n",
    "for result in results:\n",
    "    domain = urlparse(result['url']).netloc\n",
    "    depth = result['depth']\n",
    "    \n",
    "    if domain not in domains:\n",
    "        domains[domain] = 0\n",
    "    domains[domain] += 1\n",
    "    \n",
    "    if depth not in depths:\n",
    "        depths[depth] = 0\n",
    "    depths[depth] += 1\n",
    "\n",
    "print(\"\\nPages per domain:\")\n",
    "for domain, count in domains.items():\n",
    "    print(f\"  {domain}: {count} pages\")\n",
    "\n",
    "print(\"\\nPages per depth level:\")\n",
    "for depth, count in sorted(depths.items()):\n",
    "    print(f\"  Depth {depth}: {count} pages\")\n",
    "\n",
    "\n",
    "# # Step 7: Populate the KnowledgeGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the KnowledgeGraph\n",
    "kg = KnowledgeGraph()\n",
    "\n",
    "# Populate the knowledge graph from crawler results\n",
    "populate_knowledge_graph_from_crawler(kg, results)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Knowledge graph populated with {len(kg.data)} facts from {len(results)} pages.\")\n",
    "\n",
    "# Display a sample of facts\n",
    "print(\"\\nSample facts:\")\n",
    "for fact in kg.data[:5]:\n",
    "    print(f\"Fact {fact['fact_id']}: {fact['fact_statement'][:100]}... (Source: {fact['source_name']})\")\n",
    "\n",
    "# Count facts by depth\n",
    "facts_by_depth = {}\n",
    "for fact in kg.data:\n",
    "    depth_tags = [tag for tag in fact['tags'] if tag.startswith('depth_')]\n",
    "    if depth_tags:\n",
    "        depth = int(depth_tags[0].split('_')[1])\n",
    "        if depth not in facts_by_depth:\n",
    "            facts_by_depth[depth] = 0\n",
    "        facts_by_depth[depth] += 1\n",
    "\n",
    "print(\"\\nFacts by crawl depth:\")\n",
    "for depth, count in sorted(facts_by_depth.items()):\n",
    "    print(f\"  Depth {depth}: {count} facts\")\n",
    "\n",
    "\n",
    "# # Step 8: Apply Advanced Knowledge Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the knowledge reducer\n",
    "reducer = EnhancedKnowledgeReduction(use_transformer=True)\n",
    "\n",
    "# Apply multi-stage knowledge reduction\n",
    "print(\"Applying multi-stage knowledge reduction...\")\n",
    "reduced_kg = apply_knowledge_reduction(kg, reducer)\n",
    "\n",
    "print(f\"\\nOriginal knowledge graph: {len(kg.data)} facts\")\n",
    "print(f\"Reduced knowledge graph: {len(reduced_kg.data)} facts\")\n",
    "print(f\"Reduction ratio: {len(reduced_kg.data) / len(kg.data):.2f}\")\n",
    "\n",
    "# Display sample of reduced facts\n",
    "print(\"\\nSample reduced facts:\")\n",
    "for fact in reduced_kg.data[:10]:\n",
    "    print(f\"Fact {fact['fact_id']}: {fact['fact_statement'][:100]}... (Source: {fact['source_name']})\")\n",
    "\n",
    "\n",
    "# # Step 9: Serialize the KnowledgeGraph for Portability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca4657",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KnowledgeGraphPortable:\n",
    "    def __init__(self, knowledge_graph=None):\n",
    "        \"\"\"\n",
    "        Initialize the portable knowledge graph.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_graph: KnowledgeGraph instance to serialize\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        if knowledge_graph:\n",
    "            self.serialize(knowledge_graph)\n",
    "    \n",
    "    def serialize(self, knowledge_graph):\n",
    "        \"\"\"\n",
    "        Serialize a knowledge graph to JSON.\n",
    "        \n",
    "        Args:\n",
    "            knowledge_graph: KnowledgeGraph instance to serialize\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        for fact in knowledge_graph.data:\n",
    "            # Convert enum to string for JSON serialization\n",
    "            fact_copy = fact.copy()\n",
    "            if isinstance(fact_copy['reliability'], ReliabilityRating):\n",
    "                fact_copy['reliability'] = fact_copy['reliability'].name\n",
    "            self.data.append(fact_copy)\n",
    "    \n",
    "    def save_to_json(self, filename, shard_size=100):\n",
    "        \"\"\"\n",
    "        Save the serialized knowledge graph to JSON files with sharding.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): Base filename to save to\n",
    "            shard_size (int): Number of facts per shard\n",
    "        \"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)\n",
    "        \n",
    "        # Shard the data\n",
    "        for i in range(0, len(self.data), shard_size):\n",
    "            shard = self.data[i:i+shard_size]\n",
    "            shard_filename = f\"{os.path.splitext(filename)[0]}_shard_{i//shard_size}{os.path.splitext(filename)[1]}\"\n",
    "            with open(shard_filename, 'w') as f:\n",
    "                json.dump(shard, f, indent=2)\n",
    "    \n",
    "    def load_from_json(self, filename_pattern):\n",
    "        \"\"\"\n",
    "        Load the serialized knowledge graph from JSON files.\n",
    "        \n",
    "        Args:\n",
    "            filename_pattern (str): Pattern to match JSON files\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        \n",
    "        # Find all matching files\n",
    "        import glob\n",
    "        files = glob.glob(filename_pattern)\n",
    "        \n",
    "        for file in sorted(files):\n",
    "            with open(file, 'r') as f:\n",
    "                shard_data = json.load(f)\n",
    "                self.data.extend(shard_data)\n",
    "    \n",
    "    def deserialize(self):\n",
    "        \"\"\"\n",
    "        Deserialize the JSON data to a KnowledgeGraph instance.\n",
    "        \n",
    "        Returns:\n",
    "            KnowledgeGraph: Deserialized knowledge graph\n",
    "        \"\"\"\n",
    "        kg = KnowledgeGraph()\n",
    "        \n",
    "        for fact in self.data:\n",
    "            # Convert string back to enum\n",
    "            if isinstance(fact['reliability'], str):\n",
    "                fact['reliability'] = ReliabilityRating[fact['reliability']]\n",
    "            \n",
    "            # Add fact to knowledge graph\n",
    "            kg.add_fact(\n",
    "                fact_statement=fact['fact_statement'],\n",
    "                source_id=fact['source_id'],\n",
    "                source_name=fact['source_name'],\n",
    "                reliability=fact['reliability'],\n",
    "                category=fact['category'],\n",
    "                tags=fact['tags'],\n",
    "                quality_score=fact['quality_score']\n",
    "            )\n",
    "        \n",
    "        return kg\n",
    "\n",
    "# Create a portable version of the reduced knowledge graph\n",
    "kg_portable = KnowledgeGraphPortable(reduced_kg)\n",
    "\n",
    "# Save to JSON files with sharding\n",
    "kg_portable.save_to_json(\"reduced_knowledge_graph.json\", shard_size=100)\n",
    "\n",
    "# List the created JSON files\n",
    "import glob\n",
    "json_files = glob.glob(\"reduced_knowledge_graph*.json\")\n",
    "print(f\"Created {len(json_files)} JSON files:\")\n",
    "for file in sorted(json_files):\n",
    "    print(f\"  {file}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
