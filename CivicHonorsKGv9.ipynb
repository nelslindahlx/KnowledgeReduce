{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRPnbd13I3DbpSv9wsx0wo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelslindahlx/KnowledgeReduce/blob/main/CivicHonorsKGv9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Summary\n",
        "\n",
        "The code provided is a comprehensive Python script designed to run in Google Colab for creating a knowledge graph from data scraped from two websites. Here's a summary of the steps and key functionalities of the code:\n",
        "\n",
        "1. **Install Necessary Libraries**:\n",
        "   - The script begins by installing required Python libraries: `networkx` for graph-related operations, `beautifulsoup4` and `lxml` for web scraping, and `requests` for handling HTTP requests.\n",
        "\n",
        "2. **Import Libraries and Setup Logging**:\n",
        "   - Essential libraries are imported, and Python's logging module is configured for better output readability and debugging.\n",
        "\n",
        "3. **Web Scraping Functions**:\n",
        "   - Two separate functions, `scrape_website_for_couplets` and `scrape_another_website_for_couplets`, are defined. Each is tailored to scrape data from specific websites: \"https://civichonors.com/\" and \"https://www.nelslindahl.com/\".\n",
        "   - The scraping functions extract titles and summaries/descriptions from the articles present on these websites, storing them as \"couplets\" (pairs of related information).\n",
        "\n",
        "4. **Create the Knowledge Graph**:\n",
        "   - The `create_knowledge_graph_from_couplets` function constructs a network graph using the `networkx` library. Each couplet results in two nodes (representing entities or attributes) and an edge between them.\n",
        "\n",
        "5. **Export the Knowledge Graph**:\n",
        "   - The `export_graph` function exports the constructed graph into GraphML and GEXF formats, ensuring the graph's portability and compatibility with various graph analysis tools.\n",
        "\n",
        "6. **Evaluate the Knowledge Graph**:\n",
        "   - `evaluate_knowledge_graph` assesses the structure of the graph, providing insights into the number of nodes and edges, detection of isolated nodes, connectivity, and other structural properties.\n",
        "\n",
        "7. **Integrate Data from Another Website**:\n",
        "   - Additional data from \"https://www.nelslindahl.com/\" is scraped and integrated into the existing graph using `integrate_new_data`, enhancing the graph's comprehensiveness.\n",
        "\n",
        "8. **Execution Flow**:\n",
        "   - The script executes these functions in sequence: scraping data from both websites, creating and evaluating the graph, then integrating the new data, re-evaluating, and finally exporting the updated graph.\n",
        "\n",
        "The code effectively demonstrates how to build a knowledge graph by extracting and structuring data from multiple web sources. It showcases essential tasks like web scraping, graph construction, data integration, and graph analysis, making it a versatile template for similar data processing and knowledge representation projects."
      ],
      "metadata": {
        "id": "oQMXsnmINI7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Necessary Libraries"
      ],
      "metadata": {
        "id": "4wwMAraCLwf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FImeAFEUJ28a",
        "outputId": "029c0a3a-547e-4ee5-ee76-e3dc2ff24778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install networkx beautifulsoup4 requests lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import Libraries and Setup Logging"
      ],
      "metadata": {
        "id": "J0sp5iWyLyor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import networkx as nx\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)"
      ],
      "metadata": {
        "id": "FBpsh8ERL00Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Define the Web Scraping Function"
      ],
      "metadata": {
        "id": "aiZ9eU0QL2Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_website_for_couplets(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "        articles = soup.find_all('article')\n",
        "        couplets = [(article.find('h2').get_text(strip=True), article.find('p').get_text(strip=True)) for article in articles if article.find('h2') and article.find('p')]\n",
        "        return couplets\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error scraping {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "def scrape_another_website_for_couplets(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "        # Replace the following line with the correct selector for www.nelslindahl.com\n",
        "        articles = soup.find_all('article')\n",
        "        couplets = [(article.find('h2').get_text(strip=True), article.find('p').get_text(strip=True)) for article in articles if article.find('h2') and article.find('p')]\n",
        "        return couplets\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error scraping {url}: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "yR3M0ImJL44F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Create the Knowledge Graph"
      ],
      "metadata": {
        "id": "vws-1p-LL629"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_knowledge_graph_from_couplets(couplets):\n",
        "    G = nx.Graph()\n",
        "    for entity, attribute in couplets:\n",
        "        G.add_node(entity)\n",
        "        G.add_node(attribute)\n",
        "        G.add_edge(entity, attribute)\n",
        "    return G"
      ],
      "metadata": {
        "id": "h0jUIxbqL8eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Export the Knowledge Graph"
      ],
      "metadata": {
        "id": "B1TBLzdaL-Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_graph(graph, filename, format):\n",
        "    try:\n",
        "        file_path = f\"{filename}.{format}\"\n",
        "        if format.lower() == 'gexf':\n",
        "            nx.write_gexf(graph, file_path)\n",
        "        elif format.lower() == 'graphml':\n",
        "            nx.write_graphml(graph, file_path)\n",
        "        logging.info(f\"Graph successfully exported as {file_path}.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to export graph: {e}\")"
      ],
      "metadata": {
        "id": "Rtm2tyqrL_W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Evaluate the Knowledge Graph\n"
      ],
      "metadata": {
        "id": "A43u0fTDMhJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_knowledge_graph(graph):\n",
        "    num_nodes = graph.number_of_nodes()\n",
        "    num_edges = graph.number_of_edges()\n",
        "    print(f\"The graph has {num_nodes} nodes and {num_edges} edges.\")\n",
        "    isolated_nodes = list(nx.isolates(graph))\n",
        "    print(f\"Number of isolated nodes: {len(isolated_nodes)}\")\n",
        "    if nx.is_connected(graph):\n",
        "        print(\"The graph is connected.\")\n",
        "    else:\n",
        "        largest_cc = max(nx.connected_components(graph), key=len)\n",
        "        print(f\"Size of the largest connected component: {len(largest_cc)}\")"
      ],
      "metadata": {
        "id": "3ZbXXQgUMl6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Integrate Data from Another Website"
      ],
      "metadata": {
        "id": "_mjUXoItQ_ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def integrate_new_data(graph, new_data):\n",
        "    for entity, attribute in new_data:\n",
        "        if not graph.has_node(entity):\n",
        "            graph.add_node(entity)\n",
        "        if not graph.has_node(attribute):\n",
        "            graph.add_node(attribute)\n",
        "        graph.add_edge(entity, attribute)\n",
        "    return graph"
      ],
      "metadata": {
        "id": "VHB6Rt4GRBrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Execution Flow"
      ],
      "metadata": {
        "id": "bv1loKb_REUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape Data from the First Website (https://civichonors.com/):"
      ],
      "metadata": {
        "id": "VwpGUby_R9d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "couplets = scrape_website_for_couplets(\"https://civichonors.com/\")"
      ],
      "metadata": {
        "id": "Bz5MyDgFRxlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Initial Knowledge Graph:"
      ],
      "metadata": {
        "id": "EmeAzdkhR_Zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = create_knowledge_graph_from_couplets(couplets)"
      ],
      "metadata": {
        "id": "-WF7lT4PRxif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Initial Graph:"
      ],
      "metadata": {
        "id": "r1ZIE-DISBO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_knowledge_graph(G)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FplYXiaRRxfO",
        "outputId": "479679fb-6b2b-4dc8-db4d-843bbbddab11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The graph has 2 nodes and 1 edges.\n",
            "Number of isolated nodes: 0\n",
            "The graph is connected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape Data from the Second Website (https://www.nelslindahl.com/):"
      ],
      "metadata": {
        "id": "faWF1lftSDC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_couplets = scrape_another_website_for_couplets(\"https://www.nelslindahl.com/\")"
      ],
      "metadata": {
        "id": "HacN_-JjRxcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrate and Update the Graph:"
      ],
      "metadata": {
        "id": "0ByiOIS7SFAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = integrate_new_data(G, new_couplets)"
      ],
      "metadata": {
        "id": "sfex0auMRxYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-evaluate the Updated Graph:"
      ],
      "metadata": {
        "id": "bi3SJNIMSG2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_knowledge_graph(G)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS9plT3VRxO9",
        "outputId": "a103ab7f-ed51-4d18-d0b0-8670fb24abe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The graph has 4 nodes and 2 edges.\n",
            "Number of isolated nodes: 0\n",
            "Size of the largest connected component: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the Updated Graph:"
      ],
      "metadata": {
        "id": "nAwc_5bKSIiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export_graph(G, \"updated_knowledge_graph\", \"gexf\")\n",
        "export_graph(G, \"updated_knowledge_graph\", \"graphml\")"
      ],
      "metadata": {
        "id": "lqzn1W0qR59z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}